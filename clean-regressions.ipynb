{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10783414,"sourceType":"datasetVersion","datasetId":6691461},{"sourceId":11401041,"sourceType":"datasetVersion","datasetId":6602831}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nos.listdir('/kaggle/input') \n\nembeddings_path = \"/kaggle/input/embeddings\"\nprint(os.listdir(embeddings_path))\nimport pandas as pd\n\nspeeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\nnews_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:15:31.306541Z","iopub.execute_input":"2025-04-16T13:15:31.306985Z","iopub.status.idle":"2025-04-16T13:23:28.451042Z","shell.execute_reply.started":"2025-04-16T13:15:31.306951Z","shell.execute_reply":"2025-04-16T13:23:28.449665Z"}},"outputs":[{"name":"stdout","text":"['speeches_with_embeddings.csv', 'speeches_embeddings_sentiment.csv', 'news_embeddings_sentiment.csv', 'all_texts_embeddings.joblib', 'news_with_embeddings.csv']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# Convert and verify date columns\nnews_embeddings['news_date'] = pd.to_datetime(news_embeddings['Date'])\nspeeches_embeddings['speech_date'] = pd.to_datetime(speeches_embeddings['date'])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n#Temporal window calculation and expansion\ndef generate_temporal_pairs(news_df, speeches_df, window_days=3):\n    \"\"\"Generate news-speech pairs within a symmetric temporal window (-3 to +3 days)\"\"\"\n    pairs = []\n    chunk_size = 2000\n    news_chunks = np.array_split(news_df, len(news_df) // chunk_size + 1)\n    \n    for chunk in news_chunks:\n        for _, row in chunk.iterrows():\n            news_date = row['news_date']\n            start_date = news_date - pd.Timedelta(days=window_days) \n            end_date = news_date + pd.Timedelta(days=window_days)  \n            \n            mask = (speeches_df['speech_date'] >= start_date) & (speeches_df['speech_date'] <= end_date)\n            speech_ids = speeches_df[mask].index.tolist()\n            pairs.extend([(row.name, s_id) for s_id in speech_ids])\n    \n    return pd.DataFrame(pairs, columns=['news_id', 'speech_id'])\n\n\nalignment_df = generate_temporal_pairs(news_embeddings, speeches_embeddings)\n\n#optimized embeddings calculation\ndef load_embeddings_half(df, col_name):\n    embeddings = []\n    for i, row in df.iterrows():\n        if isinstance(row[col_name], str):\n            arr = np.fromstring(row[col_name].strip(\"[]\"), sep=\" \", dtype=np.float16)\n        else:\n            arr = np.array(row[col_name], dtype=np.float16)\n        embeddings.append(torch.tensor(arr, device=device).half())\n        if i % 1000 == 0: torch.cuda.empty_cache()\n    return torch.stack(embeddings)\n\nnews_tensor = load_embeddings_half(news_embeddings, 'news_embeddings')\nspeeches_tensor = load_embeddings_half(speeches_embeddings, 'speech_embeddings')\n\n#Batched cosine similarity computation\ndef compute_cosine_similarities(pairs_df, news_emb, speech_emb, batch_size=8192):\n    news_norm = F.normalize(news_emb, p=2, dim=1)\n    speech_norm = F.normalize(speech_emb, p=2, dim=1)\n    similarities = []\n    for i in range(0, len(pairs_df), batch_size):\n        batch = pairs_df.iloc[i:i+batch_size]\n        news_batch = news_norm[batch['news_id'].values]\n        speech_batch = speech_norm[batch['speech_id'].values]\n        similarities.append(F.cosine_similarity(news_batch, speech_batch).cpu().numpy())\n        del news_batch, speech_batch\n        torch.cuda.empty_cache()\n    return np.concatenate(similarities)\n\nalignment_df['cosine_similarity'] = compute_cosine_similarities(alignment_df, news_tensor, speeches_tensor)\n\n#include metadata to the embeddings to track temporal dependencies\ndef add_temporal_features(pairs_df, news_df, speeches_df):\n    pairs_df = pairs_df.merge(\n        news_df[['news_date']],\n        left_on='news_id',\n        right_index=True\n    ).merge(\n        speeches_df[['speech_date']],\n        left_on='speech_id',\n        right_index=True\n    )\n    pairs_df['days_diff'] = (pairs_df['news_date'] - pairs_df['speech_date']).dt.days\n    return pairs_df\n\nenriched_df = add_temporal_features(alignment_df, news_embeddings, speeches_embeddings)\n\n#Save data to avoid rerunning everything again \nenriched_df.to_parquet('news_speech_similarities.parquet', engine='pyarrow', compression='zstd')\nprint(\"Processing complete. Results saved with columns:\", enriched_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:29:41.530947Z","iopub.execute_input":"2025-04-16T13:29:41.531870Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Load processed data\ndf = pd.read_parquet('news_speech_similarities.parquet') #We can use this code when we compute everything again, else we have to roead the parquet\n#df = pd.read_parquet(SIMILARITIES_PATH) #Piece of code when we have the path defined, otherwise it'll work by loading them again. \n# Convert to datetime and normalize (remove time components)\ndf['news_date'] = pd.to_datetime(df['news_date']).dt.normalize()\ndf['year'] = df['news_date'].dt.year\n\n# Create daily aggregates with std dev\ndaily_agg = df.groupby('news_date')['cosine_similarity'].agg(['mean', 'std']).reset_index()\ndaily_agg.columns = ['date', 'cosine_similarity', 'std_dev']\n\n# Extend full_dates to include October 2024 explicitly\nend_date = pd.to_datetime('2024-10-31')  # Adjust as needed\nfull_dates = pd.date_range(\n    start=daily_agg['date'].min(), \n    end=end_date, \n    freq='D'\n)\ndaily_agg = daily_agg.set_index('date').reindex(full_dates).reset_index().rename(columns={'index': 'date'})\n\n# Calculate bounds\ndaily_agg['upper_bound'] = daily_agg['cosine_similarity'] + daily_agg['std_dev'].fillna(0)\ndaily_agg['lower_bound'] = daily_agg['cosine_similarity'] - daily_agg['std_dev'].fillna(0)\n\n# Create monthly aggregates (fill NaN with 0 for plotting)\nmonthly_agg = daily_agg.set_index('date').resample('M')['cosine_similarity'].mean().fillna(0).reset_index()\nmonthly_agg['month_label'] = monthly_agg['date'].dt.strftime('%b\\n%Y')\n\n# Get unique years present in data\nyears = daily_agg['date'].dt.year.unique()\n\n# Set up plot\nplt.figure(figsize=(18, 8 * len(years)))\n\nfor i, year in enumerate(years, 1):\n    year_mask = daily_agg['date'].dt.year == year\n    yearly_daily = daily_agg[year_mask]\n    yearly_monthly = monthly_agg[monthly_agg['date'].dt.year == year]\n    \n    if yearly_daily.empty:\n        continue\n    \n    ax = plt.subplot(len(years), 1, i)\n    \n    # Daily plot with variability\n    ax.plot(yearly_daily['date'], \n            yearly_daily['cosine_similarity'], \n            color='#2ca02c', \n            linewidth=1.5,\n            label='Daily Average')\n    \n    ax.fill_between(yearly_daily['date'],\n                    yearly_daily['upper_bound'],\n                    yearly_daily['lower_bound'],\n                    color='gray', alpha=0.3, \n                    label='Daily Std Dev')\n    \n    # Monthly markers (plot even if value is 0)\n    ax.scatter(yearly_monthly['date'], \n               yearly_monthly['cosine_similarity'],\n               color='darkblue', \n               s=100,\n               zorder=5,\n               label='Monthly Average')\n    \n    # Annotate monthly values (skip if 0)\n    for _, row in yearly_monthly.iterrows():\n        if row['cosine_similarity'] != 0:\n            ax.text(row['date'], row['cosine_similarity']+0.02,\n                    f\"{row['cosine_similarity']:.2f}\",\n                    ha='center', va='bottom',\n                    fontsize=9, color='darkblue')\n    \n    # Highlight missing days\n    missing_mask = yearly_daily['cosine_similarity'].isna()\n    ax.fill_between(yearly_daily['date'],\n                    yearly_daily['cosine_similarity'].min() - 0.1,\n                    yearly_daily['cosine_similarity'].max() + 0.1,\n                    where=missing_mask,\n                    color='red', alpha=0.1,\n                    label='Missing Days')\n    \n    # Formatting\n    ax.set_title(f'{year} Daily/Monthly Speech-News Cosine Similarity', pad=20)\n    ax.set_xlabel('')\n    ax.set_ylabel('Cosine Similarity', fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='upper right')\n    \n    # Set monthly x-ticks\n    ax.set_xticks(yearly_monthly['date'])\n    ax.set_xticklabels(yearly_monthly['month_label'])\n    \n    # Set y-axis limits\n    y_min = max(daily_agg['cosine_similarity'].min() - 0.1, 0)\n    y_max = min(daily_agg['cosine_similarity'].max() + 0.1, 1)\n    ax.set_ylim(y_min, y_max)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:28:11.684487Z","iopub.execute_input":"2025-04-16T13:28:11.685519Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_72/322553216.py:27: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  monthly_agg = daily_agg.set_index('date').resample('M')['cosine_similarity'].mean().fillna(0).reset_index()\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import re\n# again extracting top media outlets. Maybe there is a more efficient way to do this. \ndef extract_outlet(url):\n    patterns = [r\"https?://(?:www\\.)?([^/.]+)\\.\"]\n    match = re.search(patterns[0], str(url), re.IGNORECASE)\n    return match.group(1).lower() if match else \"unknown\"\n\n# Create outlet column in news_embeddings\nnews_embeddings['outlet'] = news_embeddings['Link'].apply(extract_outlet)","metadata":{"trusted":true,"execution":{"iopub.execute_input":"2025-04-16T13:28:19.492520Z","iopub.status.idle":"2025-04-16T13:28:19.575014Z","shell.execute_reply.started":"2025-04-16T13:28:19.492479Z","shell.execute_reply":"2025-04-16T13:28:19.573831Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Now this will work\n\nenriched_df = enriched_df.merge(\n    news_embeddings[['outlet']],\n    left_on='news_id',\n    right_index=True,\n    how='left'\n)\n\n# Final check\nprint(\"\\nColumns in enriched_df:\", enriched_df.columns.tolist())\nprint(\"Sample outlets:\", enriched_df['outlet'].unique()[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:28:19.576113Z","iopub.execute_input":"2025-04-16T13:28:19.576541Z","iopub.status.idle":"2025-04-16T13:28:24.898473Z","shell.execute_reply.started":"2025-04-16T13:28:19.576506Z","shell.execute_reply":"2025-04-16T13:28:24.896880Z"}},"outputs":[{"name":"stdout","text":"\nColumns in enriched_df: ['news_id', 'speech_id', 'cosine_similarity', 'news_date', 'speech_date', 'days_diff', 'outlet']\nSample outlets: ['bbc' 'politica' 'oem' 'eleconomista' 'milenio']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### First regressions\n\nIn regression 1, I have to change the variable days_diff into dummies. \nIn regression 2, I have to  ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.api as sm\n\n# 1. Create dummy variables for outlets\ndummies_outlet = pd.get_dummies(enriched_df['outlet'], prefix='outlet')\n\n# 2. Filter for top outlets\ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()\ndummy_outlet_columns = [f'outlet_{outlet}' for outlet in top_outlets if f'outlet_{outlet}' in dummies_outlet.columns]\ndummies_outlet = dummies_outlet[dummy_outlet_columns]\n\n# 3. Create dummies for days_diff\nenriched_df['days_diff'] = pd.to_numeric(enriched_df['days_diff'], errors='coerce')\ndummies_days = pd.get_dummies(enriched_df['days_diff'], prefix='days_diff')\ndummies_days = dummies_days.drop(columns='days_diff_0', errors='ignore')  # Drop baseline\n\n# 4. Combine dummies\ndummies = pd.concat([dummies_outlet, dummies_days], axis=1)\n\n# 5. Handle missing values\nvalid_data = enriched_df[['cosine_similarity']].join(dummies).dropna()\n\n# ✅ Ensure all columns are numeric\nX = sm.add_constant(valid_data[dummies.columns]).astype(float)\ny = valid_data['cosine_similarity'].astype(float)\n\n# 6. Run regression\nmodel = sm.OLS(y, X).fit(cov_type='HC3')\n\n# 7. Output summary\nprint(\"\\n=== Outlet + Days Diff Fixed Effects Model ===\")\nprint(model.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:34:53.005381Z","iopub.execute_input":"2025-04-16T09:34:53.005700Z","iopub.status.idle":"2025-04-16T09:36:01.118644Z","shell.execute_reply.started":"2025-04-16T09:34:53.005670Z","shell.execute_reply":"2025-04-16T09:36:01.117670Z"}},"outputs":[{"name":"stdout","text":"\n=== Outlet + Days Diff Fixed Effects Model ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.078\nModel:                            OLS   Adj. R-squared:                  0.078\nMethod:                 Least Squares   F-statistic:                 1.500e+05\nDate:                Wed, 16 Apr 2025   Prob (F-statistic):               0.00\nTime:                        09:36:01   Log-Likelihood:             2.7810e+07\nNo. Observations:            24418171   AIC:                        -5.562e+07\nDf Residuals:                24418154   BIC:                        -5.562e+07\nDf Model:                          16                                         \nCovariance Type:                  HC3                                         \n=======================================================================================\n                          coef    std err          z      P>|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                   0.3033   4.87e-05   6224.306      0.000       0.303       0.303\noutlet_infobae         -0.0141   4.26e-05   -332.210      0.000      -0.014      -0.014\noutlet_proceso          0.0411   4.77e-05    862.093      0.000       0.041       0.041\noutlet_oem              0.0510   7.39e-05    689.626      0.000       0.051       0.051\noutlet_politica         0.0084   7.27e-05    115.410      0.000       0.008       0.009\noutlet_elfinanciero     0.0096   7.76e-05    123.225      0.000       0.009       0.010\noutlet_forbes           0.0049   8.52e-05     57.880      0.000       0.005       0.005\noutlet_elpais          -0.0249   8.88e-05   -280.766      0.000      -0.025      -0.025\noutlet_eleconomista     0.0458   9.65e-05    474.536      0.000       0.046       0.046\noutlet_lasillarota      0.0502   9.01e-05    557.676      0.000       0.050       0.050\noutlet_milenio          0.0202      0.000    161.305      0.000       0.020       0.020\ndays_diff_-3           -0.0056   5.82e-05    -95.902      0.000      -0.006      -0.005\ndays_diff_-2           -0.0064   6.03e-05   -106.652      0.000      -0.007      -0.006\ndays_diff_-1           -0.0057   5.95e-05    -96.689      0.000      -0.006      -0.006\ndays_diff_1            -0.0071    5.8e-05   -121.791      0.000      -0.007      -0.007\ndays_diff_2            -0.0089   5.84e-05   -152.518      0.000      -0.009      -0.009\ndays_diff_3            -0.0114   5.85e-05   -194.581      0.000      -0.012      -0.011\n==============================================================================\nOmnibus:                    59897.842   Durbin-Watson:                   0.012\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            61493.083\nSkew:                          -0.109   Prob(JB):                         0.00\nKurtosis:                       3.112   Cond. No.                         9.11\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Regression with temporal dependencies (dummies per month and year)¶","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:39:09.648051Z","iopub.execute_input":"2025-04-16T09:39:09.649418Z","iopub.status.idle":"2025-04-16T09:39:09.888160Z","shell.execute_reply.started":"2025-04-16T09:39:09.649376Z","shell.execute_reply":"2025-04-16T09:39:09.887222Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"384"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(\"Unique days_diff values:\", enriched_df['days_diff'].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:29:30.642332Z","iopub.execute_input":"2025-04-16T13:29:30.643008Z","iopub.status.idle":"2025-04-16T13:29:30.764832Z","shell.execute_reply.started":"2025-04-16T13:29:30.642967Z","shell.execute_reply":"2025-04-16T13:29:30.763846Z"}},"outputs":[{"name":"stdout","text":"Unique days_diff values: [-3 -2 -1  0  1  2  3]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.formula.api as smf\n\n# --- Data Preparation ---\n# Convert to categorical types upfront\nenriched_df = enriched_df.astype({\n    'outlet': 'category',\n    'news_date': 'datetime64[ns]',\n    'speech_date': 'datetime64[ns]'\n})\n\n# Create temporal features without expanding memory\nenriched_df['month'] = enriched_df['news_date'].dt.month.astype('int8')\nenriched_df['year'] = enriched_df['news_date'].dt.year.astype('int16')\n\n# Filter to top outlets based on frequency\ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()\n\n# --- Minimal changes to recode outlet ---\n# First, add 'Other' as a valid category, then recode.\nenriched_df['outlet'] = enriched_df['outlet'].cat.add_categories('Other')\nenriched_df.loc[~enriched_df['outlet'].isin(top_outlets), 'outlet'] = 'Other'\nenriched_df['outlet'] = enriched_df['outlet'].cat.set_categories(top_outlets + ['Other'])\n\n# Downcast numerical columns for efficiency\nenriched_df['cosine_similarity'] = pd.to_numeric(enriched_df['cosine_similarity'], downcast='float')\nenriched_df['days_diff'] = pd.to_numeric(enriched_df['days_diff'], downcast='integer')\n\n# If days_diff should only be between -3 and +3, filter the data:\nenriched_df = enriched_df[enriched_df['days_diff'].between(-3, 3)]\n\n# --- Instead of using the full dataset, take a subsample for estimation ---\n# Adjust the sample size as needed (here, 100000 observations are used)\nsample_size = 10000000\nenriched_sample = enriched_df.sample(n=sample_size, random_state=42)\n\n# --- Model Specification ---\n# Treat days_diff as categorical to generate dummy variables (7 dummies max)\nformula = \"\"\"cosine_similarity ~ C(days_diff) +\nC(outlet, Treatment('Other')) + \nC(month) + \nC(year)\n\"\"\"\n\n# Fit model on the subsample with robust standard errors\nmodel = smf.ols(formula, data=enriched_sample, missing='drop').fit(cov_type='HC3')\n\nprint(model.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:29:20.799072Z","iopub.execute_input":"2025-04-16T13:29:20.799495Z","iopub.status.idle":"2025-04-16T13:29:22.715001Z","shell.execute_reply.started":"2025-04-16T13:29:20.799462Z","shell.execute_reply":"2025-04-16T13:29:22.713615Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_72/3545730134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# --- Minimal changes to recode outlet ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# First, add 'Other' as a valid category, then recode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0menriched_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menriched_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0menriched_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0menriched_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_outlets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outlet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Other'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0menriched_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menriched_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_outlets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Other'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_create_delegator_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delegate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m_delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2941\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36madd_categories\u001b[0;34m(self, new_categories)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0malready_included\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_categories\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malready_included\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1331\u001b[0m                 \u001b[0;34mf\"new categories must not include old categories: {already_included}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: new categories must not include old categories: {'Other'}"],"ename":"ValueError","evalue":"new categories must not include old categories: {'Other'}","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.formula.api as smf\n\n# --- Data Preparation ---\n# Convert to categorical types upfront\nenriched_df = enriched_df.astype({\n    'outlet': 'category',\n    'news_date': 'datetime64[ns]',\n    'speech_date': 'datetime64[ns]'\n})\n\n# Create temporal features without expanding memory\nenriched_df['month'] = enriched_df['news_date'].dt.month.astype('int8')\nenriched_df['year'] = enriched_df['news_date'].dt.year.astype('int16')\n\n# Filter to top outlets based on frequency\ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()\n\n# --- Minimal change to solve the error ---\n# First add 'Other' as a valid category:\nenriched_df['outlet'] = enriched_df['outlet'].cat.add_categories('Other')\n# Now recode any outlet not in top_outlets to 'Other'\nenriched_df.loc[~enriched_df['outlet'].isin(top_outlets), 'outlet'] = 'Other'\n# Optionally, reset the category order:\nenriched_df['outlet'] = enriched_df['outlet'].cat.set_categories(top_outlets + ['Other'])\n\n# Downcast numerical columns for efficiency\nenriched_df['cosine_similarity'] = pd.to_numeric(enriched_df['cosine_similarity'], downcast='float')\nenriched_df['days_diff'] = pd.to_numeric(enriched_df['days_diff'], downcast='integer')\n\n# --- Model Specification ---\n# Treat days_diff as categorical to produce dummy variables (should be 7 dummies for -3,...,3)\nformula = \"\"\"cosine_similarity ~ C(days_diff) +\nC(outlet, Treatment('Other')) + \nC(month) + \nC(year)\n\"\"\"\n\n# Fit the model; missing='drop' automatically drops rows with NA values.\nmodel = smf.ols(formula, data=enriched_df, missing='drop').fit(cov_type='HC3')\n\nprint(model.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:04:16.252175Z","iopub.execute_input":"2025-04-16T13:04:16.252497Z","execution_failed":"2025-04-16T13:09:59.844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.formula.api as smf\n\n# --- Data Preparation ---\n# Convert to categorical types upfront\nenriched_df = enriched_df.astype({\n    'outlet': 'category',\n    'news_date': 'datetime64[ns]',\n    'speech_date': 'datetime64[ns]'\n})\n\n# Create temporal features without expanding memory\nenriched_df['month'] = enriched_df['news_date'].dt.month.astype('int8')\nenriched_df['year'] = enriched_df['news_date'].dt.year.astype('int16')\n\n# Filter to top outlets using category reordering:\ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()\n\n# Build a unique list of categories; if 'Other' is not already present, add it.\ncategories = top_outlets.copy()\nif 'Other' not in categories:\n    categories.append('Other')\n\n# Set the categories and recode non-top outlets to 'Other'\nenriched_df['outlet'] = enriched_df['outlet'].cat.set_categories(categories)\nenriched_df.loc[~enriched_df['outlet'].isin(top_outlets), 'outlet'] = 'Other'\n\n# Downcast numerical columns\nenriched_df['cosine_similarity'] = pd.to_numeric(\n    enriched_df['cosine_similarity'], \n    downcast='float'\n)\nenriched_df['days_diff'] = pd.to_numeric(\n    enriched_df['days_diff'], \n    downcast='integer'\n)\n\n# --- Model Specification ---\n# Use formula API with categorical variables\nformula = \"\"\"cosine_similarity ~ days_diff +\nC(outlet, Treatment('Other')) + \nC(month) + \nC(year)\n\"\"\"\n\n# Fit model with reduced memory footprint\nmodel = smf.ols(\n    formula, \n    data=enriched_df,\n    missing='drop'  # Automatically drops NA rows\n).fit(cov_type='HC3')\n\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}