{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.listdir('/kaggle/input')\nembeddings_path = \"/kaggle/input/embeddings\"\nprint(os.listdir(embeddings_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:50:29.769601Z","iopub.execute_input":"2025-02-09T21:50:29.769907Z","iopub.status.idle":"2025-02-09T21:50:29.778168Z","shell.execute_reply.started":"2025-02-09T21:50:29.769884Z","shell.execute_reply":"2025-02-09T21:50:29.777539Z"}},"outputs":[{"name":"stdout","text":"['speeches_with_embeddings.csv', 'news_with_embeddings.csv']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Similarity algorithm efficiently ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nspeeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\nnews_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n\n# Display first few rows\nprint(\"News Data:\")\nprint(news_embeddings.head())\n\nprint(\"\\nSpeeches Data:\")\nprint(speeches_embeddings.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:50:33.241347Z","iopub.execute_input":"2025-02-09T21:50:33.241693Z","iopub.status.idle":"2025-02-09T21:55:35.583021Z","shell.execute_reply.started":"2025-02-09T21:50:33.241668Z","shell.execute_reply":"2025-02-09T21:55:35.582093Z"}},"outputs":[{"name":"stdout","text":"News Data:\n          Index                                               Link  \\\n0  1_01_12_2018  https://www.bbc.com/mundo/noticias-america-lat...   \n1  2_01_12_2018  https://politica.expansion.mx/presidencia/2018...   \n2  3_01_12_2018  https://oem.com.mx/elsoldemexico/mexico/en-don...   \n3  4_01_12_2018  https://politica.expansion.mx/presidencia/2018...   \n4  5_01_12_2018  https://www.eleconomista.com.mx/politica/Nicol...   \n\n                                              Domain  \\\n0  BBC\\nToma de protesta de AMLO: las 5 tradicion...   \n1  Expansión Política\\nAMLO rinde protesta y prom...   \n2  El Sol de México\\n¿Hay Ley Seca este 1 de dici...   \n3  Expansión Política\\nAMLO es un \"líder persiste...   \n4  El Economista\\nNicolás Maduro llega a Palacio ...   \n\n                                               Title        Date  \\\n0  Toma de protesta de AMLO: las 5 tradiciones qu...  2018-12-01   \n1        AMLO rinde protesta y promete no reelegirse  2018-12-01   \n2  ¿Hay Ley Seca este 1 de diciembre por cambio d...  2018-12-01   \n3  AMLO es un \"líder persistente\", dice la superc...  2018-12-01   \n4  Nicolás Maduro llega a Palacio Nacional; no as...  2018-12-01   \n\n                                             Content month_abbr  \\\n0  Fuente de la imagen, Getty Images Desde su cam...        dic   \n1  Síguenos en nuestras redes sociales: CIUDAD DE...        dic   \n2  Por la toma de posesión de Andrés Manuel López...        dic   \n3  Síguenos en nuestras redes sociales: CIUDAD DE...        dic   \n4  Lectura 3:00 min Nicolás Maduro arribó este sá...        dic   \n\n                                   processed_content  \\\n0  fuente imagen getty images campaña presidencia...   \n1  síguenos redes sociales ciudad méxico adnpolít...   \n2  toma posesión andrés manuel lópez obrador pres...   \n3  síguenos redes sociales ciudad méxico adnpolít...   \n4  lectura 3:00 min nicolás maduro   arribó sábad...   \n\n                                         news_chunks  \\\n0  fuente imagen getty images campaña presidencia...   \n1  síguenos redes sociales ciudad méxico adnpolít...   \n2  toma posesión andrés manuel lópez obrador pres...   \n3  síguenos redes sociales ciudad méxico adnpolít...   \n4  lectura 3:00 min nicolás maduro   arribó sábad...   \n\n                                     news_embeddings  \n0  [ 3.77706587e-01  9.13102329e-02 -1.38176888e-...  \n1  [ 2.61866331e-01  2.99132258e-01  1.76378831e-...  \n2  [ 5.06281674e-01  3.32773924e-02 -3.04715186e-...  \n3  [ 3.07641208e-01  7.52940923e-02 -5.62011823e-...  \n4  [ 3.04801702e-01  3.39728445e-01  3.13091815e-...  \n\nSpeeches Data:\n   Unnamed: 0  X  speech_id  \\\n0           1  1          1   \n1           1  1          1   \n2           1  1          1   \n3           1  1          1   \n4           1  1          1   \n\n                                               title  \\\n0  Versión estenográfica de la conferencia de pre...   \n1  Versión estenográfica de la conferencia de pre...   \n2  Versión estenográfica de la conferencia de pre...   \n3  Versión estenográfica de la conferencia de pre...   \n4  Versión estenográfica de la conferencia de pre...   \n\n                                                urls  \\\n0  https://lopezobrador.org.mx/2024/01/09/version...   \n1  https://lopezobrador.org.mx/2024/01/09/version...   \n2  https://lopezobrador.org.mx/2024/01/09/version...   \n3  https://lopezobrador.org.mx/2024/01/09/version...   \n4  https://lopezobrador.org.mx/2024/01/09/version...   \n\n                                             content        date  \\\n0  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n1  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n2  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n3  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n4  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n\n                                       speech_chunks  \\\n0  2024: Año de Felipe Carrillo Puerto, benemérit...   \n1  Son los tres informes. Pero antes quiero dar a...   \n2  Entonces, ya voy a estar en TikTok y quiero in...   \n3  Entonces, ofrecer una disculpa y enviarle un a...   \n4  Me sumo al deseo de este año que sea lo mejor ...   \n\n                                   speech_embeddings  \n0  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n1  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n2  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n3  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n4  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(type(news_embeddings['news_embeddings'].iloc[0]))\n#since it is a class 'str' then we need to convert the embedding as a numpy array\n# Convert 'date' column to datetime if it is not already in datetime format\nspeeches_embeddings['date'] = pd.to_datetime(speeches_embeddings['date'], errors='coerce')\nnews_embeddings['Date'] = pd.to_datetime(news_embeddings['Date'], errors='coerce')\n\n# Subset the data to 2019 and reset indices\n\nnews_2019_reset = news_embeddings[news_embeddings['Date'].dt.year == 2019].reset_index(drop=True)\nspeeches_2019_reset = speeches_embeddings[speeches_embeddings['date'].dt.year == 2019].reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:55:35.583958Z","iopub.execute_input":"2025-02-09T21:55:35.584253Z","iopub.status.idle":"2025-02-09T21:55:35.644130Z","shell.execute_reply.started":"2025-02-09T21:55:35.584230Z","shell.execute_reply":"2025-02-09T21:55:35.643463Z"}},"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\n\n# Function to clean and convert the embeddings\ndef clean_and_convert(embedding_str):\n    # Remove any unwanted characters like brackets or newline characters\n    cleaned_str = embedding_str.replace('[', '').replace(']', '').replace('\\n', '')\n    \n    # Convert the cleaned string to a numpy array\n    return np.array(cleaned_str.split()).astype(float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:55:35.650760Z","iopub.execute_input":"2025-02-09T21:55:35.651021Z","iopub.status.idle":"2025-02-09T21:55:35.665122Z","shell.execute_reply.started":"2025-02-09T21:55:35.651003Z","shell.execute_reply":"2025-02-09T21:55:35.664306Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch \n\n# Step 2: Clean and convert the embeddings for 2019 data\ncleaned_news_embeddings_2019 = news_2019_reset['news_embeddings'].apply(clean_and_convert)\ncleaned_speech_embeddings_2019 = speeches_2019_reset['speech_embeddings'].apply(clean_and_convert)\n\n# Verify the cleaned embeddings\nprint(\"Cleaned news embedding (first sample) for 2019:\", cleaned_news_embeddings_2019.iloc[0])\nprint(\"Cleaned speech embedding (first sample) for 2019:\", cleaned_speech_embeddings_2019.iloc[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:55:35.665934Z","iopub.execute_input":"2025-02-09T21:55:35.666200Z","iopub.status.idle":"2025-02-09T21:55:52.557275Z","shell.execute_reply.started":"2025-02-09T21:55:35.666174Z","shell.execute_reply":"2025-02-09T21:55:52.556358Z"}},"outputs":[{"name":"stdout","text":"Cleaned news embedding (first sample) for 2019: [ 7.00545609e-02  1.59867704e-01  1.82033628e-01 -1.10902913e-01\n -1.68071046e-01  7.82872558e-01  1.24405897e+00  9.35690045e-01\n -8.79230499e-01  1.69901311e-01  3.14941794e-01 -8.36316347e-01\n  1.94153115e-01 -1.05180137e-01 -1.43057257e-01 -4.09085423e-01\n  4.37348098e-01  3.12500400e-03  5.30271530e-01 -4.83261496e-02\n -7.44194686e-01  5.34093797e-01  6.49602234e-01 -1.16227746e+00\n -8.81177306e-01 -6.07623994e-01  7.66389966e-01  3.55166703e-01\n -4.08001542e-01  5.95054999e-02 -4.70891893e-01 -3.36702853e-01\n  2.59495556e-01 -8.38763833e-01 -2.42981687e-01 -3.25389415e-01\n -3.09586227e-01  3.96134198e-01  4.72463161e-01 -6.93257987e-01\n -3.02345544e-01 -5.06430805e-01  3.93638819e-01  8.52328613e-02\n  5.45643926e-01  6.85154796e-01 -1.47084430e-01  2.22051278e-01\n -5.23011625e-01 -5.16374409e-01  1.38684058e+00 -5.68437874e-01\n -4.49357063e-01 -1.91186011e-01 -2.05529436e-01 -5.80088556e-01\n -2.78938144e-01  3.71591657e-01  1.32135594e+00 -3.93423080e-01\n  5.91218293e-01 -1.77500531e-01 -4.71017271e-01  8.30251276e-01\n  3.77016626e-02 -4.15873706e-01  1.39194280e-01  1.76936910e-01\n  5.42509019e-01  3.73453826e-01  4.64951843e-01  5.20026445e-01\n -4.33932424e-01 -8.87546688e-02  5.91341674e-01  3.39532673e-01\n -1.00013208e+00 -5.11439204e-01 -2.37501815e-01 -5.03521681e-01\n  1.95344344e-01  1.48172751e-01 -1.27406740e+00  2.96013772e-01\n  1.37470990e-01 -7.37873971e-01  5.42951584e-01 -2.64206380e-01\n -5.00378013e-01  8.06588158e-02 -8.78424466e-01  4.06067848e-01\n  3.28525864e-02 -1.11642814e+00 -3.39013100e-01 -1.15139432e-01\n -1.93780497e-01  1.06406188e+00  7.50209466e-02 -3.79035860e-01\n -2.84452379e-01  1.43379599e-01 -6.10554069e-02 -3.44294101e-01\n  1.68946013e-01 -2.60313064e-01 -1.35613844e-01 -1.33293462e+00\n  5.84116280e-01  3.13710004e-01 -4.45829839e-01 -1.02921641e+00\n -1.02659380e+00 -1.91787779e-01 -5.67015469e-01  4.03581470e-01\n -1.06792784e+00  5.84999204e-01 -3.82935941e-01 -9.10328984e-01\n  4.05314922e-01  7.82228470e-01 -5.40634930e-01 -4.75355200e-02\n  2.78004825e-01  3.45711261e-01 -7.97916055e-01 -1.00254083e+00\n  6.29943728e-01 -5.38320482e-01  1.72983274e-01  1.18940100e-01\n  1.57959238e-01 -2.47232527e-01 -1.75598532e-01 -9.38921392e-01\n  4.42518070e-02 -1.60576086e-02  5.50408125e-01 -1.12101227e-01\n  5.40541828e-01 -6.19187057e-01 -6.83623314e-01  4.77961391e-01\n  7.77637243e-01 -2.10982308e-01 -2.37795189e-02 -9.85614419e-01\n -2.35323727e-01 -1.19362324e-01  1.50125533e-01 -5.31431258e-01\n -2.07525603e-02  1.00758612e+00 -1.84075996e-01  2.63608783e-01\n  1.19224258e-01  6.77562118e-01  9.07747388e-01  8.82469952e-01\n -1.69458687e-01 -7.76201904e-01  2.24691838e-01 -4.15196568e-02\n -3.13760132e-01 -3.30846757e-01 -2.35710308e-01  5.31975448e-01\n -7.62136877e-01  6.65780008e-01  3.57742816e-01 -7.28799328e-02\n  5.82452893e-01 -4.62876767e-01  2.11503133e-01 -9.88741577e-01\n -5.74582517e-01  7.82960877e-02 -3.30634892e-01  3.84477437e-01\n -1.21001132e-01 -4.15954255e-02 -4.88717258e-01  2.62091756e-01\n  2.13869959e-01 -5.70339635e-02  4.55501765e-01 -9.51610327e-01\n  1.43306628e-01  1.14742897e-01 -9.33191657e-01 -5.22617877e-01\n  8.38218033e-01  5.16620040e-01 -2.97304153e-01  4.43607897e-01\n -1.19068599e+00 -9.70718384e-01  4.22027335e-02  3.75130385e-01\n -2.10221969e-02  1.75287500e-01 -5.52946031e-01 -2.58815587e-01\n  4.99703467e-01  4.07933503e-01  4.44831729e-01  2.06948578e-01\n -6.61076963e-01  8.51908267e-01  1.36177689e-01  4.84431326e-01\n  3.76356751e-01  1.50025427e-01  4.68409181e-01 -4.50008065e-01\n  9.27244965e-03  1.92244992e-01 -6.23408616e-01  8.61206949e-01\n -3.40539366e-01  2.00500512e+00  4.76412207e-01 -3.80144507e-01\n -1.67422935e-01 -5.40994406e-01  3.62269551e-01  8.80210578e-01\n -1.13491893e+00 -4.80143517e-01  1.81830928e-01 -5.01913965e-01\n -4.05449793e-02 -4.58803356e-01  5.15447319e-01  1.74552143e-01\n  1.01556087e+00  1.21529035e-01  9.23579186e-02 -5.62776923e-01\n -4.63043451e-02  1.33957696e+00 -2.48178571e-01  2.98234761e-01\n -3.25338870e-01 -1.90970629e-01 -1.11855693e-01 -5.21327078e-01\n  9.56922024e-02  8.34082484e-01 -7.74354756e-01 -3.35030675e-01\n -4.70004648e-01 -4.09925848e-01 -7.70964473e-02 -7.16523677e-02\n  5.21982253e-01  1.69262156e-01  3.17660451e-01 -2.68320709e-01\n -7.47242987e-01  2.75001079e-01  4.51686472e-01  4.33566093e-01\n  4.74514812e-01  4.76680398e-01 -1.71513259e-01 -7.81738579e-01\n  5.32978535e-01  8.36001709e-02  2.77568251e-01 -7.77093947e-01\n  1.63538766e+00 -1.99993417e-01  2.50158340e-01 -1.52399644e-01\n -4.98224825e-01 -9.26380828e-02 -4.84432608e-01 -1.73177868e-01\n -7.28748292e-02  4.21243578e-01 -8.82581025e-02  9.55562443e-02\n -3.44158471e-01 -3.51588219e-01  3.94937433e-02 -8.10794413e-01\n -4.50755894e-01  8.16952586e-01 -8.07392359e-01  9.12323177e-01\n  7.14291513e-01 -1.87647447e-01 -1.14167638e-01 -3.03337872e-01\n -3.85899425e-01 -6.74774766e-01  6.62950516e-01  5.13363183e-01\n -5.80862761e-01  1.32814085e+00 -1.80596566e+00 -5.14439285e-01\n -5.60075939e-01 -9.01904523e-01 -1.15730271e-01  6.63569033e-01\n -2.59174228e-01  1.19894624e-01 -7.46804118e-01  1.54247928e+00\n -5.05906194e-02 -1.01841247e+00 -2.84190010e-02 -1.13796413e+00\n  2.92263925e-01 -6.53710514e-02 -5.19987345e-01 -4.97896940e-01\n -1.02409315e+00 -1.98811516e-01 -6.79709017e-01 -3.23882669e-01\n  5.69296675e-03  6.80906057e-01  1.44124293e+00  7.40423560e-01\n -3.72002780e-01 -3.35392773e-01 -1.21482122e+00 -9.21337187e-01\n -5.28282166e-01  4.63806063e-01  9.51264054e-02  4.13240165e-01\n -8.60281438e-02 -8.68723810e-01  5.30096069e-02 -1.74992517e-01\n -3.04628816e-02 -2.49173269e-01  3.40887845e-01 -4.10111517e-01\n -4.55511324e-02 -1.83332190e-01  3.98536354e-01  8.54127631e-02\n  6.07205331e-01  8.34877253e-01  4.69316334e-01  4.81986523e-01\n -4.96327758e-01 -5.83996952e-01 -3.11640084e-01 -2.71250695e-01\n -1.80943366e-02  1.59120277e-01  8.17025155e-02  1.31359454e-02\n -9.17105377e-01 -3.88489276e-01 -1.53520644e-01  3.59127015e-01\n -7.99225941e-02 -1.25398362e+00 -1.05013001e+00 -6.76200569e-01\n -7.73809254e-01  7.06071496e-01 -3.18351179e-01 -4.11179215e-01\n -1.36477304e+00 -9.03374612e-01 -6.84899509e-01  6.99066281e-01\n -5.07439792e-01 -6.80720508e-02 -3.25246342e-02  3.87150824e-01\n -3.49093974e-01 -9.27359223e-01 -3.86372656e-02 -6.50337100e-01\n -1.06133616e+00  6.25220016e-02  9.78607297e-01 -6.60677075e-01\n  6.11327469e-01  1.21807134e+00  2.83671133e-02  5.71117461e-01\n  3.52826506e-01 -9.11087915e-02 -4.57611531e-01 -5.69572568e-01\n  1.01683295e+00  9.17198896e-01 -7.93648660e-01 -5.33728659e-01\n  4.41540331e-01 -6.08365119e-01 -1.47961184e-01  1.01725852e+00\n -3.81816924e-01 -5.17795905e-02  9.13214386e-01  2.34120086e-01\n -7.42669940e-01 -2.44978622e-01  2.66912401e-01  3.69217753e-01\n -1.43081412e-01  7.89198160e-01 -6.62574470e-01 -1.40923560e-01\n  1.38224864e+00  1.80390522e-01 -2.75198489e-01  1.99729666e-01\n  6.76637650e-01  3.93991292e-01 -3.80579829e-01 -4.66710925e-01\n  6.88362896e-01 -1.06553507e+00 -3.63880485e-01 -1.01454699e+00\n -2.49382004e-01  1.03708364e-01  3.80381972e-01  1.18529403e+00\n -2.67564356e-01  2.81773716e-01 -4.45182264e-01 -2.38691583e-01\n -9.32468057e-01  3.07960510e-01  4.72051233e-01 -1.39529204e+00\n  2.97239751e-01  1.32097781e+00  2.50222504e-01 -6.19715989e-01\n  9.98949781e-02  2.37908676e-01  1.12489961e-01 -2.62433022e-01\n  2.58603513e-01 -9.46418762e-01 -5.39818883e-01 -1.41488087e+00\n -4.36936826e-01  3.30912620e-01  1.26792765e+00  1.37123871e+00\n  2.28918418e-01  9.67668220e-02 -4.48120058e-01 -5.17894566e-01\n -2.01261476e-01  6.31380618e-01  6.29380226e-01  2.65654266e-01\n  3.19612063e-02 -4.67760086e-01 -1.19920909e+00 -7.52961695e-01\n -1.28557992e+00  6.75429642e-01 -1.70251280e-01  4.85161901e-01\n  1.28285933e+00  3.64779025e-01  7.44648129e-02 -3.14747483e-01\n  7.66503632e-01  2.79432476e-01  1.41185358e-01 -3.84736896e-01\n -2.01427713e-01  6.18846118e-01 -6.94713175e-01 -2.03622848e-01\n  1.68582439e-01  5.43199420e-01  3.49682607e-02  2.20798939e-01\n -3.52775194e-02  1.08235851e-01 -4.26949084e-01  2.78627664e-01\n -2.97579408e-01 -9.36615050e-01 -1.82442740e-02 -4.13540334e-01\n -5.52273035e-01 -1.17501605e+00  1.41861305e-01 -1.80055425e-01\n  2.35304430e-01  1.71761364e-01 -5.92664123e-01  4.20621574e-01\n  6.53679788e-01  3.72438490e-01  2.94015199e-01  1.83444113e-01\n -2.04771832e-01 -9.68716443e-01 -8.45224738e-01 -1.23450477e-02\n  4.00130659e-01  1.70618400e-01 -9.32044983e-01  2.07366243e-01\n  3.18258077e-01 -2.45351776e-01  1.83669567e-01  1.11018591e-01\n -7.30564117e-01  1.46111175e-01  1.61058292e-01 -7.83114210e-02\n  1.85037956e-01 -8.04000199e-01  4.99850094e-01  1.65931746e-01\n  5.25432825e-01  2.06993714e-01 -6.07728541e-01 -3.75429571e-01\n  1.07070155e-01 -2.33780831e-01 -2.75202751e-01 -3.35936934e-01\n -1.83023393e+00  2.78325558e-01 -5.57318702e-02  4.82635230e-01\n -2.39613075e-02  2.32229903e-01  4.41217542e-01 -1.49611640e+00\n  9.22068000e-01 -2.98000067e-01 -3.49729359e-01 -2.89400697e-01\n  4.69858110e-01 -2.25061595e-01  2.01310381e-01 -6.62217319e-01\n  1.82589754e-01  3.29430908e-01  5.25745936e-03 -6.66362345e-01\n  3.41242671e-01  7.68833041e-01  2.69588685e+00 -3.46035101e-02\n  2.43602946e-01 -1.59214899e-01 -3.15767318e-01 -2.64736325e-01\n  9.43682641e-02  1.60262728e+00  3.99537057e-01  1.18481480e-01\n -2.38074839e-01  3.87066416e-02  1.90582559e-01 -2.99117208e-01\n  1.31183535e-01 -7.58124739e-02 -4.13423985e-01 -5.41459799e-01\n  7.77557343e-02 -1.70172453e-01  1.82046726e-01  3.02490175e-01\n  3.21878880e-01  3.69078457e-01  2.61017382e-01 -1.30507305e-01\n -6.78917468e-01  1.60560536e+00  1.76386327e-01  2.10639551e-01\n  2.06798017e-01  2.87298620e-01 -7.22193003e-01 -1.30883992e-01\n  4.83974159e-01  3.05974693e-03 -1.04988861e+00  3.46358746e-01\n -1.45929396e-01 -3.27217042e-01  3.08789611e-01 -7.53104568e-01\n -4.56486315e-01 -1.35423750e-01 -2.98468769e-01  4.44478869e-01\n  2.50072360e-01 -3.44279706e-01  1.03972249e-01 -1.10048354e+00\n  6.29754141e-02  7.79528201e-01 -3.74706596e-01  1.05145526e+00\n -2.97881305e-01  9.12346542e-01 -3.66157651e-01 -3.72374088e-01\n -1.44030843e-02 -6.26156747e-01  2.41996441e-03 -7.95540750e-01\n  7.49281466e-01 -1.61380678e-01  1.80280685e-01 -1.03528297e+00\n  4.63224918e-01  5.48727512e-02 -4.60319519e-01 -3.01051974e-01\n  2.28658140e-01 -7.98994958e-01  6.23479486e-01 -9.63411868e-01\n -2.13218376e-01 -3.12885284e-01  4.99626517e-01 -4.96566072e-02\n  8.39714170e-01 -1.57453984e-01 -5.65470517e-01 -7.50142157e-01\n  7.11373568e-01  2.44681165e-01  3.03114653e-02 -1.83623955e-02\n  6.97169840e-01  5.61765432e-01  5.23256779e-01 -3.62924963e-01\n  4.57713604e-02  1.89913645e-01 -2.64371112e-02  6.76903903e-01\n -5.81403911e-01 -5.17929912e-01 -1.10710613e-01 -2.68119331e-02\n -1.13481307e+00 -7.09140003e-01 -2.11360063e-02 -3.77147824e-01\n -5.94538271e-01  1.37229383e-01 -7.98546791e-01  7.58118808e-01\n  3.35663229e-01 -1.35139704e-01  1.47011997e-02 -2.77573407e-01\n -5.33618212e-01  1.07233572e+00 -6.79892242e-01 -7.17613876e-01\n -1.98805556e-01 -3.70668739e-01  1.00695126e-01 -5.10370672e-01\n  2.34905913e-01  2.25374624e-02 -9.92488027e-01 -1.16836376e-01\n -2.88288474e-01  2.77127951e-01 -1.91706177e-02  8.91401529e-01\n -1.35173261e-01 -7.29829967e-02  1.65261269e-01  2.96182558e-02\n  7.31315732e-01  7.48109818e-01  6.02842867e-01 -1.20826137e+00\n  2.92348295e-01 -6.01629764e-02  3.48665148e-01  5.20410001e-01\n  4.63558942e-01 -2.47183934e-01  9.90757823e-01  9.09993112e-01\n -4.15960580e-01  4.07974631e-01 -2.29051113e-01 -1.02901781e+00\n -1.45069256e-01 -5.84207475e-01 -3.47165734e-01 -6.05004802e-02\n -1.25394547e+00 -2.32788756e-01  6.44387007e-01  1.18787813e+00\n  6.14667654e-01 -9.93005693e-01  5.97374082e-01  1.64877921e-01\n -4.95244972e-02 -3.84942204e-01 -1.04669309e+00 -4.67761099e-01\n  4.28800195e-01 -2.78436333e-01  1.16195655e+00 -7.98883140e-01\n -2.00608000e-01  4.14604574e-01 -7.54731119e-01  2.43484721e-01\n -3.61172855e-01 -3.00707191e-01 -2.33060643e-01 -1.86495692e-01\n  7.17402339e-01  1.80395901e-01  1.09552956e+00  1.27748519e-01\n  8.59787583e-01 -5.46323597e-01 -1.01626003e+00  1.50024641e+00\n -3.98237079e-01  4.76250499e-01  3.70612144e-01 -2.86968350e-01\n -2.00468168e-01  1.23338224e-02 -3.97910446e-01  6.52029738e-02\n -4.53166336e-01 -5.05009107e-02  9.52406108e-01 -8.11910555e-02\n -3.07212055e-01 -5.78733981e-01 -4.35164511e-01 -4.84922349e-01\n -6.39488757e-01  2.80574679e-01 -2.19898019e-02  8.16481650e-01\n  4.59178030e-01  5.42555869e-01 -3.25571835e-01  1.25180691e-01\n -2.30748311e-01 -3.99562985e-01 -9.37617794e-02 -1.07474569e-02]\nCleaned speech embedding (first sample) for 2019: [-3.44920129e-01 -3.09315979e-01  7.77079724e-03 -2.37565227e-02\n -3.81232023e-01 -1.78216636e-01  7.30504155e-01  1.41300708e-01\n  2.89998233e-01  1.58268232e-02  1.40173271e-01  5.35930872e-01\n -4.27965373e-01  5.88194966e-01 -2.72140712e-01 -3.89662683e-01\n  1.88075632e-01 -6.97959542e-01  4.33664709e-01 -1.33600205e-01\n -1.12664416e-01 -2.12935179e-01 -1.12163469e-01  2.93019656e-02\n -2.53637612e-01 -4.21316534e-01  4.20284748e-01 -3.00766192e-02\n -1.13802224e-01 -2.08832726e-01  2.80815661e-01  2.57724017e-01\n  2.63356492e-02  3.91200334e-01 -2.31971085e-01 -2.82654166e-01\n  1.81134209e-01  3.79516900e-01 -1.08381160e-01  2.94214964e-01\n  4.37536016e-02 -7.27685511e-01  1.23311877e-01 -1.31746024e-01\n -2.91791379e-01  2.79126596e-02  1.54115051e-01  7.00190544e-01\n  2.79327929e-01  1.03620835e-01 -6.34254336e-01  1.19979605e-01\n -4.92771268e-01 -1.67246789e-01  3.07608575e-01 -5.24735928e-01\n  3.20409894e-01 -1.61958009e-01  1.20293573e-01 -1.79414436e-01\n -5.99535406e-01  1.78896487e-01  1.02210328e-01 -3.08541954e-01\n -3.27447683e-01  1.10084750e-01  2.06161290e-02  1.11577407e-01\n -2.09626973e-01  2.56762564e-01 -4.08155434e-02 -3.34704876e-01\n -8.25005211e-03  1.71580851e-01 -2.45489120e-01 -4.03970003e-01\n  2.63861299e-01  1.01809829e-01  9.63111594e-02  4.91353512e-01\n  1.42706811e-01  2.40962952e-02  7.31722964e-03  4.08115685e-01\n -2.23576635e-01  3.21600348e-01  1.91981062e-01  2.46447399e-02\n -4.45800006e-01 -1.46720991e-01 -1.28053814e-01  3.60733122e-01\n  1.84853405e-01 -5.06678177e-03 -3.91891420e-01  8.30683857e-02\n  4.48252320e-01 -4.73063290e-01 -3.32966924e-01 -4.67484891e-01\n -4.46293294e-01 -1.28588947e-02 -2.24682111e-02  4.56992239e-01\n -5.77725805e-02 -2.30787739e-01  1.61857948e-01  9.34762776e-01\n -1.44809648e-01 -4.72620964e-01  1.65796071e-01  5.32034457e-01\n  5.13210654e-01 -6.78770021e-02  4.93222475e-01  1.22749053e-01\n  2.47913957e-01  4.69721049e-01  1.45805895e-01  3.37042123e-01\n  1.06815234e-01  1.64040066e-02 -1.08201832e-01  2.15466261e-01\n  2.44425088e-01 -7.65136480e-02  8.78887773e-02  5.76427653e-02\n -1.94094419e-01  3.63375276e-01 -3.67461860e-01 -2.75069594e-01\n  8.64026099e-02  1.48814797e-01 -2.74624348e-01  2.19830751e-01\n  6.10108152e-02 -2.37661898e-01 -2.70758748e-01 -3.71895075e-01\n -2.59500206e-01 -4.95443791e-01 -4.54375893e-01  4.77886319e-01\n -4.02520478e-01  4.54579527e-03 -1.91648424e-01  9.46951509e-02\n  6.45384073e-01 -7.49396393e-04  4.44123328e-01 -5.68615079e-01\n -9.22149867e-02 -2.95003891e-01 -1.30751804e-01  8.62842560e-01\n -1.43791974e-01  3.43460977e-01 -5.98494075e-02 -5.43242514e-01\n -6.58433974e-01 -1.96577329e-02  7.52477527e-01 -1.77065119e-01\n  4.45478618e-01 -3.30189824e-01 -2.77335823e-01 -1.74303055e-01\n -2.23946750e-01 -5.91348171e-01 -3.17246765e-02  2.34339982e-01\n -2.94784345e-02  1.12688869e-01 -5.02869999e-03 -5.56110293e-02\n  1.10744357e-01 -1.05861366e-01  6.27013922e-01 -2.26914451e-01\n  1.59019232e-01 -5.29540956e-01  3.58952545e-02 -1.61074847e-01\n  1.54616505e-01 -1.57880738e-01  5.64703524e-01 -5.43099225e-01\n -9.47556645e-02  3.81139308e-01  3.92481983e-01  2.76672274e-01\n -2.72862971e-01 -2.00111233e-02  1.78307146e-01 -1.99569345e-01\n -1.31860077e-01 -3.28659922e-01  2.71255970e-01 -7.80980065e-02\n  8.68114755e-02  6.05213404e-01 -1.30785093e-01 -8.30544084e-02\n  3.16682339e-01  5.28794229e-01  1.12171382e-01  1.42292663e-01\n -2.24140763e-01  1.35929897e-01  6.22393787e-01  8.92352909e-02\n -6.82668626e-01  1.81623757e-01 -5.14640808e-01 -1.35714635e-01\n -2.29713470e-02 -1.86629500e-03  5.32864332e-01 -4.98288237e-02\n  9.26251858e-02 -1.92340929e-02 -2.07185745e-01 -9.23817635e-01\n -1.36435494e-01 -2.43782103e-01 -4.56704229e-01  5.42804673e-02\n  3.10102165e-01 -1.28204867e-01 -3.42213809e-01  2.48799652e-01\n -3.28812212e-01 -4.92993802e-01 -3.92689526e-01 -5.80207527e-01\n  9.46623087e-01 -3.44671488e-01  4.60790336e-01  4.08961438e-04\n  4.50676158e-02  3.48848283e-01  2.37173047e-02  1.52416706e-01\n  5.88979959e-01  6.52223304e-02 -3.25104371e-02 -1.00739822e-01\n -1.72658056e-01  2.55886346e-01 -4.40914810e-01 -1.22861370e-01\n  4.58829254e-01 -6.63014770e-01  3.13072324e-01 -2.25492269e-01\n -6.31361365e-01  1.65353030e-01 -1.57239065e-01 -6.41775727e-01\n -1.24380216e-01 -3.30529436e-02 -6.60853833e-03  9.47406664e-02\n -1.05261337e-03  2.97691345e-01 -1.32792294e-01  2.15376392e-01\n -1.51064187e-01  5.96762359e-01 -5.07898092e-01 -3.68166596e-01\n  9.99191552e-02 -1.06711283e-01 -1.68561578e-01 -2.83410728e-01\n  5.70936948e-02 -1.11245327e-01  4.12440032e-01 -9.14343894e-01\n  3.88795674e-01 -1.11968331e-01  4.46390808e-01 -1.98887102e-02\n  1.53238073e-01 -9.55762193e-02  1.38868820e-02 -4.01798725e-01\n -6.29768491e-01  3.55450362e-02  2.58863531e-02  1.33891225e-01\n  1.86539650e-01 -5.76764584e-01 -5.66099763e-01  3.22173238e-01\n  2.89165020e-01  2.21779216e-02  4.07240421e-01  1.33155987e-01\n -4.80319083e-01 -3.75539362e-01  2.91073263e-01  1.67406425e-01\n  4.52864543e-02 -9.61711705e-02  3.08159024e-01  8.05683509e-02\n -2.94098556e-01  4.09790337e-01 -2.68932194e-01  1.83424950e-01\n  4.13392961e-01 -6.59272745e-02 -3.63719463e-03 -4.88713205e-01\n -2.68066347e-01 -6.13571882e-01  2.02297091e-01 -4.35442477e-01\n  5.78480244e-01 -6.86415508e-02 -4.59747016e-02 -2.45857999e-01\n  7.91438222e-02  4.27325256e-02  2.08407879e-01 -7.39058495e-01\n  4.56758022e-01  5.04199684e-01  7.88850114e-02  2.15781610e-02\n  3.62456739e-01  2.71571483e-02  9.20988321e-02 -2.71675646e-01\n -7.57629275e-01  2.81917095e-01 -1.17161334e-01 -6.03410602e-01\n -3.90970826e-01  2.49480247e-01  8.64316881e-01  1.35920513e-02\n -9.39342827e-02 -8.02451015e-01  6.07019961e-02 -2.12842166e-01\n -7.92280436e-02 -9.85326320e-02  5.23803495e-02 -3.25479329e-01\n  2.58021712e-01 -3.44549000e-01 -2.91191220e-01 -2.36556977e-01\n  3.85065913e-01 -5.82529783e-01 -3.71536255e-01  3.34082693e-02\n  1.60767734e-01  4.87873793e-01 -5.73086977e-01  6.21001303e-01\n -3.41462731e-01 -2.27574497e-01 -1.01631321e-01 -1.50513470e-01\n  6.41327947e-02  9.04622525e-02  3.14317286e-01  3.50994766e-01\n -1.08647726e-01 -1.24841489e-01 -1.87916219e-01  1.69790059e-01\n -6.97216168e-02 -9.81284678e-02  1.41995519e-01  4.43833411e-01\n -2.30681092e-01 -1.60661116e-01 -5.61099410e-01 -1.02958977e-01\n -1.05107009e-01  4.34427820e-02 -5.49827576e-01  2.04199851e-01\n  1.11308554e-03  5.31813741e-01  3.14695835e-01 -4.67939824e-01\n -2.47286037e-01  2.22859532e-01 -3.38092506e-01 -2.53212273e-01\n -1.78256080e-01 -6.23038188e-02  7.01602995e-02  1.34571627e-01\n  9.42353830e-02 -5.47822833e-01  7.76678771e-02 -5.28870940e-01\n -1.29945487e-01 -1.36676785e-02  2.97512114e-01  1.86098561e-01\n -3.48174393e-01  8.07317123e-02 -3.32805514e-01  3.08588654e-01\n  3.32802773e-01 -6.38814807e-01 -2.88325787e-01 -1.62053078e-01\n -4.94205564e-01 -3.12170029e-01 -2.36609101e-01 -3.96384597e-01\n -1.41320620e-02 -2.31964201e-01 -3.23148519e-01  1.41645402e-01\n -3.62912476e-01  2.93449283e-01 -1.80697694e-01  1.78915247e-01\n  4.86185580e-01 -1.62226751e-01  3.81451175e-02 -4.16886389e-01\n -2.89264228e-02 -2.69970059e-01  2.72571772e-01  2.95705855e-01\n -1.99652642e-01 -5.79130292e-01 -6.58791423e-01  1.58756822e-01\n -1.48827702e-01  1.02393955e-01  7.41601437e-02  1.49776354e-01\n  1.59197837e-01 -2.63527513e-01  3.15951377e-01 -3.99881750e-01\n -2.31423944e-01 -3.60437751e-01  9.67657715e-02 -4.20297921e-01\n -5.86767912e-01  1.35071844e-01  3.43300104e-01 -1.09085009e-01\n -2.26688772e-01  1.04860440e-01 -3.06360811e-01  4.11372900e-01\n -9.09575522e-02 -3.98478359e-01 -4.05430853e-01  3.70448738e-01\n  1.85491025e-01 -8.04468244e-02 -2.55198866e-01 -1.76044971e-01\n  3.36808324e-01  6.05063796e-01  3.35552633e-01 -3.99539992e-02\n  6.92300797e-01  8.39995444e-02 -2.92114884e-01 -1.68348670e-01\n  1.94218621e-01 -3.30632448e-01  9.73087270e-03 -1.63133681e-01\n -9.36624184e-02 -2.89722264e-01 -1.02846563e-01  7.61312544e-02\n -2.78679401e-01  5.29522419e-01  5.62272184e-02 -3.09010055e-02\n  1.85985491e-01 -2.26583794e-01 -1.78890109e-01 -3.67739797e-02\n  1.95543021e-01  1.58119380e-01 -3.92221928e-01 -5.04409850e-01\n  3.46392453e-01 -4.76277053e-01  3.51941586e-01 -6.71685755e-01\n  3.04447591e-01 -5.03618598e-01 -3.20359081e-01 -1.60645470e-02\n -5.19808233e-02 -5.03192544e-02  1.19307168e-01 -7.64923036e-01\n -1.80964530e-01 -1.87129647e-01  1.99156672e-01  1.76054969e-01\n  2.92057216e-01  1.19007573e-01 -1.60910338e-01  2.47577131e-01\n  3.10144126e-01  6.96670823e-03 -1.29211619e-01  6.63244486e-01\n -3.79970253e-01  1.24588246e-02 -2.76881039e-01 -2.23728884e-02\n -1.74835518e-01 -8.43280375e-01  7.15174556e-01 -2.27856174e-01\n -7.19903857e-02 -3.03132772e-01 -5.78473032e-01  1.52932763e-01\n  8.62729698e-02 -2.42392540e-01 -4.88193929e-01 -1.71882227e-01\n  3.52080941e-01 -3.16299908e-02 -1.10608973e-01  2.11909488e-01\n -2.24201940e-03  1.03175461e-01 -4.04245779e-02  1.22079514e-01\n  2.35412240e-01 -3.05362016e-01  3.12944680e-01 -1.03812227e-02\n  2.18378216e-01  2.79204249e-01  1.57669205e-02  4.97502349e-02\n -6.54077157e-02  2.83080600e-02 -4.13929522e-02 -4.13168371e-01\n  5.66544235e-02  3.69969197e-02 -3.57491112e+00 -2.51032799e-01\n -7.73357153e-02  3.52839828e-01 -2.23995626e-01 -2.10931003e-01\n -5.18271625e-02  1.77960873e-01 -1.66342437e-01 -6.02992326e-02\n -5.42608261e-01 -3.18625748e-01  1.92972273e-01  1.20200656e-01\n -2.26622552e-01  6.42068386e-01 -3.38068791e-02 -3.53030451e-02\n -2.29516476e-01 -2.96093114e-02 -1.84377566e-01 -7.63792694e-01\n  1.30060166e-02 -4.63537693e-01 -1.93543434e-01 -1.21721320e-01\n -5.43638170e-02  2.44306713e-01  2.76807100e-01 -2.56656945e-01\n  1.62510872e-01  5.72620451e-01  6.10996723e-01 -2.87369937e-01\n  2.12619796e-01 -6.99187815e-01 -1.03233688e-01  2.86342293e-01\n -3.18101078e-01 -3.01854342e-01 -3.00293088e-01 -4.24711369e-02\n  9.41998959e-02  6.04774654e-01 -7.26726651e-01  1.23555688e-02\n  1.73711032e-01  1.64301977e-01 -8.12287536e-03  3.29411149e-01\n  2.45639935e-01  3.47959101e-01 -2.61932611e-02 -1.72644600e-01\n -6.22223206e-02  3.01394403e-01  4.54509586e-01  6.37683272e-01\n  3.98974001e-01  4.93376821e-01 -1.68036848e-01 -5.46930060e-02\n  3.75374138e-01  3.22661936e-01 -1.14308000e-01  6.19810373e-02\n  8.55306834e-02 -5.69382980e-02 -3.45370285e-02 -2.97936767e-01\n  5.20089328e-01 -2.01014921e-01  3.40251923e-01 -3.16297799e-01\n -1.34120196e-01 -3.29988264e-02 -2.50180990e-01 -6.88479304e-01\n  5.32716990e-01 -2.61767536e-01 -6.56944335e-01  2.68150330e-01\n  1.14662722e-01 -2.82463372e-01  2.25032881e-01 -2.19777048e-01\n  1.32742263e-02  3.81479144e-01  3.78676414e-01 -9.52780768e-02\n -1.19399019e-01  1.65283084e-01 -6.01619929e-02 -1.10531896e-01\n -1.48926556e-01 -9.78880562e-03  5.21813750e-01 -3.83941114e-01\n  1.00921959e-01  1.24116972e-01  1.74460188e-01 -1.39745280e-01\n  2.51056492e-01 -4.41759825e-01 -4.24490720e-01 -1.07291922e-01\n  1.66903049e-01  3.73719454e-01  8.26290576e-04  1.79625824e-02\n -5.09062290e-01  9.64642167e-02 -2.24744409e-01  6.65148944e-02\n -2.68361032e-01 -4.82237227e-02  2.47000977e-02  1.79056376e-01\n  1.07046314e-01 -2.36909986e-01 -4.02292877e-01 -2.39158683e-02\n -2.56149098e-02 -3.51494730e-01 -9.08953026e-02  1.82657003e-01\n  6.26508355e-01 -1.17316850e-01 -5.76250136e-01 -6.23667479e-01\n  1.62598118e-01 -2.06881240e-01  1.49569511e-01 -1.98249286e-03\n  1.91342831e-02  1.59664378e-01  7.79405832e-02 -1.67004406e-01\n -7.01880157e-02  5.73302805e-02 -3.44996929e-01  7.38824904e-02\n -4.94104624e-01 -8.08202177e-02 -1.07208237e-01 -1.62808686e-01\n -6.64043427e-02 -4.82029766e-01 -2.29393333e-01 -2.12937489e-01\n  2.62966514e-01  2.58191407e-01  3.59597713e-01  7.52300143e-01\n -4.55324829e-01 -4.95545000e-01  1.17916286e-01  2.15304136e-01\n -3.47446442e-01 -2.70492435e-02 -1.31161362e-01 -6.93251416e-02\n  7.04796463e-02  1.06745899e-01  3.83910686e-01 -4.79858331e-02\n  4.83956456e-01  2.30391204e-01  3.23901266e-01  1.32584125e-01\n -2.21289158e-01  6.07195981e-02  1.15489781e-01  2.14343090e-02\n -1.87139541e-01 -2.79381335e-01  4.38312352e-01 -9.62983817e-04\n -2.15772707e-02 -1.83752656e-01 -1.83832854e-01  3.11846798e-03\n -3.50498676e-01 -1.38721541e-01 -1.85339987e-01 -3.57934117e-01\n -1.70851126e-06  2.18190122e-02 -2.88207352e-01  6.76543117e-02\n  2.04810828e-01 -4.22105193e-01  3.55254710e-01 -4.97303069e-01\n  8.93650949e-03 -1.20403074e-01 -4.38036203e-01  3.46718937e-01\n -1.06718510e-01 -9.40940455e-02 -5.39613031e-02 -5.29857576e-01\n  1.28139749e-01 -9.72311944e-02  2.48909593e-01 -4.10528295e-02\n  4.45431992e-02 -5.69641232e-01  2.36660808e-01  2.20604181e-01]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nnews_matrix_2019 = np.vstack(cleaned_news_embeddings_2019.values)\nspeech_matrix_2019 = np.vstack(cleaned_speech_embeddings_2019.values)\n\n\n# Convert NumPy arrays to PyTorch tensors and move to GPU\nnews_tensor_2019 = torch.tensor(news_matrix_2019, dtype=torch.float32).cuda()\nspeech_tensor_2019 = torch.tensor(speech_matrix_2019, dtype=torch.float32).cuda()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:55:52.558194Z","iopub.execute_input":"2025-02-09T21:55:52.558706Z","iopub.status.idle":"2025-02-09T21:55:52.996182Z","shell.execute_reply.started":"2025-02-09T21:55:52.558675Z","shell.execute_reply":"2025-02-09T21:55:52.995513Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport time\n\n# Step 1: Save tensors to disk before loading them\ntorch.save(news_tensor_2019.cpu(), '/kaggle/working/news_tensor_2019.pt')\ntorch.save(speech_tensor_2019.cpu(), '/kaggle/working/speech_tensor_2019.pt')\n\n# Step 2: Load tensors from disk and move them to GPU\nnews_tensor_2019 = torch.load('/kaggle/working/news_tensor_2019.pt').cuda()\nspeech_tensor_2019 = torch.load('/kaggle/working/speech_tensor_2019.pt').cuda()\n\n# Step 3: Compute cosine similarity in chunks\ndef compute_batch_cosine_similarity(embedding, batch_embeddings):\n    # Ensure embedding is 2D (batch_size, embedding_size)\n    embedding = embedding.unsqueeze(0) if embedding.dim() == 1 else embedding\n    batch_embeddings = batch_embeddings / batch_embeddings.norm(dim=1, keepdim=True)  # Normalize batch\n\n    # Normalize the embedding (1xembedding_size)\n    embedding = embedding / embedding.norm(dim=1, keepdim=True)  # Normalize single embedding\n    return torch.mm(batch_embeddings, embedding.T).squeeze()  # Cosine similarity\n\n\ndef compute_similarities_in_chunks(speech_tensor, news_tensor, chunk_size=1000):\n    similarities = []\n    for i in range(0, len(news_tensor), chunk_size):\n        chunk = news_tensor[i:i+chunk_size]\n        cosine_sim = compute_batch_cosine_similarity(speech_tensor, chunk)\n        similarities.extend(cosine_sim.cpu().tolist())\n    return similarities\n\n# Step 4: Process the tensor in chunks\nsimilarities_2019 = []\nstart_time = time.time()\n\nfor idx, speech_embedding_2019 in enumerate(speech_tensor_2019):\n    cosine_similarities = compute_similarities_in_chunks(speech_embedding_2019, news_tensor_2019)\n    \n    # Store results\n    for news_id, sim_value in enumerate(cosine_similarities):\n        similarities_2019.append({\n            'speech_id': idx,\n            'news_id': news_id,\n            'cosine_similarity': sim_value\n        })\n        \n    # Print progress\n    if idx % 1000 == 0:\n        elapsed_time = time.time() - start_time\n        remaining_time = (elapsed_time / (idx + 1)) * (len(speech_tensor_2019) - (idx + 1))\n        print(f\"Processed {idx + 1}/{len(speech_tensor_2019)} speeches. ETA: {remaining_time:.2f}s\")\n\n# Step 5: Save results to disk\nsimilarities_df_2019 = pd.DataFrame(similarities_2019)\nsimilarities_df_2019.to_parquet('/kaggle/working/similarities_2019.parquet')\n\nprint(\"Finished computing cosine similarities.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T21:55:52.996915Z","iopub.execute_input":"2025-02-09T21:55:52.997144Z","iopub.status.idle":"2025-02-09T21:56:53.290974Z","shell.execute_reply.started":"2025-02-09T21:55:52.997124Z","shell.execute_reply":"2025-02-09T21:56:53.289812Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-dc3944fe2d4e>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  news_tensor_2019 = torch.load('/kaggle/working/news_tensor_2019.pt').cuda()\n<ipython-input-9-dc3944fe2d4e>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  speech_tensor_2019 = torch.load('/kaggle/working/speech_tensor_2019.pt').cuda()\n","output_type":"stream"},{"name":"stdout","text":"Processed 1/32874 speeches. ETA: 7517.82s\nProcessed 1001/32874 speeches. ETA: 80.51s\nProcessed 2001/32874 speeches. ETA: 72.74s\nProcessed 3001/32874 speeches. ETA: 68.85s\nProcessed 4001/32874 speeches. ETA: 66.00s\nProcessed 5001/32874 speeches. ETA: 63.75s\nProcessed 6001/32874 speeches. ETA: 61.26s\nProcessed 7001/32874 speeches. ETA: 58.68s\nProcessed 8001/32874 speeches. ETA: 56.24s\nProcessed 9001/32874 speeches. ETA: 53.92s\nProcessed 10001/32874 speeches. ETA: 51.50s\nProcessed 11001/32874 speeches. ETA: 49.15s\nProcessed 12001/32874 speeches. ETA: 46.86s\nProcessed 13001/32874 speeches. ETA: 44.60s\nProcessed 14001/32874 speeches. ETA: 42.30s\nProcessed 15001/32874 speeches. ETA: 40.02s\nProcessed 16001/32874 speeches. ETA: 37.76s\nProcessed 17001/32874 speeches. ETA: 35.51s\nProcessed 18001/32874 speeches. ETA: 33.28s\nProcessed 19001/32874 speeches. ETA: 31.10s\nProcessed 20001/32874 speeches. ETA: 28.84s\nProcessed 21001/32874 speeches. ETA: 26.58s\nProcessed 22001/32874 speeches. ETA: 24.34s\nProcessed 23001/32874 speeches. ETA: 22.09s\nProcessed 24001/32874 speeches. ETA: 19.85s\nProcessed 25001/32874 speeches. ETA: 17.61s\nProcessed 26001/32874 speeches. ETA: 15.37s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-dc3944fe2d4e>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_embedding_2019\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_tensor_2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcosine_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarities_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_embedding_2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_tensor_2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-dc3944fe2d4e>\u001b[0m in \u001b[0;36mcompute_similarities_in_chunks\u001b[0;34m(speech_tensor, news_tensor, chunk_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_batch_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-dc3944fe2d4e>\u001b[0m in \u001b[0;36mcompute_batch_cosine_similarity\u001b[0;34m(embedding, batch_embeddings)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Normalize the embedding (1xembedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize single embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             )\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1671\u001b[0;31m def norm(  # noqa: F811\n\u001b[0m\u001b[1;32m   1672\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9}]}