{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully working scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time \n",
    "import random\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "chromedriver_path = r'C:\\Users\\luisf\\Desktop\\MDS_thesis\\scraper_gn\\chromedriver-win64\\chromedriver.exe'\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "search_query = \"Andrés Manuel López Obrador\"\n",
    "start_date = datetime.strptime(\"2019-12-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2019-12-02\",\"%Y-%m-%d\")\n",
    "max_pages = 1\n",
    "daily_limit=1000 \n",
    "\n",
    "current_date=start_date\n",
    "count=0\n",
    "current_page=1\n",
    "url = f'https://www.google.com/search?q={search_query}+after:{start_date}+before:{end_date}&tbm=nws&sort=date' #the &gl=mx establishes the region\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for extracting information url by url \n",
    "import requests\n",
    "\n",
    "def extract_full_text(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Try common tags where article content is stored\n",
    "        paragraphs = soup.find_all('p')  # Extract all paragraphs\n",
    "        full_text = \" \".join([p.get_text() for p in paragraphs])\n",
    "        return full_text.strip() if full_text else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article text from {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL for 2019-12-01: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-01+before:2019-12-02&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-01\n",
      "Error extracting article text from https://www.washingtonpost.com/es/post-opinion/2019/12/01/un-anio-de-amlo-lo-bueno-lo-malo-y-lo-feo/: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=10)\n",
      "Skipped article at https://www.washingtonpost.com/es/post-opinion/2019/12/01/un-anio-de-amlo-lo-bueno-lo-malo-y-lo-feo/ due to extraction error.\n",
      "Error extracting article text from https://www.france24.com/es/20191201-inseguridad-estancamiento-econ%C3%B3mico-y-apoyos-sociales-marcan-el-primer-a%C3%B1o-de-amlo: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191201-inseguridad-estancamiento-econ%C3%B3mico-y-apoyos-sociales-marcan-el-primer-a%C3%B1o-de-amlo\n",
      "Skipped article at https://www.france24.com/es/20191201-inseguridad-estancamiento-econ%C3%B3mico-y-apoyos-sociales-marcan-el-primer-a%C3%B1o-de-amlo due to extraction error.\n",
      "Finished scraping for 2019-12-01. Moving to the next day.\n",
      "Scraping URL for 2019-12-02: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-02+before:2019-12-03&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-02\n",
      "Finished scraping for 2019-12-02. Moving to the next day.\n",
      "Scraping URL for 2019-12-03: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-03+before:2019-12-04&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-03\n",
      "Finished scraping for 2019-12-03. Moving to the next day.\n",
      "Scraping URL for 2019-12-04: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-04+before:2019-12-05&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-04\n",
      "Finished scraping for 2019-12-04. Moving to the next day.\n",
      "Scraping URL for 2019-12-05: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-05+before:2019-12-06&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-05\n",
      "Error extracting article text from https://www.france24.com/es/20191206-andr%C3%A9s-manuel-l%C3%B3pez-obrador-pide-que-ee-uu-no-intervenga-en-m%C3%A9xico: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191206-andr%C3%A9s-manuel-l%C3%B3pez-obrador-pide-que-ee-uu-no-intervenga-en-m%C3%A9xico\n",
      "Skipped article at https://www.france24.com/es/20191206-andr%C3%A9s-manuel-l%C3%B3pez-obrador-pide-que-ee-uu-no-intervenga-en-m%C3%A9xico due to extraction error.\n",
      "Error extracting article text from https://tabasco.gob.mx/noticias/amlo-desde-tabasco-se-lograra-resurgimiento-de-industria-petrolera: HTTPSConnectionPool(host='tabasco.gob.mx', port=443): Max retries exceeded with url: /noticias/amlo-desde-tabasco-se-lograra-resurgimiento-de-industria-petrolera (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C24D4BE8D0>, 'Connection to tabasco.gob.mx timed out. (connect timeout=10)'))\n",
      "Skipped article at https://tabasco.gob.mx/noticias/amlo-desde-tabasco-se-lograra-resurgimiento-de-industria-petrolera due to extraction error.\n",
      "Finished scraping for 2019-12-05. Moving to the next day.\n",
      "Scraping URL for 2019-12-06: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-06+before:2019-12-07&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-06\n",
      "Error extracting article text from https://www.france24.com/es/20191206-andr%C3%A9s-manuel-l%C3%B3pez-obrador-pide-que-ee-uu-no-intervenga-en-m%C3%A9xico: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191206-andr%C3%A9s-manuel-l%C3%B3pez-obrador-pide-que-ee-uu-no-intervenga-en-m%C3%A9xico\n",
      "Skipped article at https://www.france24.com/es/20191206-andr%C3%A9s-manuel-l%C3%B3pez-obrador-pide-que-ee-uu-no-intervenga-en-m%C3%A9xico due to extraction error.\n",
      "Error extracting article text from https://tabasco.gob.mx/noticias/amlo-desde-tabasco-se-lograra-resurgimiento-de-industria-petrolera: HTTPSConnectionPool(host='tabasco.gob.mx', port=443): Max retries exceeded with url: /noticias/amlo-desde-tabasco-se-lograra-resurgimiento-de-industria-petrolera (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C24D824650>, 'Connection to tabasco.gob.mx timed out. (connect timeout=10)'))\n",
      "Skipped article at https://tabasco.gob.mx/noticias/amlo-desde-tabasco-se-lograra-resurgimiento-de-industria-petrolera due to extraction error.\n",
      "Error extracting article text from https://www.france24.com/es/20191207-trump-suspende-temporalmente-la-designaci%C3%B3n-de-los-c%C3%A1rteles-mexicanos-como-terroristas-1: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191207-trump-suspende-temporalmente-la-designaci%C3%B3n-de-los-c%C3%A1rteles-mexicanos-como-terroristas-1\n",
      "Skipped article at https://www.france24.com/es/20191207-trump-suspende-temporalmente-la-designaci%C3%B3n-de-los-c%C3%A1rteles-mexicanos-como-terroristas-1 due to extraction error.\n",
      "Finished scraping for 2019-12-06. Moving to the next day.\n",
      "Scraping URL for 2019-12-07: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-07+before:2019-12-08&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-07\n",
      "Error extracting article text from https://www.france24.com/es/20191207-trump-suspende-temporalmente-la-designaci%C3%B3n-de-los-c%C3%A1rteles-mexicanos-como-terroristas-1: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191207-trump-suspende-temporalmente-la-designaci%C3%B3n-de-los-c%C3%A1rteles-mexicanos-como-terroristas-1\n",
      "Skipped article at https://www.france24.com/es/20191207-trump-suspende-temporalmente-la-designaci%C3%B3n-de-los-c%C3%A1rteles-mexicanos-como-terroristas-1 due to extraction error.\n",
      "Error extracting article text from https://tabasco.gob.mx/noticias/recorre-amlo-plataforma-xikin-ubicado-en-aguas-del-golfo-de-mexico: HTTPSConnectionPool(host='tabasco.gob.mx', port=443): Max retries exceeded with url: /noticias/recorre-amlo-plataforma-xikin-ubicado-en-aguas-del-golfo-de-mexico (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C24DA5A930>, 'Connection to tabasco.gob.mx timed out. (connect timeout=10)'))\n",
      "Skipped article at https://tabasco.gob.mx/noticias/recorre-amlo-plataforma-xikin-ubicado-en-aguas-del-golfo-de-mexico due to extraction error.\n",
      "Finished scraping for 2019-12-07. Moving to the next day.\n",
      "Scraping URL for 2019-12-08: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-08+before:2019-12-09&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-08\n",
      "Skipped article at https://www.debate.com.mx/estados/Hijos-de-AMLO-incursionan-al-mercado-con-bebidas-a-base-de-cacao-20191208-0078.html due to extraction error.\n",
      "Finished scraping for 2019-12-08. Moving to the next day.\n",
      "Scraping URL for 2019-12-09: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-09+before:2019-12-10&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-09\n",
      "Error extracting article text from https://www.france24.com/es/20191210-m%C3%A9xico-ee-uu-y-canad%C3%A1-firman-el-acuerdo-t-mec-y-allanan-el-camino-para-su-ratificaci%C3%B3n: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191210-m%C3%A9xico-ee-uu-y-canad%C3%A1-firman-el-acuerdo-t-mec-y-allanan-el-camino-para-su-ratificaci%C3%B3n\n",
      "Skipped article at https://www.france24.com/es/20191210-m%C3%A9xico-ee-uu-y-canad%C3%A1-firman-el-acuerdo-t-mec-y-allanan-el-camino-para-su-ratificaci%C3%B3n due to extraction error.\n",
      "Skipped article at https://www.debate.com.mx/viral/Enfrentan-a-AMLO-musculoso-con-Felipe-Calderon-fortachon-en-redes-20191209-0145.html due to extraction error.\n",
      "Finished scraping for 2019-12-09. Moving to the next day.\n",
      "Scraping URL for 2019-12-10: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-10+before:2019-12-11&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-10\n",
      "Error extracting article text from https://www.france24.com/es/20191210-m%C3%A9xico-ee-uu-y-canad%C3%A1-firman-el-acuerdo-t-mec-y-allanan-el-camino-para-su-ratificaci%C3%B3n: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191210-m%C3%A9xico-ee-uu-y-canad%C3%A1-firman-el-acuerdo-t-mec-y-allanan-el-camino-para-su-ratificaci%C3%B3n\n",
      "Skipped article at https://www.france24.com/es/20191210-m%C3%A9xico-ee-uu-y-canad%C3%A1-firman-el-acuerdo-t-mec-y-allanan-el-camino-para-su-ratificaci%C3%B3n due to extraction error.\n",
      "Error extracting article text from https://confidencial.digital/opinion/presidentes-contra-la-prensa/: 403 Client Error: Forbidden for url: https://confidencial.digital/opinion/presidentes-contra-la-prensa/\n",
      "Skipped article at https://confidencial.digital/opinion/presidentes-contra-la-prensa/ due to extraction error.\n",
      "Skipped article at https://www.debate.com.mx/politica/Defiende-AMLO-pintura-de-Zapata-gay-artistas-tienen-libertad-20191211-0097.html due to extraction error.\n",
      "Finished scraping for 2019-12-10. Moving to the next day.\n",
      "Scraping URL for 2019-12-11: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-11+before:2019-12-12&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-11\n",
      "Skipped article at https://www.debate.com.mx/politica/AMLO-se-disculpa-y-explica-la-razon-de-hablar-lento-20191212-0287.html due to extraction error.\n",
      "Error extracting article text from https://confidencial.digital/opinion/presidentes-contra-la-prensa/: 403 Client Error: Forbidden for url: https://confidencial.digital/opinion/presidentes-contra-la-prensa/\n",
      "Skipped article at https://confidencial.digital/opinion/presidentes-contra-la-prensa/ due to extraction error.\n",
      "Skipped article at https://aristeguinoticias.com/1112/mexico/no-me-incomoda-pintura-de-zapata-pero-tiene-que-haber-conciliacion-dice-amlo/ due to extraction error.\n",
      "Finished scraping for 2019-12-11. Moving to the next day.\n",
      "Scraping URL for 2019-12-12: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-12+before:2019-12-13&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-12\n",
      "Skipped article at https://www.debate.com.mx/politica/AMLO-se-disculpa-y-explica-la-razon-de-hablar-lento-20191212-0287.html due to extraction error.\n",
      "Skipped article at https://aristeguinoticias.com/1312/mexico/tras-aprehension-de-garcia-luna-reaparece-fox-en-twitter-con-felicitaciones-a-amlo-por-t-mec/ due to extraction error.\n",
      "Finished scraping for 2019-12-12. Moving to the next day.\n",
      "Scraping URL for 2019-12-13: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-13+before:2019-12-14&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-13\n",
      "Error extracting article text from https://tabasco.gob.mx/noticias/agradece-adan-augusto-apoyo-de-amlo-para-detonar-el-desarrollo-de-tabasco: HTTPSConnectionPool(host='tabasco.gob.mx', port=443): Max retries exceeded with url: /noticias/agradece-adan-augusto-apoyo-de-amlo-para-detonar-el-desarrollo-de-tabasco (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C24D833590>, 'Connection to tabasco.gob.mx timed out. (connect timeout=10)'))\n",
      "Skipped article at https://tabasco.gob.mx/noticias/agradece-adan-augusto-apoyo-de-amlo-para-detonar-el-desarrollo-de-tabasco due to extraction error.\n",
      "Skipped article at https://aristeguinoticias.com/1312/mexico/tras-aprehension-de-garcia-luna-reaparece-fox-en-twitter-con-felicitaciones-a-amlo-por-t-mec/ due to extraction error.\n",
      "Finished scraping for 2019-12-13. Moving to the next day.\n",
      "Scraping URL for 2019-12-14: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-14+before:2019-12-15&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-14\n",
      "Error extracting article text from https://tabasco.gob.mx/noticias/agradece-adan-augusto-apoyo-de-amlo-para-detonar-el-desarrollo-de-tabasco: HTTPSConnectionPool(host='tabasco.gob.mx', port=443): Max retries exceeded with url: /noticias/agradece-adan-augusto-apoyo-de-amlo-para-detonar-el-desarrollo-de-tabasco (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C24DCA2240>, 'Connection to tabasco.gob.mx timed out. (connect timeout=10)'))\n",
      "Skipped article at https://tabasco.gob.mx/noticias/agradece-adan-augusto-apoyo-de-amlo-para-detonar-el-desarrollo-de-tabasco due to extraction error.\n",
      "Finished scraping for 2019-12-14. Moving to the next day.\n",
      "Scraping URL for 2019-12-15: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-15+before:2019-12-16&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-15\n",
      "Skipped article at https://www.debate.com.mx/deportes/AMLO-pide-unificar-las-dos-grandes-de-ligas-beisbol-en-Mexico-20191216-0180.html due to extraction error.\n",
      "Finished scraping for 2019-12-15. Moving to the next day.\n",
      "Scraping URL for 2019-12-16: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-16+before:2019-12-17&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-16\n",
      "Error extracting article text from https://www.france24.com/es/20191217-m%C3%A9xico-el-proyecto-de-tren-maya-recibi%C3%B3-amplio-respaldo-en-la-consulta-popular: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191217-m%C3%A9xico-el-proyecto-de-tren-maya-recibi%C3%B3-amplio-respaldo-en-la-consulta-popular\n",
      "Skipped article at https://www.france24.com/es/20191217-m%C3%A9xico-el-proyecto-de-tren-maya-recibi%C3%B3-amplio-respaldo-en-la-consulta-popular due to extraction error.\n",
      "Finished scraping for 2019-12-16. Moving to the next day.\n",
      "Scraping URL for 2019-12-17: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-17+before:2019-12-18&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-17\n",
      "Error extracting article text from https://www.france24.com/es/20191217-m%C3%A9xico-el-proyecto-de-tren-maya-recibi%C3%B3-amplio-respaldo-en-la-consulta-popular: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191217-m%C3%A9xico-el-proyecto-de-tren-maya-recibi%C3%B3-amplio-respaldo-en-la-consulta-popular\n",
      "Skipped article at https://www.france24.com/es/20191217-m%C3%A9xico-el-proyecto-de-tren-maya-recibi%C3%B3-amplio-respaldo-en-la-consulta-popular due to extraction error.\n",
      "Skipped article at https://www.debate.com.mx/politica/Gobernadores-del-PAN-acusan-a-AMLO-de-transferir-fracaso-en-seguridad-20191218-0187.html due to extraction error.\n",
      "Finished scraping for 2019-12-17. Moving to the next day.\n",
      "Scraping URL for 2019-12-18: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-18+before:2019-12-19&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-18\n",
      "Skipped article at https://www.debate.com.mx/politica/Gobernadores-del-PAN-acusan-a-AMLO-de-transferir-fracaso-en-seguridad-20191218-0187.html due to extraction error.\n",
      "Finished scraping for 2019-12-18. Moving to the next day.\n",
      "Scraping URL for 2019-12-19: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-19+before:2019-12-20&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-19\n",
      "Error extracting article text from https://www.washingtonpost.com/es/post-opinion/2019/12/20/caso-bartlett-amlo-se-convirtio-en-todo-lo-que-prometio-combatir-en-mexico/: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=10)\n",
      "Skipped article at https://www.washingtonpost.com/es/post-opinion/2019/12/20/caso-bartlett-amlo-se-convirtio-en-todo-lo-que-prometio-combatir-en-mexico/ due to extraction error.\n",
      "Finished scraping for 2019-12-19. Moving to the next day.\n",
      "Scraping URL for 2019-12-20: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-20+before:2019-12-21&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-20\n",
      "Error extracting article text from https://www.washingtonpost.com/es/post-opinion/2019/12/20/caso-bartlett-amlo-se-convirtio-en-todo-lo-que-prometio-combatir-en-mexico/: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=10)\n",
      "Skipped article at https://www.washingtonpost.com/es/post-opinion/2019/12/20/caso-bartlett-amlo-se-convirtio-en-todo-lo-que-prometio-combatir-en-mexico/ due to extraction error.\n",
      "Finished scraping for 2019-12-20. Moving to the next day.\n",
      "Scraping URL for 2019-12-21: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-21+before:2019-12-22&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-21\n",
      "Finished scraping for 2019-12-21. Moving to the next day.\n",
      "Scraping URL for 2019-12-22: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-22+before:2019-12-23&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-22\n",
      "Error extracting article text from https://www.france24.com/es/20191223-m%C3%A9xico-denuncia-la-excesiva-vigilancia-en-sus-sedes-diplom%C3%A1ticas-en-bolivia: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191223-m%C3%A9xico-denuncia-la-excesiva-vigilancia-en-sus-sedes-diplom%C3%A1ticas-en-bolivia\n",
      "Skipped article at https://www.france24.com/es/20191223-m%C3%A9xico-denuncia-la-excesiva-vigilancia-en-sus-sedes-diplom%C3%A1ticas-en-bolivia due to extraction error.\n",
      "Finished scraping for 2019-12-22. Moving to the next day.\n",
      "Scraping URL for 2019-12-23: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-23+before:2019-12-24&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-23\n",
      "Error extracting article text from https://www.france24.com/es/20191223-m%C3%A9xico-denuncia-la-excesiva-vigilancia-en-sus-sedes-diplom%C3%A1ticas-en-bolivia: 403 Client Error: Forbidden for url: https://www.france24.com/es/20191223-m%C3%A9xico-denuncia-la-excesiva-vigilancia-en-sus-sedes-diplom%C3%A1ticas-en-bolivia\n",
      "Skipped article at https://www.france24.com/es/20191223-m%C3%A9xico-denuncia-la-excesiva-vigilancia-en-sus-sedes-diplom%C3%A1ticas-en-bolivia due to extraction error.\n",
      "Skipped article at https://aristeguinoticias.com/2412/mexico/revela-amlo-que-calderon-concesiono-35-5-millones-de-hectareas-a-mineras/ due to extraction error.\n",
      "Finished scraping for 2019-12-23. Moving to the next day.\n",
      "Scraping URL for 2019-12-24: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-24+before:2019-12-25&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-24\n",
      "Skipped article at https://aristeguinoticias.com/2412/mexico/revela-amlo-que-calderon-concesiono-35-5-millones-de-hectareas-a-mineras/ due to extraction error.\n",
      "Finished scraping for 2019-12-24. Moving to the next day.\n",
      "Scraping URL for 2019-12-25: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-25+before:2019-12-26&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-25\n",
      "Skipped article at https://www.debate.com.mx/politica/Fox-llama-a-la-cura-del-populismo-es-la-protesta-y-democracia-20191226-0130.html due to extraction error.\n",
      "Finished scraping for 2019-12-25. Moving to the next day.\n",
      "Scraping URL for 2019-12-26: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-26+before:2019-12-27&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-26\n",
      "Finished scraping for 2019-12-26. Moving to the next day.\n",
      "Scraping URL for 2019-12-27: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-27+before:2019-12-28&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-27\n",
      "Skipped article at https://www.debate.com.mx/politica/AMLO-el-blanco-de-la-prensa-en-el-Dia-de-los-Inocentes-20191228-0121.html due to extraction error.\n",
      "Finished scraping for 2019-12-27. Moving to the next day.\n",
      "Scraping URL for 2019-12-28: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-28+before:2019-12-29&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-28\n",
      "Skipped article at https://www.debate.com.mx/politica/AMLO-el-blanco-de-la-prensa-en-el-Dia-de-los-Inocentes-20191228-0121.html due to extraction error.\n",
      "Error extracting article text from https://imparcialoaxaca.mx/oaxaca/anuncia-amlo-canal-interoceanico-en-el-istmo/: HTTPSConnectionPool(host='imparcialoaxaca.mx', port=443): Read timed out. (read timeout=10)\n",
      "Skipped article at https://imparcialoaxaca.mx/oaxaca/anuncia-amlo-canal-interoceanico-en-el-istmo/ due to extraction error.\n",
      "Finished scraping for 2019-12-28. Moving to the next day.\n",
      "Scraping URL for 2019-12-29: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-29+before:2019-12-30&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-29\n",
      "Finished scraping for 2019-12-29. Moving to the next day.\n",
      "Scraping URL for 2019-12-30: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-30+before:2019-12-31&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-30\n",
      "Error extracting article text from https://factual.afp.com/es-un-fotomontaje-la-persona-entre-lopez-obrador-y-quirino-ordaz-era-el-dueno-de-un-restaurante: 403 Client Error: Forbidden for url: https://factual.afp.com/es-un-fotomontaje-la-persona-entre-lopez-obrador-y-quirino-ordaz-era-el-dueno-de-un-restaurante\n",
      "Skipped article at https://factual.afp.com/es-un-fotomontaje-la-persona-entre-lopez-obrador-y-quirino-ordaz-era-el-dueno-de-un-restaurante due to extraction error.\n",
      "Finished scraping for 2019-12-30. Moving to the next day.\n",
      "Scraping URL for 2019-12-31: https://www.google.com/search?q=Andrés Manuel López Obrador+after:2019-12-31+before:2020-01-01&tbm=nws&sort=date\n",
      "Scraping page 1 for 2019-12-31\n",
      "Skipped article at https://www.debate.com.mx/politica/AMLO-dice-que-El-Chapo-tenia-mismo-poder-que-el-presidente-de-Mexico-20200101-0032.html due to extraction error.\n",
      "Finished scraping for 2019-12-31. Moving to the next day.\n"
     ]
    }
   ],
   "source": [
    "news_data = []\n",
    "\n",
    "while current_date <= end_date:\n",
    "    formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    next_date = current_date + timedelta(days=1)\n",
    "    formatted_next_date = next_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Update the URL for the daily range\n",
    "    url = f'https://www.google.com/search?q={search_query}+after:{formatted_date}+before:{formatted_next_date}&tbm=nws&sort=date'\n",
    "    print(f\"Scraping URL for {formatted_date}: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    count = 0\n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages and count < daily_limit:\n",
    "        print(f\"Scraping page {current_page} for {formatted_date}\")\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.SoaBEf'))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout on {formatted_date}, page {current_page}: {e}\")\n",
    "            break\n",
    "\n",
    "        news_results = driver.find_elements(By.CSS_SELECTOR, 'div.SoaBEf')\n",
    "        if not news_results:\n",
    "            print(f\"No articles found on page {current_page}. Moving to next day.\")\n",
    "            break\n",
    "\n",
    "        for news_div in news_results:\n",
    "            if count >= daily_limit:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                news_link = news_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                news_title = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"]').text\n",
    "                news_domain = news_div.find_element(By.CSS_SELECTOR, 'div').text\n",
    "                news_date = news_div.find_element(By.CSS_SELECTOR, 'div.OSrXXb.rbYSKb.LfVVr > span').text\n",
    "                \n",
    "                # Extract full article text by visiting the URL\n",
    "                full_text = extract_full_text(news_link)\n",
    "                if not full_text:\n",
    "                    print(f\"Skipped article at {news_link} due to extraction error.\")\n",
    "                    continue\n",
    "                \n",
    "                # Append the full data to the list\n",
    "                news_data.append([\n",
    "                    news_link, news_domain, news_title, news_date, full_text\n",
    "                ])\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping an article: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, '#pnnext'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            next_button.click()\n",
    "            current_page += 1\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "        except Exception as e:\n",
    "            print(f\"No more pages or error clicking 'Next': {e}\")\n",
    "            break\n",
    "\n",
    "    current_date = next_date\n",
    "    print(f\"Finished scraping for {formatted_date}. Moving to the next day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Link  \\\n",
      "0  https://www.gob.mx/se/articulos/avances-de-los...   \n",
      "1  https://www.bbc.com/mundo/noticias-america-lat...   \n",
      "2  https://ibero.mx/prensa/opinion-primer-ano-de-...   \n",
      "3  https://www.dw.com/es/m%C3%A9xico-l%C3%B3pez-o...   \n",
      "4  https://www.infobae.com/america/mexico/2019/12...   \n",
      "\n",
      "                                              Domain  \\\n",
      "0  gob.mx\\nAvances de los Programas del Gobierno ...   \n",
      "1  BBC\\nInforme de AMLO: las decisiones de López ...   \n",
      "2  IBERO\\n#OPINIÓN Primer año de gobierno: la est...   \n",
      "3  DW\\nMéxico: López Obrador cumple primer año co...   \n",
      "4  Infobae\\nLópez Obrador a un año de gobierno: “...   \n",
      "\n",
      "                                               Title        Date  \\\n",
      "0  Avances de los Programas del Gobierno del Pres...  1 dic 2019   \n",
      "1  Informe de AMLO: las decisiones de López Obrad...  1 dic 2019   \n",
      "2  #OPINIÓN Primer año de gobierno: la estrategia...  2 dic 2019   \n",
      "3  México: López Obrador cumple primer año con pr...  2 dic 2019   \n",
      "4  López Obrador a un año de gobierno: “Todavía l...  1 dic 2019   \n",
      "\n",
      "                                             Content  \n",
      "0  Secretaría de Economía | 01 de diciembre de 20...  \n",
      "1  Fuente de la imagen, AFP Polémico. Es la palab...  \n",
      "2  Llega el presidente Andrés Manuel López Obrado...  \n",
      "3  El aniversario de gobierno coincide con una nu...  \n",
      "4  8 Ene, 2025 El presidente de México, Andrés Ma...  \n"
     ]
    }
   ],
   "source": [
    "columns = ['Link', 'Domain', 'Title', 'Date', 'Content']\n",
    "news_df = pd.DataFrame(news_data, columns=columns)\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv(\"first_batch.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous code that partially worked \n",
    "Loop with pagination and visiting each webpage using the function above to extract the content of each news.\n",
    "The following functions partially work. Up I am working on new ones. Jan 8 2025  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for extracting information url by url \n",
    "def extract_full_text(article_url):\n",
    "    try:\n",
    "        # Send a request to the article URL\n",
    "        response = requests.get(article_url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the main article text (this varies by website)\n",
    "        # Adjust the selector to target the main content\n",
    "        article_body = soup.find_all('p')\n",
    "        full_text = \" \".join([p.text for p in article_body])\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article text from {article_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL for 2018-12-03: https://www.google.com/search?q=mañanera amlo+after:2018-12-03+before:2018-12-04&tbm=nws&sort=date\n",
      "Scraping page 1 for 2018-12-03\n",
      "Error extracting article text from https://www.milenio.com/politica/la-mananera-de-que-hablo-amlo-en-su-primera-conferencia: name 'BeautifulSoup' is not defined\n",
      "Skipped article at https://www.milenio.com/politica/la-mananera-de-que-hablo-amlo-en-su-primera-conferencia due to extraction error.\n",
      "Error extracting article text from https://www.eleconomista.com.mx/politica/De-que-hablo-AMLO-en-su-primera-conferencia-de-prensa-matutina-20181203-0070.html: name 'BeautifulSoup' is not defined\n",
      "Skipped article at https://www.eleconomista.com.mx/politica/De-que-hablo-AMLO-en-su-primera-conferencia-de-prensa-matutina-20181203-0070.html due to extraction error.\n",
      "Error extracting article text from https://www.eluniversal.com.mx/nacion/el-dedito-el-gallito-y-el-beisbol-asi-eran-las-mananeras-de-amlo/: name 'BeautifulSoup' is not defined\n",
      "Skipped article at https://www.eluniversal.com.mx/nacion/el-dedito-el-gallito-y-el-beisbol-asi-eran-las-mananeras-de-amlo/ due to extraction error.\n",
      "Error extracting article text from https://lahoguera.mx/amlo-tengo-las-riendas-del-poder-arranca-las-mananeras/: name 'BeautifulSoup' is not defined\n",
      "Skipped article at https://lahoguera.mx/amlo-tengo-las-riendas-del-poder-arranca-las-mananeras/ due to extraction error.\n",
      "Error extracting article text from https://www.debate.com.mx/mexico/residencia-presidencial-los-pinos-20181203-0047.html: 403 Client Error: Forbidden for url: https://www.debate.com.mx/mexico/residencia-presidencial-los-pinos-20181203-0047.html\n",
      "Skipped article at https://www.debate.com.mx/mexico/residencia-presidencial-los-pinos-20181203-0047.html due to extraction error.\n",
      "Error extracting article text from https://nacion321.com/gobierno/la-mujer-que-se-colo-a-palacio-nacional-para-abrazar-a-amlo/: name 'BeautifulSoup' is not defined\n",
      "Skipped article at https://nacion321.com/gobierno/la-mujer-que-se-colo-a-palacio-nacional-para-abrazar-a-amlo/ due to extraction error.\n",
      "Error extracting article text from https://www.milenio.com/politica/pan-compara-amlo-hitler-censura-responde-presidente: name 'BeautifulSoup' is not defined\n",
      "Skipped article at https://www.milenio.com/politica/pan-compara-amlo-hitler-censura-responde-presidente due to extraction error.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m news_date \u001b[38;5;241m=\u001b[39m news_div\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv.OSrXXb.rbYSKb.LfVVr > span\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Extract full article text by visiting the URL\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_full_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m full_text:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipped article at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnews_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to extraction error.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mextract_full_text\u001b[1;34m(article_url)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_full_text\u001b[39m(article_url):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Send a request to the article URL\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an error for bad HTTP responses\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    695\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "news_data = []\n",
    "\n",
    "while current_date <= end_date:\n",
    "    formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    next_date = current_date + timedelta(days=1)\n",
    "    formatted_next_date = next_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Update the URL for the daily range\n",
    "    url = f'https://www.google.com/search?q={search_query}+after:{formatted_date}+before:{formatted_next_date}&tbm=nws&sort=date'\n",
    "    print(f\"Scraping URL for {formatted_date}: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    count = 0\n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages and count < daily_limit:\n",
    "        print(f\"Scraping page {current_page} for {formatted_date}\")\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.SoaBEf'))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout on {formatted_date}, page {current_page}: {e}\")\n",
    "            break\n",
    "\n",
    "        news_results = driver.find_elements(By.CSS_SELECTOR, 'div.SoaBEf')\n",
    "        if not news_results:\n",
    "            print(f\"No articles found on page {current_page}. Moving to next day.\")\n",
    "            break\n",
    "\n",
    "        for news_div in news_results:\n",
    "            if count >= daily_limit:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                news_link = news_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                news_title = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"]').text\n",
    "                news_domain = news_div.find_element(By.CSS_SELECTOR, 'div').text\n",
    "                news_date = news_div.find_element(By.CSS_SELECTOR, 'div.OSrXXb.rbYSKb.LfVVr > span').text\n",
    "                \n",
    "                # Extract full article text by visiting the URL\n",
    "                full_text = extract_full_text(news_link)\n",
    "                if not full_text:\n",
    "                    print(f\"Skipped article at {news_link} due to extraction error.\")\n",
    "                    continue\n",
    "                \n",
    "                # Append the full data to the list\n",
    "                news_data.append([\n",
    "                    news_link, news_domain, news_title, news_date, full_text\n",
    "                ])\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping an article: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, '#pnnext'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            next_button.click()\n",
    "            current_page += 1\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "        except Exception as e:\n",
    "            print(f\"No more pages or error clicking 'Next': {e}\")\n",
    "            break\n",
    "\n",
    "    current_date = next_date\n",
    "    print(f\"Finished scraping for {formatted_date}. Moving to the next day.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#loop with pagination, let's see if it works: \n",
    "(IT works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL for 2018-12-01: https://www.google.com/search?q=mañanera amlo+after:2018-12-01+before:2018-12-02&tbm=nws&sort=date\n",
      "Scraping page 1 for 2018-12-01\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "No more pages or error clicking 'Next': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF64966FB05+28789]\n",
      "\t(No symbol) [0x00007FF6495D86E0]\n",
      "\t(No symbol) [0x00007FF64947592A]\n",
      "\t(No symbol) [0x00007FF6494C930E]\n",
      "\t(No symbol) [0x00007FF6494C95FC]\n",
      "\t(No symbol) [0x00007FF6495128A7]\n",
      "\t(No symbol) [0x00007FF6494EF47F]\n",
      "\t(No symbol) [0x00007FF64950F654]\n",
      "\t(No symbol) [0x00007FF6494EF1E3]\n",
      "\t(No symbol) [0x00007FF6494BA938]\n",
      "\t(No symbol) [0x00007FF6494BBAA1]\n",
      "\tGetHandleVerifier [0x00007FF6499A933D+3410093]\n",
      "\tGetHandleVerifier [0x00007FF6499BE7DD+3497293]\n",
      "\tGetHandleVerifier [0x00007FF6499B2A73+3448803]\n",
      "\tGetHandleVerifier [0x00007FF649737BBB+848171]\n",
      "\t(No symbol) [0x00007FF6495E3C3F]\n",
      "\t(No symbol) [0x00007FF6495DF6E4]\n",
      "\t(No symbol) [0x00007FF6495DF87D]\n",
      "\t(No symbol) [0x00007FF6495CED49]\n",
      "\tBaseThreadInitThunk [0x00007FFF8BBCE8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FFF8D0BFBCC+44]\n",
      "\n",
      "Finished scraping for 2018-12-01. Moving to the next day.\n",
      "Scraping URL for 2018-12-02: https://www.google.com/search?q=mañanera amlo+after:2018-12-02+before:2018-12-03&tbm=nws&sort=date\n",
      "Scraping page 1 for 2018-12-02\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "Error scraping an article: name 'news_data' is not defined\n",
      "No more pages or error clicking 'Next': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF64966FB05+28789]\n",
      "\t(No symbol) [0x00007FF6495D86E0]\n",
      "\t(No symbol) [0x00007FF64947592A]\n",
      "\t(No symbol) [0x00007FF6494C930E]\n",
      "\t(No symbol) [0x00007FF6494C95FC]\n",
      "\t(No symbol) [0x00007FF6495128A7]\n",
      "\t(No symbol) [0x00007FF6494EF47F]\n",
      "\t(No symbol) [0x00007FF64950F654]\n",
      "\t(No symbol) [0x00007FF6494EF1E3]\n",
      "\t(No symbol) [0x00007FF6494BA938]\n",
      "\t(No symbol) [0x00007FF6494BBAA1]\n",
      "\tGetHandleVerifier [0x00007FF6499A933D+3410093]\n",
      "\tGetHandleVerifier [0x00007FF6499BE7DD+3497293]\n",
      "\tGetHandleVerifier [0x00007FF6499B2A73+3448803]\n",
      "\tGetHandleVerifier [0x00007FF649737BBB+848171]\n",
      "\t(No symbol) [0x00007FF6495E3C3F]\n",
      "\t(No symbol) [0x00007FF6495DF6E4]\n",
      "\t(No symbol) [0x00007FF6495DF87D]\n",
      "\t(No symbol) [0x00007FF6495CED49]\n",
      "\tBaseThreadInitThunk [0x00007FFF8BBCE8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FFF8D0BFBCC+44]\n",
      "\n",
      "Finished scraping for 2018-12-02. Moving to the next day.\n",
      "Scraping URL for 2018-12-03: https://www.google.com/search?q=mañanera amlo+after:2018-12-03+before:2018-12-04&tbm=nws&sort=date\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.google.com/search?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+after:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+before:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_next_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&tbm=nws&sort=date\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping URL for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Pagination variables for the daily range\u001b[39;00m\n\u001b[0;32m     12\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:363\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:352\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    350\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:306\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    304\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    305\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:326\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    323\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 326\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luisf\\anaconda3\\envs\\scraper\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while current_date <= end_date:\n",
    "    formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    next_date = current_date + timedelta(days=1)\n",
    "    formatted_next_date = next_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Update the URL for the daily range\n",
    "    url = f'https://www.google.com/search?q={search_query}+after:{formatted_date}+before:{formatted_next_date}&tbm=nws&sort=date'\n",
    "    print(f\"Scraping URL for {formatted_date}: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Pagination variables for the daily range\n",
    "    count = 0\n",
    "    current_page = 1\n",
    "\n",
    "    # Pagination loop\n",
    "    while current_page <= max_pages and count < daily_limit:\n",
    "        print(f\"Scraping page {current_page} for {formatted_date}\")\n",
    "\n",
    "        try:\n",
    "            # Wait for articles to load\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.SoaBEf'))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout while waiting for articles on {formatted_date}, page {current_page}: {e}\")\n",
    "            break  # Exit pagination loop if no articles load\n",
    "\n",
    "        news_results = driver.find_elements(By.CSS_SELECTOR, 'div.SoaBEf')\n",
    "        if not news_results:\n",
    "            print(f\"No articles found on page {current_page}. Moving to next day.\")\n",
    "            break\n",
    "\n",
    "        for news_div in news_results:\n",
    "            if count >= daily_limit:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Scrape article details\n",
    "                news_link = news_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                news_title = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"]').text\n",
    "                news_domain = news_div.find_element(By.CSS_SELECTOR, 'div').text\n",
    "                news_description = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"] + div').text\n",
    "                news_date = news_div.find_element(By.CSS_SELECTOR, 'div.OSrXXb.rbYSKb.LfVVr > span').text\n",
    "\n",
    "                news_data.append([news_link, news_domain, news_title, news_description, news_date])\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping an article: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Handle pagination\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, '#pnnext'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "            time.sleep(random.uniform(2, 5))  # Mimic human behavior\n",
    "            next_button.click()\n",
    "            current_page += 1\n",
    "            time.sleep(random.uniform(2, 4))  # Short wait for the next page to load\n",
    "        except Exception as e:\n",
    "            print(f\"No more pages or error clicking 'Next': {e}\")\n",
    "            break  # Exit pagination loop if no next button\n",
    "\n",
    "    # Increment to the next date\n",
    "    current_date = next_date\n",
    "    print(f\"Finished scraping for {formatted_date}. Moving to the next day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'limit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Second iteration with pagination... fully functional. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m news_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_page \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_pages \u001b[38;5;129;01mand\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[43mlimit\u001b[49m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_page\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Wait for the news results to load\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'limit' is not defined"
     ]
    }
   ],
   "source": [
    "#Second iteration with pagination... fully functional. \n",
    "\n",
    "news_data = []\n",
    "\n",
    "while current_page <= max_pages and count < limit:\n",
    "    print(f\"Scraping page {current_page}\")\n",
    "    \n",
    "    try:\n",
    "        # Wait for the news results to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.SoaBEf'))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Timeout while waiting for news articles on page {current_page}: {e}\")\n",
    "        # Optional: Take a screenshot for debugging\n",
    "        driver.save_screenshot(f'timeout_page_{current_page}.png')\n",
    "        break  # Exit if articles do not load in time\n",
    "\n",
    "    # Select all news article blocks by targeting the SoaBEf class\n",
    "    news_results = driver.find_elements(By.CSS_SELECTOR, 'div.SoaBEf')\n",
    "    \n",
    "    if not news_results:\n",
    "        print(f\"No articles found on page {current_page}.\")\n",
    "        # Optional: Take a screenshot to inspect the page\n",
    "        driver.save_screenshot(f'no_articles_page_{current_page}.png')\n",
    "        break  # Exit if no articles are found\n",
    "\n",
    "    for news_div in news_results:\n",
    "        if count >= limit:\n",
    "            print(f\"Reached the limit of {limit} articles.\")\n",
    "            break  # Stop if we've reached the limit\n",
    "\n",
    "        try:\n",
    "            # Extract the link to the article\n",
    "            news_link = news_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "            # Extract the title of the article\n",
    "            news_title = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"]').text\n",
    "\n",
    "            # Extract the source domain\n",
    "            news_domain = news_div.find_element(By.CSS_SELECTOR, 'div').text\n",
    "\n",
    "            # Extract the snippet description and the publication date\n",
    "            news_description = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"] + div').text\n",
    "            news_date = news_div.find_element(By.CSS_SELECTOR, 'div.OSrXXb.rbYSKb.LfVVr > span').text\n",
    "\n",
    "            # Append the collected information to the list\n",
    "            news_data.append([news_link, news_domain, news_title, news_description, news_date])\n",
    "            count += 1  # Increment the count of news articles\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping an article on page {current_page}: {e}\")\n",
    "            # Optional: Take a screenshot for debugging\n",
    "            driver.save_screenshot(f'error_article_page_{current_page}.png')\n",
    "            continue  # Skip to the next article\n",
    "\n",
    "    # Check if we've reached the limit after scraping the current page\n",
    "    if count >= limit:\n",
    "        print(f\"Reached the limit of {limit} articles.\")\n",
    "        break\n",
    "\n",
    "    # Attempt to locate the \"Next\" button and click it using the general selector\n",
    "    try:\n",
    "        # Find the \"Next\" button using its general ID selector\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, '#pnnext'))\n",
    "        )\n",
    "        print(\"Found 'Next' button using the '#pnnext' selector.\")\n",
    "        \n",
    "        # Scroll to the \"Next\" button to ensure it's in view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "        \n",
    "        # Introduce a random sleep to mimic human behavior\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Capture the current URL before clicking \"Next\"\n",
    "        current_url = driver.current_url\n",
    "        print(f\"Current URL before clicking 'Next': {current_url}\")\n",
    "        \n",
    "        # Click the \"Next\" button\n",
    "        next_button.click()\n",
    "        print(f\"Clicked 'Next' button on page {current_page}.\")\n",
    "        \n",
    "        # Increment the current page counter\n",
    "        current_page += 1\n",
    "        \n",
    "        # Optional: Add a short sleep to wait for the next page to load\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        # Capture the new URL after clicking \"Next\"\n",
    "        new_url = driver.current_url\n",
    "        print(f\"Navigated to new URL: {new_url}\")\n",
    "        \n",
    "        # Verify that the URL has changed to prevent looping\n",
    "        if new_url == current_url:\n",
    "            print(\"URL did not change after clicking 'Next'. Possible loop detected.\")\n",
    "            break  # Exit the loop to prevent infinite cycling\n",
    "        else:\n",
    "            print(f\"Successfully navigated to page {current_page}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No more pages available or error clicking 'Next' on page {current_page}: {e}\")\n",
    "        # Optional: Take a screenshot to inspect the state when the error occurred\n",
    "        #driver.save_screenshot(f'next_button_error_page_{current_page}.png')\n",
    "        break  # Exit the loop if there's no next button or an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(news_data)\n",
    "print(len(news_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Link, Domain, Title, Description, Date]\n",
      "Index: []\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(news_data, columns=[\"Link\", \"Domain\", \"Title\", \"Description\", \"Date\"])\n",
    "print(df)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to daily_news_results_dec18.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the results to a CSV file\n",
    "csv_file_path = \"daily_news_results_dec18.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Link\", \"Domain\", \"Title\", \"Description\", \"Date\"])\n",
    "    # Write the news data\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
