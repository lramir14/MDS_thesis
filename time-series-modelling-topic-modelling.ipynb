{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### The purpose is to continue with the results. ","metadata":{}},{"cell_type":"code","source":"import os \nos.listdir('/kaggle/input') \n\nembeddings_path = \"/kaggle/input/embeddings\"\nprint(os.listdir(embeddings_path))\nimport pandas as pd\n\nspeeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\nnews_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n\n# Display first few rows\nprint(\"News Data:\")\nprint(news_embeddings.head())\n\nprint(\"\\nSpeeches Data:\")\nprint(speeches_embeddings.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:56:29.502435Z","iopub.execute_input":"2025-03-10T17:56:29.502631Z"}},"outputs":[{"name":"stdout","text":"['speeches_with_embeddings.csv', 'speeches_embeddings_sentiment.csv', 'news_embeddings_sentiment.csv', 'news_with_embeddings.csv']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# 1. Imports & Initialization\n# =====================================================================\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom datetime import datetime\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Convert date columns to datetime\nnews_embeddings['Date'] = pd.to_datetime(news_embeddings['Date'])\nspeeches_embeddings['date'] = pd.to_datetime(speeches_embeddings['date'])\n\n# =====================================================================\n# 2. Data Preparation & Date Alignment\n# =====================================================================\ndef align_dates(news_df, speeches_df):\n    \"\"\"Filter and align dates between news and speeches across all years\"\"\"\n    # Find common dates across the full dataset\n    common_dates = set(news_df['Date']).intersection(set(speeches_df['date']))\n    \n    return (\n        news_df[news_df['Date'].isin(common_dates)].reset_index(drop=True),\n        speeches_df[speeches_df['date'].isin(common_dates)].reset_index(drop=True),\n        sorted(common_dates)  # Sorted for consistency\n    )\n\nnews_data, speeches_data, common_dates = align_dates(news_embeddings, speeches_embeddings)\n\n# =====================================================================\n# 3. GPU-Optimized Embedding Processing\n# =====================================================================\ndef process_embeddings(df, col_name):\n    \"\"\"Convert text-based or list embeddings to GPU tensors\"\"\"\n    embeddings = []\n    for row in df[col_name]:\n        if isinstance(row, str):\n            arr = np.fromstring(row.strip(\"[]\"), sep=\" \", dtype=np.float32)\n        elif isinstance(row, list):\n            arr = np.array(row, dtype=np.float32)\n        embeddings.append(torch.tensor(arr, device=device))\n    return torch.stack(embeddings)\n\nnews_tensor = process_embeddings(news_data, 'news_embeddings')\nspeeches_tensor = process_embeddings(speeches_data, 'speech_embeddings')\n\n# =====================================================================\n# 4. Same-Day Cosine Similarity Calculation\n# =====================================================================\ndef compute_daily_similarities(news_tensor, speeches_tensor, dates, news_df, speeches_df):\n    \"\"\"Compute cosine similarity per date between news and speeches\"\"\"\n    # Normalize embeddings\n    news_norm = F.normalize(news_tensor, p=2, dim=1)\n    speeches_norm = F.normalize(speeches_tensor, p=2, dim=1)\n    \n    # Map dates to indices\n    unique_dates = sorted(dates)\n    date_indices = {date: i for i, date in enumerate(unique_dates)}\n    \n    # Initialize result tensors\n    daily_avg = torch.zeros(len(unique_dates), device=device)\n    daily_std = torch.zeros(len(unique_dates), device=device)\n    \n    # Compute daily cosine similarity\n    for date in unique_dates:\n        news_mask = news_df['Date'] == date\n        speech_mask = speeches_df['date'] == date\n        \n        if news_mask.any() and speech_mask.any():\n            sim_matrix = torch.mm(news_norm[news_mask], speeches_norm[speech_mask].T)\n            daily_avg[date_indices[date]] = sim_matrix.mean()\n            daily_std[date_indices[date]] = sim_matrix.std()\n    \n    return daily_avg.cpu().numpy(), daily_std.cpu().numpy(), unique_dates\n\ndaily_scores, daily_stds, valid_dates = compute_daily_similarities(\n    news_tensor, speeches_tensor, common_dates, news_data, speeches_data\n)\n\n# =====================================================================\n# 5. Result Compilation & Handling Missing Data\n# =====================================================================\n# Create a complete date range across all available years\nstart_date, end_date = min(common_dates), max(common_dates)\nfull_dates = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n# Build a DataFrame with results\ndaily_avg_df = pd.DataFrame({\n    'date': valid_dates,\n    'cosine_similarity': daily_scores,\n    'std_dev': daily_stds\n})\n\n# Merge with full date range\nfinal_df = pd.DataFrame({'date': full_dates}).merge(\n    daily_avg_df, \n    on='date', \n    how='left'\n)\n\n# Fill missing values using forward-fill with a 3-day limit\nfinal_df['cosine_similarity'] = final_df['cosine_similarity'].ffill(limit=3)\nfinal_df['std_dev'] = final_df['std_dev'].ffill(limit=3)\n\n# =====================================================================\n# 6. Create Bounds for Standard Deviation\n# =====================================================================\nfinal_df['upper_bound'] = final_df['cosine_similarity'] + final_df['std_dev']\nfinal_df['lower_bound'] = final_df['cosine_similarity'] - final_df['std_dev']\n\n# Save the processed data for later plotting\nfinal_df.to_csv(\"cosine_similarity_results_all_years.csv\", index=False)\n\nprint(\"Processing complete. Data saved for plotting.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Define the years to plot\nyears = range(2019, 2025)\n\n# Set up the figure size\nplt.figure(figsize=(14, 7 * len(years)))\n\nfor i, year in enumerate(years, 1):\n    yearly_df = final_df[final_df['date'].dt.year == year]\n    \n    if yearly_df.empty:\n        continue  # Skip empty years\n    \n    plt.subplot(len(years), 1, i)  # Create a subplot for each year\n    \n    plt.plot(yearly_df['date'], \n             yearly_df['cosine_similarity'], \n             color='#2ca02c', \n             linewidth=1.5,\n             marker='o',\n             markersize=4,\n             label='Cosine Similarity')\n\n    # Plot the upper and lower bounds for 1 standard deviation\n    plt.fill_between(yearly_df['date'], \n                     yearly_df['upper_bound'], \n                     yearly_df['lower_bound'], \n                     color='gray', alpha=0.3, label='1 Std Dev Range')\n\n    # Highlight missing data in red\n    missing_mask = yearly_df['cosine_similarity'].isna()\n    plt.fill_between(yearly_df['date'], \n                     yearly_df['cosine_similarity'], \n                     where=missing_mask,\n                     color='red', \n                     alpha=0.1,\n                     label='Missing Data')\n\n    # Formatting\n    plt.title(f'{year} Daily Average Speech-News Cosine Similarity')\n    plt.xlabel('Date')\n    plt.ylabel('Cosine Similarity')\n    plt.grid(True, alpha=0.3)\n    plt.xticks(rotation=45)\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from bertopic import BERTopic\nfrom sklearn.cluster import KMeans\n\n# Initialize topic model with GPU-accelerated UMAP\ntopic_model = BERTopic(\n    umap_model_params={'n_neighbors': 15, 'metric': 'cosine', 'random_state': 42},\n    hdbscan_model=KMeans(n_clusters=20, random_state=42),\n    nr_topics=20,  # Force consistent number of topics\n    calculate_probabilities=True\n)\n\n# Cluster speech content\nspeech_docs = speeches_data['text'].tolist()  # Assuming text column exists\nspeech_topics, _ = topic_model.fit_transform(speech_docs, embeddings=speeches_tensor.cpu().numpy())\n\n# Cluster news content\nnews_docs = news_data['content'].tolist()  # Assuming content column exists\nnews_topics, _ = topic_model.transform(news_docs, embeddings=news_tensor.cpu().numpy())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# =====================================================================\n# 8. Prepare Regression Dataset\n# =====================================================================\n# Create topic dominance features\ndef get_dominant_topic_share(topics, probabilities):\n    dominant_topic = np.argmax(probabilities, axis=1)\n    return pd.Series(dominant_topic).value_counts(normalize=True).to_dict()\n\n# Speech topic features\nspeeches_data['topic_dist'] = speeches_data.apply(\n    lambda x: get_dominant_topic_share(speech_topics, x['probabilities']), axis=1\n)\ndaily_speech_topics = speeches_data.groupby('date')['topic_dist'].agg(\n    lambda x: pd.Series(x.sum()).fillna(0)\n).add_prefix('speech_topic_')\n\n# News topic features\nnews_data['topic_dist'] = news_data.apply(\n    lambda x: get_dominant_topic_share(news_topics, x['probabilities']), axis=1\n)\ndaily_news_topics = news_data.groupby('Date')['topic_dist'].agg(\n    lambda x: pd.Series(x.sum()).fillna(0)\n).add_prefix('news_topic_')\n\n# Merge with similarity data\nregression_df = final_df.merge(\n    daily_speech_topics, \n    left_on='date', \n    right_index=True,\n    how='left'\n).merge(\n    daily_news_topics,\n    left_on='date',\n    right_index=True,\n    how='left'\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# 9. Time-Series Regression Model\n# =====================================================================\nimport statsmodels.api as sm\nfrom linearmodels import PanelOLS\n\n# Create temporal features\nregression_df['day_of_week'] = regression_df['date'].dt.dayofweek\nregression_df['time_trend'] = np.arange(len(regression_df))\n\n# Lagged similarity (t-1)\nregression_df['lagged_similarity'] = regression_df['cosine_similarity'].shift(1)\n\n# Speech occurrence indicator\nregression_df['speech_occurred'] = regression_df['date'].isin(speeches_data['date']).astype(int)\n\n# Prepare formula\ntopic_terms = ' + '.join([f'speech_topic_{i}' for i in range(20)])\nformula = f'''\ncosine_similarity ~ \n    speech_occurred +\n    {topic_terms} +\n    news_topic_0 + news_topic_1 + news_topic_2 +  # Select key news topics\n    lagged_similarity +\n    C(day_of_week) + \n    time_trend\n'''\n\n# Fit model with HAC standard errors\nmodel = sm.OLS.from_formula(\n    formula, \n    data=regression_df.dropna()\n).fit(\n    cov_type='HAC',\n    cov_kwds={'maxlags': 3},\n    use_t=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# 10. Diagnostics & Visualization\n# =====================================================================\nprint(model.summary())\n\n# Plot significant coefficients\nsignificant_results = model.params[model.pvalues < 0.05]\nplt.figure(figsize=(10, 6))\nsignificant_results.plot(kind='barh')\nplt.title('Significant Predictors of Media-Speech Alignment')\nplt.xlabel('Coefficient Size')\nplt.ylabel('Predictors')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Save model results\nwith open('regression_results.txt', 'w') as f:\n    f.write(str(model.summary()))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}