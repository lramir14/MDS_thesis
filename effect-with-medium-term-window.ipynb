{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10783414,"sourceType":"datasetVersion","datasetId":6691461},{"sourceId":10788152,"sourceType":"datasetVersion","datasetId":6602831}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nos.listdir('/kaggle/input') \n\nembeddings_path = \"/kaggle/input/embeddings\"\nprint(os.listdir(embeddings_path))\nimport pandas as pd\n\nspeeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\nnews_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n\n# Display first few rows\nprint(\"News Data:\")\nprint(news_embeddings.head())\n\nprint(\"\\nSpeeches Data:\")\nprint(speeches_embeddings.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:36:01.889929Z","iopub.execute_input":"2025-04-09T19:36:01.890226Z","iopub.status.idle":"2025-04-09T19:42:41.129083Z","shell.execute_reply.started":"2025-04-09T19:36:01.890199Z","shell.execute_reply":"2025-04-09T19:42:41.125351Z"}},"outputs":[{"name":"stdout","text":"['speeches_with_embeddings.csv', 'speeches_embeddings_sentiment.csv', 'news_embeddings_sentiment.csv', 'news_with_embeddings.csv']\nNews Data:\n          Index                                               Link  \\\n0  1_01_12_2018  https://www.bbc.com/mundo/noticias-america-lat...   \n1  2_01_12_2018  https://politica.expansion.mx/presidencia/2018...   \n2  3_01_12_2018  https://oem.com.mx/elsoldemexico/mexico/en-don...   \n3  4_01_12_2018  https://politica.expansion.mx/presidencia/2018...   \n4  5_01_12_2018  https://www.eleconomista.com.mx/politica/Nicol...   \n\n                                              Domain  \\\n0  BBC\\nToma de protesta de AMLO: las 5 tradicion...   \n1  Expansión Política\\nAMLO rinde protesta y prom...   \n2  El Sol de México\\n¿Hay Ley Seca este 1 de dici...   \n3  Expansión Política\\nAMLO es un \"líder persiste...   \n4  El Economista\\nNicolás Maduro llega a Palacio ...   \n\n                                               Title        Date  \\\n0  Toma de protesta de AMLO: las 5 tradiciones qu...  2018-12-01   \n1        AMLO rinde protesta y promete no reelegirse  2018-12-01   \n2  ¿Hay Ley Seca este 1 de diciembre por cambio d...  2018-12-01   \n3  AMLO es un \"líder persistente\", dice la superc...  2018-12-01   \n4  Nicolás Maduro llega a Palacio Nacional; no as...  2018-12-01   \n\n                                             Content month_abbr  \\\n0  Fuente de la imagen, Getty Images Desde su cam...        dic   \n1  Síguenos en nuestras redes sociales: CIUDAD DE...        dic   \n2  Por la toma de posesión de Andrés Manuel López...        dic   \n3  Síguenos en nuestras redes sociales: CIUDAD DE...        dic   \n4  Lectura 3:00 min Nicolás Maduro arribó este sá...        dic   \n\n                                   processed_content  \\\n0  fuente imagen getty images campaña presidencia...   \n1  síguenos redes sociales ciudad méxico adnpolít...   \n2  toma posesión andrés manuel lópez obrador pres...   \n3  síguenos redes sociales ciudad méxico adnpolít...   \n4  lectura 3:00 min nicolás maduro   arribó sábad...   \n\n                                         news_chunks  \\\n0  fuente imagen getty images campaña presidencia...   \n1  síguenos redes sociales ciudad méxico adnpolít...   \n2  toma posesión andrés manuel lópez obrador pres...   \n3  síguenos redes sociales ciudad méxico adnpolít...   \n4  lectura 3:00 min nicolás maduro   arribó sábad...   \n\n                                     news_embeddings  \n0  [ 3.77706587e-01  9.13102329e-02 -1.38176888e-...  \n1  [ 2.61866331e-01  2.99132258e-01  1.76378831e-...  \n2  [ 5.06281674e-01  3.32773924e-02 -3.04715186e-...  \n3  [ 3.07641208e-01  7.52940923e-02 -5.62011823e-...  \n4  [ 3.04801702e-01  3.39728445e-01  3.13091815e-...  \n\nSpeeches Data:\n   Unnamed: 0  X  speech_id  \\\n0           1  1          1   \n1           1  1          1   \n2           1  1          1   \n3           1  1          1   \n4           1  1          1   \n\n                                               title  \\\n0  Versión estenográfica de la conferencia de pre...   \n1  Versión estenográfica de la conferencia de pre...   \n2  Versión estenográfica de la conferencia de pre...   \n3  Versión estenográfica de la conferencia de pre...   \n4  Versión estenográfica de la conferencia de pre...   \n\n                                                urls  \\\n0  https://lopezobrador.org.mx/2024/01/09/version...   \n1  https://lopezobrador.org.mx/2024/01/09/version...   \n2  https://lopezobrador.org.mx/2024/01/09/version...   \n3  https://lopezobrador.org.mx/2024/01/09/version...   \n4  https://lopezobrador.org.mx/2024/01/09/version...   \n\n                                             content        date  \\\n0  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n1  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n2  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n3  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n4  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n\n                                       speech_chunks  \\\n0  2024: Año de Felipe Carrillo Puerto, benemérit...   \n1  Son los tres informes. Pero antes quiero dar a...   \n2  Entonces, ya voy a estar en TikTok y quiero in...   \n3  Entonces, ofrecer una disculpa y enviarle un a...   \n4  Me sumo al deseo de este año que sea lo mejor ...   \n\n                                   speech_embeddings  \n0  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n1  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n2  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n3  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n4  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# Convert and verify date columns\nnews_embeddings['news_date'] = pd.to_datetime(news_embeddings['Date'])\nspeeches_embeddings['speech_date'] = pd.to_datetime(speeches_embeddings['date'])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n#Temporal window calculation and expansion\ndef generate_temporal_pairs(news_df, speeches_df, window_days=3):\n    \"\"\"Generate news-speech pairs within a symmetric temporal window (-4 to +4 days)\"\"\"\n    pairs = []\n    chunk_size = 2000\n    news_chunks = np.array_split(news_df, len(news_df) // chunk_size + 1)\n    \n    for chunk in news_chunks:\n        for _, row in chunk.iterrows():\n            news_date = row['news_date']\n            start_date = news_date - pd.Timedelta(days=window_days) \n            end_date = news_date + pd.Timedelta(days=window_days)  \n            \n            mask = (speeches_df['speech_date'] >= start_date) & (speeches_df['speech_date'] <= end_date)\n            speech_ids = speeches_df[mask].index.tolist()\n            pairs.extend([(row.name, s_id) for s_id in speech_ids])\n    \n    return pd.DataFrame(pairs, columns=['news_id', 'speech_id'])\n\n\nalignment_df = generate_temporal_pairs(news_embeddings, speeches_embeddings)\n\n#optimized embeddings calculation\ndef load_embeddings_half(df, col_name):\n    embeddings = []\n    for i, row in df.iterrows():\n        if isinstance(row[col_name], str):\n            arr = np.fromstring(row[col_name].strip(\"[]\"), sep=\" \", dtype=np.float16)\n        else:\n            arr = np.array(row[col_name], dtype=np.float16)\n        embeddings.append(torch.tensor(arr, device=device).half())\n        if i % 1000 == 0: torch.cuda.empty_cache()\n    return torch.stack(embeddings)\n\nnews_tensor = load_embeddings_half(news_embeddings, 'news_embeddings')\nspeeches_tensor = load_embeddings_half(speeches_embeddings, 'speech_embeddings')\n\n#Batched cosine similarity computation\ndef compute_cosine_similarities(pairs_df, news_emb, speech_emb, batch_size=8192):\n    news_norm = F.normalize(news_emb, p=2, dim=1)\n    speech_norm = F.normalize(speech_emb, p=2, dim=1)\n    similarities = []\n    for i in range(0, len(pairs_df), batch_size):\n        batch = pairs_df.iloc[i:i+batch_size]\n        news_batch = news_norm[batch['news_id'].values]\n        speech_batch = speech_norm[batch['speech_id'].values]\n        similarities.append(F.cosine_similarity(news_batch, speech_batch).cpu().numpy())\n        del news_batch, speech_batch\n        torch.cuda.empty_cache()\n    return np.concatenate(similarities)\n\nalignment_df['cosine_similarity'] = compute_cosine_similarities(alignment_df, news_tensor, speeches_tensor)\n\n#include metadata to the embeddings to track temporal dependencies\ndef add_temporal_features(pairs_df, news_df, speeches_df):\n    pairs_df = pairs_df.merge(\n        news_df[['news_date']],\n        left_on='news_id',\n        right_index=True\n    ).merge(\n        speeches_df[['speech_date']],\n        left_on='speech_id',\n        right_index=True\n    )\n    pairs_df['days_diff'] = (pairs_df['news_date'] - pairs_df['speech_date']).dt.days\n    return pairs_df\n\nenriched_df = add_temporal_features(alignment_df, news_embeddings, speeches_embeddings)\n\n#Save data to avoid rerunning everything again \nenriched_df.to_parquet('news_speech_similarities.parquet', engine='pyarrow', compression='zstd')\nprint(\"Processing complete. Results saved with columns:\", enriched_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:42:41.138228Z","iopub.execute_input":"2025-04-09T19:42:41.138669Z","iopub.status.idle":"2025-04-09T19:47:20.980478Z","shell.execute_reply.started":"2025-04-09T19:42:41.138609Z","shell.execute_reply":"2025-04-09T19:47:20.978910Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"Processing complete. Results saved with columns: ['news_id', 'speech_id', 'cosine_similarity', 'news_date', 'speech_date', 'days_diff']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef load_safe_data():\n    \"\"\"Load data with proper dtype conversions\"\"\"\n    df = pd.read_parquet('news_speech_similarities.parquet').astype({\n        'news_id': 'int64',\n        'speech_id': 'int64',\n        'cosine_similarity': 'float32'  # Convert from float16 to float32\n    })\n    df['news_date'] = pd.to_datetime(df['news_date'])\n    return df\n\n# Load processed data\ndf = load_safe_data()\n\n#Code to include temporal features\ndef plot_temporal_trends(df, resample_freq='W', rolling_window=7):\n    \"\"\"Plot similarity trends with type-safe processing\"\"\"\n    plt.figure(figsize=(16, 8))\n    \n    # Ensure float32 type\n    temp_df = df[['news_date', 'cosine_similarity']].copy()\n    temp_df['cosine_similarity'] = temp_df['cosine_similarity'].astype('float32')\n    \n    # Resample data\n    df_temp = temp_df.set_index('news_date')\n    resampled = df_temp['cosine_similarity'].resample(resample_freq).mean()\n    \n    # Create rolling average\n    rolling_mean = resampled.rolling(window=rolling_window, center=True).mean()\n    \n    # Plot with enhanced formatting\n    plt.plot(resampled.index, resampled.values, \n            alpha=0.3, label='Weekly Average')\n    plt.plot(rolling_mean.index, rolling_mean.values,\n            linewidth=2, label=f'{rolling_window}-week Rolling Mean')\n    \n    plt.title('News-Speech Content Alignment Over Time')\n    plt.xlabel('Date', fontsize=12)\n    plt.ylabel('Cosine Similarity', fontsize=12)\n    plt.legend()\n    plt.grid(alpha=0.3)\n    plt.gca().spines[['top', 'right']].set_visible(False)\n    plt.tight_layout()\n    plt.show()\n\n#Testing execution code, tho not for testing \nif __name__ == \"__main__\":\n    df = load_safe_data()\n    \n    # Verify data types\n    print(\"Data types:\\n\", df.dtypes)\n    \n    # Plot temporal trends only\n    plot_temporal_trends(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:49:53.671885Z","iopub.status.idle":"2025-04-09T10:49:53.672353Z","shell.execute_reply":"2025-04-09T10:49:53.672156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Load processed data\ndf = pd.read_parquet('news_speech_similarities.parquet') #We can use this code when we compute everything again, else we have to roead the parquet\n#df = pd.read_parquet(SIMILARITIES_PATH) #Piece of code when we have the path defined, otherwise it'll work by loading them again. \n# Convert to datetime and normalize (remove time components)\ndf['news_date'] = pd.to_datetime(df['news_date']).dt.normalize()\ndf['year'] = df['news_date'].dt.year\n\n# Create daily aggregates with std dev\ndaily_agg = df.groupby('news_date')['cosine_similarity'].agg(['mean', 'std']).reset_index()\ndaily_agg.columns = ['date', 'cosine_similarity', 'std_dev']\n\n# Extend full_dates to include October 2024 explicitly\nend_date = pd.to_datetime('2024-10-31')  # Adjust as needed\nfull_dates = pd.date_range(\n    start=daily_agg['date'].min(), \n    end=end_date, \n    freq='D'\n)\ndaily_agg = daily_agg.set_index('date').reindex(full_dates).reset_index().rename(columns={'index': 'date'})\n\n# Calculate bounds\ndaily_agg['upper_bound'] = daily_agg['cosine_similarity'] + daily_agg['std_dev'].fillna(0)\ndaily_agg['lower_bound'] = daily_agg['cosine_similarity'] - daily_agg['std_dev'].fillna(0)\n\n# Create monthly aggregates (fill NaN with 0 for plotting)\nmonthly_agg = daily_agg.set_index('date').resample('M')['cosine_similarity'].mean().fillna(0).reset_index()\nmonthly_agg['month_label'] = monthly_agg['date'].dt.strftime('%b\\n%Y')\n\n# Get unique years present in data\nyears = daily_agg['date'].dt.year.unique()\n\n# Set up plot\nplt.figure(figsize=(18, 8 * len(years)))\n\nfor i, year in enumerate(years, 1):\n    year_mask = daily_agg['date'].dt.year == year\n    yearly_daily = daily_agg[year_mask]\n    yearly_monthly = monthly_agg[monthly_agg['date'].dt.year == year]\n    \n    if yearly_daily.empty:\n        continue\n    \n    ax = plt.subplot(len(years), 1, i)\n    \n    # Daily plot with variability\n    ax.plot(yearly_daily['date'], \n            yearly_daily['cosine_similarity'], \n            color='#2ca02c', \n            linewidth=1.5,\n            label='Daily Average')\n    \n    ax.fill_between(yearly_daily['date'],\n                    yearly_daily['upper_bound'],\n                    yearly_daily['lower_bound'],\n                    color='gray', alpha=0.3, \n                    label='Daily Std Dev')\n    \n    # Monthly markers (plot even if value is 0)\n    ax.scatter(yearly_monthly['date'], \n               yearly_monthly['cosine_similarity'],\n               color='darkblue', \n               s=100,\n               zorder=5,\n               label='Monthly Average')\n    \n    # Annotate monthly values (skip if 0)\n    for _, row in yearly_monthly.iterrows():\n        if row['cosine_similarity'] != 0:\n            ax.text(row['date'], row['cosine_similarity']+0.02,\n                    f\"{row['cosine_similarity']:.2f}\",\n                    ha='center', va='bottom',\n                    fontsize=9, color='darkblue')\n    \n    # Highlight missing days\n    missing_mask = yearly_daily['cosine_similarity'].isna()\n    ax.fill_between(yearly_daily['date'],\n                    yearly_daily['cosine_similarity'].min() - 0.1,\n                    yearly_daily['cosine_similarity'].max() + 0.1,\n                    where=missing_mask,\n                    color='red', alpha=0.1,\n                    label='Missing Days')\n    \n    # Formatting\n    ax.set_title(f'{year} Daily/Monthly Speech-News Cosine Similarity', pad=20)\n    ax.set_xlabel('')\n    ax.set_ylabel('Cosine Similarity', fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='upper right')\n    \n    # Set monthly x-ticks\n    ax.set_xticks(yearly_monthly['date'])\n    ax.set_xticklabels(yearly_monthly['month_label'])\n    \n    # Set y-axis limits\n    y_min = max(daily_agg['cosine_similarity'].min() - 0.1, 0)\n    y_max = min(daily_agg['cosine_similarity'].max() + 0.1, 1)\n    ax.set_ylim(y_min, y_max)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:47:20.981917Z","iopub.execute_input":"2025-04-09T19:47:20.982714Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-3-cc786d441fcb>:27: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  monthly_agg = daily_agg.set_index('date').resample('M')['cosine_similarity'].mean().fillna(0).reset_index()\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\nfrom matplotlib.colors import Normalize\n\n#Function to extract top media outlets \ndef extract_outlet(url):\n    \"\"\"Extract media outlet name from news article URL\"\"\"\n    patterns = [\n        r\"https?://(?:www\\.)?([^/.]+)\\.\",\n        r\"https?://([^/]+)/\",\n        r\"([a-z0-9-]+)\\.(com|org|net|edu|gov)\"\n    ]\n    for pattern in patterns:\n        match = re.search(pattern, url, re.IGNORECASE)\n        if match:\n            return match.group(1).lower()\n    return \"unknown\"\n\n\ndef plot_temporal_alignment_heatmap(df, news_metadata, resample_freq='M'):\n    \"\"\"Show alignment patterns for top 10 outlets\"\"\"\n    # Merge data and clean\n    merged_df = df.merge(\n        news_metadata[['news_id', 'outlet']],\n        on='news_id',\n        how='inner'\n    ).dropna(subset=['cosine_similarity'])\n    \n    # Get top 10 outlets by article count\n    top_outlets = merged_df.groupby('outlet')['news_id'].nunique() \\\n                          .nlargest(10).index.tolist()\n    \n    # Filter to top outlets only\n    filtered_df = merged_df[merged_df['outlet'].isin(top_outlets)]\n    \n    # Prepare temporal data\n    filtered_df['period'] = filtered_df['news_date'].dt.to_period(resample_freq)\n    pivot_data = filtered_df.groupby(['outlet', 'period'])['cosine_similarity'] \\\n                          .mean().unstack().fillna(0)\n    \n    # Sort outlets by total articles (descending)\n    outlet_order = filtered_df.groupby('outlet')['news_id'].nunique() \\\n                             .sort_values(ascending=False).index\n    pivot_data = pivot_data.loc[outlet_order]\n    \n    # Fixed sizing for 10 outlets\n    plt.figure(figsize=(18, 8))  # Width 18\", Height 8\"\n    \n    # Plot heatmap\n    ax = sns.heatmap(\n        pivot_data,\n        cmap=sns.light_palette(\"#cc0000\", as_cmap=True), #color palette and heatmap \n        norm=Normalize(vmin=0, vmax=0.5),\n        linewidths=0.3,\n        linecolor='lightgray',\n        cbar_kws={'label': 'Alignment Score (0-0.5 scale)'}\n    )\n    \n    # Formatting\n    ax.set_title('Top 10 Media Outlets: Alignment with Presidential Speeches\\n', pad=20, fontsize=14)\n    ax.set_xlabel('Time Period', labelpad=15, fontsize=12)\n    ax.set_ylabel('Media Outlet', labelpad=15, fontsize=12)\n    ax.set_xticklabels(\n        [col.strftime('%b\\n%Y') if i%3==0 else '' for i, col in enumerate(pivot_data.columns)],\n        rotation=0,\n        fontsize=9\n    )\n    plt.tight_layout()\n    plt.show()\n\n#Code to execute instructions, not testing. \n\nif __name__ == \"__main__\":\n    try:\n        # Load data\n        news_df = news_embeddings\n        similarities_df = pd.read_parquet('news_speech_similarities.parquet')\n        \n        # Add outlet information\n        news_df['outlet'] = news_df['Link'].apply(extract_outlet)\n        news_metadata = news_df[['outlet']].reset_index().rename(columns={'index': 'news_id'})\n        \n        # Generate visualization\n        plot_temporal_alignment_heatmap(\n            similarities_df,\n            news_metadata,\n            resample_freq='M'  # Monthly aggregation\n        )\n        \n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        print(\"Troubleshooting steps:\")\n        print(f\"1. Verify files exist in: {DATA_DIR}\")\n        print(f\"2. Check file contents: {os.listdir(DATA_DIR)}\")\n        print(\"3. Confirm required columns exist in dataframes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:05:08.265074Z","iopub.execute_input":"2025-04-01T12:05:08.265338Z","iopub.status.idle":"2025-04-01T12:05:20.082736Z","shell.execute_reply.started":"2025-04-01T12:05:08.265316Z","shell.execute_reply":"2025-04-01T12:05:20.081804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Statistical Analysis \n\nArianna emphasized the unit or level. Meaning is this article level or not? And then, if it is then it makes sense to have it as an article level with time-dependencies. ","metadata":{}},{"cell_type":"code","source":"\n# 1. Daily Aggregated Data\n#I have to establish within the text the level (article level and speech level)\nimport pandas as pd\nimport statsmodels.api as sm\n\n\ndf_daily = daily_agg[['date', 'cosine_similarity']].copy()\ndf_daily['time'] = (df_daily['date'] - df_daily['date'].min()).dt.days\n\n# Drop missing values\ndf_daily = df_daily.dropna(subset=['cosine_similarity'])\n\n\n# Basic OLS: Similarity ~ Time\nX = sm.add_constant(df_daily['time'])\ny = df_daily['cosine_similarity']\n\nmodel_time = sm.OLS(y, X).fit()\nprint(\"=== Basic Time Regression ===\")\nprint(model_time.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:37:40.437620Z","iopub.execute_input":"2025-04-08T09:37:40.437991Z","iopub.status.idle":"2025-04-08T09:37:40.474959Z","shell.execute_reply.started":"2025-04-08T09:37:40.437962Z","shell.execute_reply":"2025-04-08T09:37:40.473787Z"}},"outputs":[{"name":"stdout","text":"=== Basic Time Regression ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.048\nModel:                            OLS   Adj. R-squared:                  0.047\nMethod:                 Least Squares   F-statistic:                     103.5\nDate:                Tue, 08 Apr 2025   Prob (F-statistic):           9.12e-24\nTime:                        09:37:40   Log-Likelihood:                 3876.5\nNo. Observations:                2078   AIC:                            -7749.\nDf Residuals:                    2076   BIC:                            -7738.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.2811      0.002    172.486      0.000       0.278       0.284\ntime        1.355e-05   1.33e-06     10.175      0.000    1.09e-05    1.62e-05\n==============================================================================\nOmnibus:                       18.743   Durbin-Watson:                   1.472\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               25.018\nSkew:                          -0.114   Prob(JB):                     3.69e-06\nKurtosis:                       3.487   Cond. No.                     2.43e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.43e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Effects of outlets. ","metadata":{}},{"cell_type":"code","source":"import re\n# again extracting top media outlets. Maybe there is a more efficient way to do this. \ndef extract_outlet(url):\n    patterns = [r\"https?://(?:www\\.)?([^/.]+)\\.\"]\n    match = re.search(patterns[0], str(url), re.IGNORECASE)\n    return match.group(1).lower() if match else \"unknown\"\n\n# Create outlet column in news_embeddings\nnews_embeddings['outlet'] = news_embeddings['Link'].apply(extract_outlet)\n\n# Verify\nprint(\"News columns:\", news_embeddings.columns.tolist())\nprint(\"Sample outlets:\", news_embeddings['outlet'].unique()[:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(enriched_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:35:33.975714Z","iopub.execute_input":"2025-04-09T16:35:33.976068Z","iopub.status.idle":"2025-04-09T16:35:33.987393Z","shell.execute_reply.started":"2025-04-09T16:35:33.976038Z","shell.execute_reply":"2025-04-09T16:35:33.986123Z"}},"outputs":[{"name":"stdout","text":"          news_id  speech_id  cosine_similarity  news_date speech_date  \\\n0               0     150494           0.315430 2018-12-01  2018-12-04   \n1               0     150495           0.315430 2018-12-01  2018-12-04   \n2               0     150496           0.315430 2018-12-01  2018-12-04   \n3               0     150497           0.315430 2018-12-01  2018-12-04   \n4               0     150498           0.315430 2018-12-01  2018-12-04   \n...           ...        ...                ...        ...         ...   \n24418166    42773     150618           0.273682 2024-10-03  2024-09-30   \n24418167    42773     150619           0.273682 2024-10-03  2024-09-30   \n24418168    42773     150620           0.273682 2024-10-03  2024-09-30   \n24418169    42773     150621           0.273682 2024-10-03  2024-09-30   \n24418170    42773     150622           0.273682 2024-10-03  2024-09-30   \n\n          days_diff  \n0                -3  \n1                -3  \n2                -3  \n3                -3  \n4                -3  \n...             ...  \n24418166          3  \n24418167          3  \n24418168          3  \n24418169          3  \n24418170          3  \n\n[24418171 rows x 6 columns]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Now this will work\n\nenriched_df = enriched_df.merge(\n    news_embeddings[['outlet']],\n    left_on='news_id',\n    right_index=True,\n    how='left'\n)\n\n# Final check\nprint(\"\\nColumns in enriched_df:\", enriched_df.columns.tolist())\nprint(\"Sample outlets:\", enriched_df['outlet'].unique()[:5])","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-04-09T19:47:34.272464Z","shell.execute_reply.started":"2025-04-09T19:47:28.979374Z","shell.execute_reply":"2025-04-09T19:47:34.271200Z"}},"outputs":[{"name":"stdout","text":"\nColumns in enriched_df: ['news_id', 'speech_id', 'cosine_similarity', 'news_date', 'speech_date', 'days_diff', 'outlet']\nSample outlets: ['bbc' 'politica' 'oem' 'eleconomista' 'milenio']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(enriched_df['days_diff'].nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:22:07.354886Z","iopub.status.idle":"2025-04-09T15:22:07.355200Z","shell.execute_reply":"2025-04-09T15:22:07.355071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import statsmodels.api as sm\n\n# Simple regression: cosine similarity ~ Days between speech and news\nX_time = sm.add_constant(enriched_df[['days_diff']])\ny = enriched_df['cosine_similarity']\n\nmodel_time = sm.OLS(y, X_time).fit(cov_type='HC3')\nprint(\"=== Time Trend Model ===\")\nprint(model_time.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T09:26:25.934477Z","iopub.execute_input":"2025-04-09T09:26:25.934857Z","iopub.status.idle":"2025-04-09T09:26:33.429086Z","shell.execute_reply.started":"2025-04-09T09:26:25.934827Z","shell.execute_reply":"2025-04-09T09:26:33.427868Z"}},"outputs":[{"name":"stdout","text":"=== Time Trend Model ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.001\nModel:                            OLS   Adj. R-squared:                  0.001\nMethod:                 Least Squares   F-statistic:                 1.285e+04\nDate:                Wed, 09 Apr 2025   Prob (F-statistic):               0.00\nTime:                        09:26:33   Log-Likelihood:             2.6827e+07\nNo. Observations:            24418171   AIC:                        -5.365e+07\nDf Residuals:                24418169   BIC:                        -5.365e+07\nDf Model:                           1                                         \nCovariance Type:                  HC3                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.3045   1.63e-05   1.86e+04      0.000       0.304       0.305\ndays_diff     -0.0009   8.05e-06   -113.366      0.000      -0.001      -0.001\n==============================================================================\nOmnibus:                    76450.416   Durbin-Watson:                   0.012\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            77259.480\nSkew:                          -0.135   Prob(JB):                         0.00\nKurtosis:                       3.052   Cond. No.                         2.00\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.api as sm\n\n# 1. Get top outlets. So again this is repeated \ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()#im duplocating data here \n# 2. Create dummy variables with proper naming \ndummies = pd.get_dummies(enriched_df['outlet'], prefix='outlet')\n# 3. Filter columns using PREFIXED names\ndummy_columns = [f'outlet_{outlet}' for outlet in top_outlets if f'outlet_{outlet}' in dummies.columns]\ndummies = dummies[dummy_columns]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:53:12.927889Z","iopub.execute_input":"2025-04-09T12:53:12.928388Z","iopub.status.idle":"2025-04-09T12:53:19.521311Z","shell.execute_reply.started":"2025-04-09T12:53:12.928338Z","shell.execute_reply":"2025-04-09T12:53:19.520275Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# 4. Ensure numerical data types\ndummies = dummies.astype(int)  # Convert boolean dummies to integers\nenriched_df['days_diff'] = pd.to_numeric(enriched_df['days_diff'], errors='coerce')\n\n# 5. Handle missing values\nvalid_data = enriched_df[['days_diff', 'cosine_similarity']].join(dummies).dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:53:19.522510Z","iopub.execute_input":"2025-04-09T12:53:19.523235Z","iopub.status.idle":"2025-04-09T12:53:31.314117Z","shell.execute_reply.started":"2025-04-09T12:53:19.523190Z","shell.execute_reply":"2025-04-09T12:53:31.312879Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 6. Create regression matrix\nX = sm.add_constant(valid_data[['days_diff'] + dummy_columns])\ny = valid_data['cosine_similarity']\n\n# 7. Run regression\nmodel = sm.OLS(y, X).fit(cov_type='HC3')\nprint(\"\\n=== Outlet Fixed Effects Model ===\")\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T09:32:16.219984Z","iopub.execute_input":"2025-04-09T09:32:16.220446Z","iopub.status.idle":"2025-04-09T09:33:01.229098Z","shell.execute_reply.started":"2025-04-09T09:32:16.220415Z","shell.execute_reply":"2025-04-09T09:33:01.227667Z"}},"outputs":[{"name":"stdout","text":"\n=== Outlet Fixed Effects Model ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.077\nModel:                            OLS   Adj. R-squared:                  0.077\nMethod:                 Least Squares   F-statistic:                 2.139e+05\nDate:                Wed, 09 Apr 2025   Prob (F-statistic):               0.00\nTime:                        09:33:01   Log-Likelihood:             2.7793e+07\nNo. Observations:            24418171   AIC:                        -5.559e+07\nDf Residuals:                24418159   BIC:                        -5.559e+07\nDf Model:                          11                                         \nCovariance Type:                  HC3                                         \n=======================================================================================\n                          coef    std err          z      P>|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                   0.2969   2.96e-05      1e+04      0.000       0.297       0.297\ndays_diff              -0.0009   7.79e-06   -110.535      0.000      -0.001      -0.001\noutlet_infobae         -0.0141   4.26e-05   -330.167      0.000      -0.014      -0.014\noutlet_proceso          0.0410   4.78e-05    856.546      0.000       0.041       0.041\noutlet_oem              0.0509   7.38e-05    689.614      0.000       0.051       0.051\noutlet_politica         0.0085   7.27e-05    116.921      0.000       0.008       0.009\noutlet_elfinanciero     0.0097   7.75e-05    125.191      0.000       0.010       0.010\noutlet_forbes           0.0051   8.53e-05     59.805      0.000       0.005       0.005\noutlet_elpais          -0.0249   8.88e-05   -280.021      0.000      -0.025      -0.025\noutlet_eleconomista     0.0460   9.66e-05    475.710      0.000       0.046       0.046\noutlet_lasillarota      0.0500   9.04e-05    552.835      0.000       0.050       0.050\noutlet_milenio          0.0201      0.000    160.837      0.000       0.020       0.020\n==============================================================================\nOmnibus:                    56636.759   Durbin-Watson:                   0.012\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            58176.857\nSkew:                          -0.106   Prob(JB):                         0.00\nKurtosis:                       3.111   Cond. No.                         16.4\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Regression with temporal dependencies (dummies per month and year) ","metadata":{}},{"cell_type":"code","source":"print(enriched_df.columns)\nprint(enriched_df[\"news_date\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:39:59.605426Z","iopub.execute_input":"2025-04-09T14:39:59.605856Z","iopub.status.idle":"2025-04-09T14:39:59.615111Z","shell.execute_reply.started":"2025-04-09T14:39:59.605826Z","shell.execute_reply":"2025-04-09T14:39:59.613708Z"}},"outputs":[{"name":"stdout","text":"Index(['news_id', 'speech_id', 'cosine_similarity', 'news_date', 'speech_date',\n       'days_diff', 'outlet'],\n      dtype='object')\n0          2018-12-01\n1          2018-12-01\n2          2018-12-01\n3          2018-12-01\n4          2018-12-01\n              ...    \n24418166   2024-10-03\n24418167   2024-10-03\n24418168   2024-10-03\n24418169   2024-10-03\n24418170   2024-10-03\nName: news_date, Length: 24418171, dtype: datetime64[ns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"if 'outlet_x' in enriched_df.columns and 'outlet_y' in enriched_df.columns:\n    enriched_df = enriched_df.drop(columns=['outlet_x', 'outlet_y'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:39:51.043457Z","iopub.execute_input":"2025-04-09T10:39:51.043776Z","iopub.status.idle":"2025-04-09T10:39:51.060919Z","shell.execute_reply.started":"2025-04-09T10:39:51.043746Z","shell.execute_reply":"2025-04-09T10:39:51.059673Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 1. Parse the date columns (if not already done)\nenriched_df['news_date'] = pd.to_datetime(enriched_df['news_date'])\nenriched_df['speech_date'] = pd.to_datetime(enriched_df['speech_date'])\n\n# 2. Create 'year-month' variable from news_date\nenriched_df['year_month'] = enriched_df['news_date'].dt.to_period('M')\n\n# 3. Create dummies for each month\nmonth_dummies = pd.get_dummies(enriched_df['year_month'], prefix='month')\n\n# 4. Rebuild valid_data matrix (if not done yet)\ndummies = pd.get_dummies(enriched_df['outlet'], prefix='outlet')\ndummy_columns = [f'outlet_{outlet}' for outlet in top_outlets if f'outlet_{outlet}' in dummies.columns]\ndummies = dummies[dummy_columns].astype(int)\n# 5. Combine everything for regression\nvalid_data = enriched_df[['days_diff', 'cosine_similarity']].join(dummies).join(month_dummies)\nvalid_data = valid_data.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:35:51.467700Z","iopub.execute_input":"2025-04-09T12:35:51.468037Z","iopub.status.idle":"2025-04-09T12:36:09.965749Z","shell.execute_reply.started":"2025-04-09T12:35:51.468010Z","shell.execute_reply":"2025-04-09T12:36:09.964893Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"enriched_df['speech_month'] = enriched_df['speech_date'].dt.strftime('%Y-%m')\nenriched_df['news_month'] = enriched_df['news_date'].dt.strftime('%Y-%m')\n\nenriched_df['speech_year'] = enriched_df['speech_date'].dt.year\nenriched_df['news_year'] = enriched_df['news_date'].dt.year\n\n# This will now give: '2018-12', as a string\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:37:04.265709Z","iopub.execute_input":"2025-04-09T12:37:04.266118Z","iopub.status.idle":"2025-04-09T12:41:01.872401Z","shell.execute_reply.started":"2025-04-09T12:37:04.266071Z","shell.execute_reply":"2025-04-09T12:41:01.871247Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(enriched_df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:40:04.211403Z","iopub.execute_input":"2025-04-09T14:40:04.211787Z","iopub.status.idle":"2025-04-09T14:40:04.221822Z","shell.execute_reply.started":"2025-04-09T14:40:04.211757Z","shell.execute_reply":"2025-04-09T14:40:04.220244Z"}},"outputs":[{"name":"stdout","text":"   news_id  speech_id  cosine_similarity  news_date speech_date  days_diff  \\\n0        0     150494            0.31543 2018-12-01  2018-12-04         -3   \n1        0     150495            0.31543 2018-12-01  2018-12-04         -3   \n2        0     150496            0.31543 2018-12-01  2018-12-04         -3   \n3        0     150497            0.31543 2018-12-01  2018-12-04         -3   \n4        0     150498            0.31543 2018-12-01  2018-12-04         -3   \n5        0     150499            0.31543 2018-12-01  2018-12-04         -3   \n6        0     150500            0.31543 2018-12-01  2018-12-04         -3   \n7        0     150501            0.31543 2018-12-01  2018-12-04         -3   \n8        0     150502            0.31543 2018-12-01  2018-12-04         -3   \n9        0     150503            0.31543 2018-12-01  2018-12-04         -3   \n\n  outlet  \n0    bbc  \n1    bbc  \n2    bbc  \n3    bbc  \n4    bbc  \n5    bbc  \n6    bbc  \n7    bbc  \n8    bbc  \n9    bbc  \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#no dummies because of space complexity. \n### More efficient code: \n\ndf = enriched_df.copy()\ndf['time_index'] = (df['news_date'] - df['news_date'].min()).dt.days #Now time_index = 0 corresponds to the earliest news_date, and it increases linearly with time.\n#this variable time index is a numerical encoding of time. It captures the chronological order of the observations. \n\ntop_outlets = df['outlet'].value_counts().nlargest(10).index.tolist()\ndf['outlet_top'] = df['outlet'].where(df['outlet'].isin(top_outlets), 'Other')\n\nX_outlets = pd.get_dummies(df['outlet_top'], prefix='outlet', drop_first=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:58:12.913161Z","iopub.execute_input":"2025-04-09T13:58:12.913709Z","iopub.status.idle":"2025-04-09T13:58:27.511068Z","shell.execute_reply.started":"2025-04-09T13:58:12.913662Z","shell.execute_reply":"2025-04-09T13:58:27.507930Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import statsmodels.api as sm\n\nX = pd.concat([\n    df[['days_diff', 'time_index']],\n    X_outlets\n], axis=1)\n\nX = sm.add_constant(X)  # adds intercept\ny = df['cosine_similarity']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:03:52.254636Z","iopub.execute_input":"2025-04-09T14:03:52.255045Z","iopub.status.idle":"2025-04-09T14:03:53.701235Z","shell.execute_reply.started":"2025-04-09T14:03:52.255006Z","shell.execute_reply":"2025-04-09T14:03:53.700168Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"X = X.astype('float64')\ny = y.astype('float64')\n\nprint(X.dtypes)\nprint(y.dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:04:00.272141Z","iopub.execute_input":"2025-04-09T14:04:00.272540Z","iopub.status.idle":"2025-04-09T14:04:01.311746Z","shell.execute_reply.started":"2025-04-09T14:04:00.272502Z","shell.execute_reply":"2025-04-09T14:04:01.309563Z"}},"outputs":[{"name":"stdout","text":"const                  float64\ndays_diff              float64\ntime_index             float64\noutlet_eleconomista    float64\noutlet_elfinanciero    float64\noutlet_elpais          float64\noutlet_forbes          float64\noutlet_infobae         float64\noutlet_lasillarota     float64\noutlet_milenio         float64\noutlet_oem             float64\noutlet_politica        float64\noutlet_proceso         float64\ndtype: object\nfloat64\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model = sm.OLS(y, X).fit()\nprint(model.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:04:17.840014Z","iopub.execute_input":"2025-04-09T14:04:17.840439Z","iopub.status.idle":"2025-04-09T14:04:47.335153Z","shell.execute_reply.started":"2025-04-09T14:04:17.840404Z","shell.execute_reply":"2025-04-09T14:04:47.333589Z"}},"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.107\nModel:                            OLS   Adj. R-squared:                  0.107\nMethod:                 Least Squares   F-statistic:                 2.444e+05\nDate:                Wed, 09 Apr 2025   Prob (F-statistic):               0.00\nTime:                        14:04:47   Log-Likelihood:             2.8205e+07\nNo. Observations:            24418171   AIC:                        -5.641e+07\nDf Residuals:                24418158   BIC:                        -5.641e+07\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                   0.2640   4.46e-05   5918.118      0.000       0.264       0.264\ndays_diff              -0.0010   7.73e-06   -127.458      0.000      -0.001      -0.001\ntime_index           2.505e-05   2.74e-08    915.544      0.000     2.5e-05    2.51e-05\noutlet_eleconomista     0.0422      0.000    415.579      0.000       0.042       0.042\noutlet_elfinanciero     0.0012   8.02e-05     14.793      0.000       0.001       0.001\noutlet_elpais          -0.0265      0.000   -262.639      0.000      -0.027      -0.026\noutlet_forbes           0.0065   8.69e-05     74.450      0.000       0.006       0.007\noutlet_infobae         -0.0188   4.07e-05   -460.388      0.000      -0.019      -0.019\noutlet_lasillarota      0.0590      0.000    572.232      0.000       0.059       0.059\noutlet_milenio          0.0250      0.000    211.036      0.000       0.025       0.025\noutlet_oem              0.0602   7.37e-05    816.786      0.000       0.060       0.060\noutlet_politica         0.0087   7.34e-05    119.079      0.000       0.009       0.009\noutlet_proceso          0.0336   5.04e-05    666.203      0.000       0.033       0.034\n==============================================================================\nOmnibus:                    59912.868   Durbin-Watson:                   0.013\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            61652.128\nSkew:                          -0.108   Prob(JB):                         0.00\nKurtosis:                       3.116   Cond. No.                     1.24e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.24e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import gc\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:05:46.587717Z","iopub.execute_input":"2025-04-09T14:05:46.588192Z","iopub.status.idle":"2025-04-09T14:05:48.733644Z","shell.execute_reply.started":"2025-04-09T14:05:46.588151Z","shell.execute_reply":"2025-04-09T14:05:48.731968Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"577"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# Efficient code ","metadata":{}},{"cell_type":"code","source":"print(enriched_df.head(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:47:34.273611Z","iopub.execute_input":"2025-04-09T19:47:34.273975Z","iopub.status.idle":"2025-04-09T19:47:34.283877Z","shell.execute_reply.started":"2025-04-09T19:47:34.273889Z","shell.execute_reply":"2025-04-09T19:47:34.282513Z"}},"outputs":[{"name":"stdout","text":"   news_id  speech_id  cosine_similarity  news_date speech_date  days_diff  \\\n0        0     150494            0.31543 2018-12-01  2018-12-04         -3   \n1        0     150495            0.31543 2018-12-01  2018-12-04         -3   \n\n  outlet  \n0    bbc  \n1    bbc  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.formula.api as smf\n\n# --- Data Preparation ---\n# Convert to categorical types upfront\nenriched_df = enriched_df.astype({\n    'outlet': 'category',\n    'news_date': 'datetime64[ns]',\n    'speech_date': 'datetime64[ns]'\n})\n\n# Create temporal features without expanding memory\nenriched_df['month'] = enriched_df['news_date'].dt.month.astype('int8')\nenriched_df['year'] = enriched_df['news_date'].dt.year.astype('int16')\n\n# Filter to top outlets using category reordering\ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()\nenriched_df['outlet'] = enriched_df['outlet'].cat.set_categories(top_outlets + ['Other'])\nenriched_df['outlet'] = enriched_df['outlet'].fillna('Other')\n\n# Downcast numerical columns\nenriched_df['cosine_similarity'] = pd.to_numeric(\n    enriched_df['cosine_similarity'], \n    downcast='float'\n)\nenriched_df['days_diff'] = pd.to_numeric(\n    enriched_df['days_diff'], \n    downcast='integer'\n)\n\n# --- Memory Optimization ---\n# Keep only necessary columns\nkeep_cols = ['cosine_similarity', 'days_diff', 'outlet', 'month', 'year']\nenriched_df = enriched_df[keep_cols].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:47:34.284890Z","iopub.execute_input":"2025-04-09T19:47:34.285216Z","iopub.status.idle":"2025-04-09T19:47:40.869302Z","shell.execute_reply.started":"2025-04-09T19:47:34.285156Z","shell.execute_reply":"2025-04-09T19:47:40.868312Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Force garbage collection\nimport gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:47:40.870481Z","iopub.execute_input":"2025-04-09T19:47:40.871063Z","iopub.status.idle":"2025-04-09T19:47:41.037618Z","shell.execute_reply.started":"2025-04-09T19:47:40.871031Z","shell.execute_reply":"2025-04-09T19:47:41.036466Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# --- Model Specification ---\n# Use formula API with categorical variables\nformula = \"\"\"cosine_similarity ~ days_diff +\nC(outlet, Treatment('Other')) + \nC(month) + \nC(year)\n\"\"\"\n\n# Fit model with reduced memory footprint\nmodel = smf.ols(\n    formula, \n    data=enriched_df,\n    missing='drop'  # Automatically drops NA rows\n).fit(cov_type='HC3')\n\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:43:49.896476Z","iopub.execute_input":"2025-04-09T15:43:49.896892Z","iopub.status.idle":"2025-04-09T15:49:37.831646Z","shell.execute_reply.started":"2025-04-09T15:43:49.896856Z","shell.execute_reply":"2025-04-09T15:49:37.830476Z"}},"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.120\nModel:                            OLS   Adj. R-squared:                  0.120\nMethod:                 Least Squares   F-statistic:                 1.316e+05\nDate:                Wed, 09 Apr 2025   Prob (F-statistic):               0.00\nTime:                        15:49:37   Log-Likelihood:             2.8382e+07\nNo. Observations:            24418171   AIC:                        -5.676e+07\nDf Residuals:                24418142   BIC:                        -5.676e+07\nDf Model:                          28                                         \nCovariance Type:                  HC3                                         \n=================================================================================================================\n                                                    coef    std err          z      P>|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------------\nIntercept                                         0.2672      0.000    914.537      0.000       0.267       0.268\nC(outlet, Treatment('Other'))[T.infobae]         -0.0183   4.19e-05   -437.633      0.000      -0.018      -0.018\nC(outlet, Treatment('Other'))[T.proceso]          0.0372   4.72e-05    788.298      0.000       0.037       0.037\nC(outlet, Treatment('Other'))[T.oem]              0.0579   7.17e-05    808.342      0.000       0.058       0.058\nC(outlet, Treatment('Other'))[T.politica]         0.0105   7.24e-05    144.863      0.000       0.010       0.011\nC(outlet, Treatment('Other'))[T.elfinanciero]     0.0010   7.88e-05     12.618      0.000       0.001       0.001\nC(outlet, Treatment('Other'))[T.forbes]           0.0060   8.45e-05     71.409      0.000       0.006       0.006\nC(outlet, Treatment('Other'))[T.elpais]          -0.0241   8.82e-05   -273.720      0.000      -0.024      -0.024\nC(outlet, Treatment('Other'))[T.eleconomista]     0.0412   9.65e-05    427.192      0.000       0.041       0.041\nC(outlet, Treatment('Other'))[T.lasillarota]      0.0616    9.4e-05    654.925      0.000       0.061       0.062\nC(outlet, Treatment('Other'))[T.milenio]          0.0227      0.000    180.992      0.000       0.022       0.023\nC(month)[T.2]                                     0.0133   7.85e-05    168.832      0.000       0.013       0.013\nC(month)[T.3]                                     0.0137    8.1e-05    169.176      0.000       0.014       0.014\nC(month)[T.4]                                    -0.0005   8.43e-05     -5.558      0.000      -0.001      -0.000\nC(month)[T.5]                                     0.0005   8.19e-05      6.354      0.000       0.000       0.001\nC(month)[T.6]                                     0.0068    8.3e-05     82.178      0.000       0.007       0.007\nC(month)[T.7]                                     0.0040   7.54e-05     53.176      0.000       0.004       0.004\nC(month)[T.8]                                    -0.0028   7.67e-05    -36.055      0.000      -0.003      -0.003\nC(month)[T.9]                                     0.0108   7.77e-05    139.602      0.000       0.011       0.011\nC(month)[T.10]                                    0.0096   8.63e-05    111.329      0.000       0.009       0.010\nC(month)[T.11]                                    0.0133   8.61e-05    154.166      0.000       0.013       0.013\nC(month)[T.12]                                    0.0201    8.9e-05    225.887      0.000       0.020       0.020\nC(year)[T.2019]                                   0.0026      0.000      9.038      0.000       0.002       0.003\nC(year)[T.2020]                                   0.0111      0.000     38.196      0.000       0.010       0.012\nC(year)[T.2021]                                   0.0056      0.000     19.427      0.000       0.005       0.006\nC(year)[T.2022]                                   0.0148      0.000     51.733      0.000       0.014       0.015\nC(year)[T.2023]                                   0.0266      0.000     92.855      0.000       0.026       0.027\nC(year)[T.2024]                                   0.0471      0.000    164.004      0.000       0.047       0.048\ndays_diff                                        -0.0010   7.61e-06   -126.981      0.000      -0.001      -0.001\n==============================================================================\nOmnibus:                    83059.295   Durbin-Watson:                   0.013\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            87433.984\nSkew:                          -0.122   Prob(JB):                         0.00\nKurtosis:                       3.164   Cond. No.                         92.8\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import statsmodels.formula.api as smf\n\nformula = \"\"\"cosine_similarity ~ \n             days_diff + \n             C(outlet, Treatment('Other')) + \n             C(month) + \n             C(year)\"\"\"\n\nmodel_fe = smf.ols(formula, data=enriched_df, missing='drop').fit(cov_type='HC3')\nprint(model_fe.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:36:40.369669Z","iopub.execute_input":"2025-04-09T16:36:40.370190Z","iopub.status.idle":"2025-04-09T16:42:27.722374Z","shell.execute_reply.started":"2025-04-09T16:36:40.370150Z","shell.execute_reply":"2025-04-09T16:42:27.721216Z"}},"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.120\nModel:                            OLS   Adj. R-squared:                  0.120\nMethod:                 Least Squares   F-statistic:                 1.316e+05\nDate:                Wed, 09 Apr 2025   Prob (F-statistic):               0.00\nTime:                        16:42:27   Log-Likelihood:             2.8382e+07\nNo. Observations:            24418171   AIC:                        -5.676e+07\nDf Residuals:                24418142   BIC:                        -5.676e+07\nDf Model:                          28                                         \nCovariance Type:                  HC3                                         \n=================================================================================================================\n                                                    coef    std err          z      P>|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------------\nIntercept                                         0.2672      0.000    914.537      0.000       0.267       0.268\nC(outlet, Treatment('Other'))[T.infobae]         -0.0183   4.19e-05   -437.633      0.000      -0.018      -0.018\nC(outlet, Treatment('Other'))[T.proceso]          0.0372   4.72e-05    788.298      0.000       0.037       0.037\nC(outlet, Treatment('Other'))[T.oem]              0.0579   7.17e-05    808.342      0.000       0.058       0.058\nC(outlet, Treatment('Other'))[T.politica]         0.0105   7.24e-05    144.863      0.000       0.010       0.011\nC(outlet, Treatment('Other'))[T.elfinanciero]     0.0010   7.88e-05     12.618      0.000       0.001       0.001\nC(outlet, Treatment('Other'))[T.forbes]           0.0060   8.45e-05     71.409      0.000       0.006       0.006\nC(outlet, Treatment('Other'))[T.elpais]          -0.0241   8.82e-05   -273.720      0.000      -0.024      -0.024\nC(outlet, Treatment('Other'))[T.eleconomista]     0.0412   9.65e-05    427.192      0.000       0.041       0.041\nC(outlet, Treatment('Other'))[T.lasillarota]      0.0616    9.4e-05    654.925      0.000       0.061       0.062\nC(outlet, Treatment('Other'))[T.milenio]          0.0227      0.000    180.992      0.000       0.022       0.023\nC(month)[T.2]                                     0.0133   7.85e-05    168.832      0.000       0.013       0.013\nC(month)[T.3]                                     0.0137    8.1e-05    169.176      0.000       0.014       0.014\nC(month)[T.4]                                    -0.0005   8.43e-05     -5.558      0.000      -0.001      -0.000\nC(month)[T.5]                                     0.0005   8.19e-05      6.354      0.000       0.000       0.001\nC(month)[T.6]                                     0.0068    8.3e-05     82.178      0.000       0.007       0.007\nC(month)[T.7]                                     0.0040   7.54e-05     53.176      0.000       0.004       0.004\nC(month)[T.8]                                    -0.0028   7.67e-05    -36.055      0.000      -0.003      -0.003\nC(month)[T.9]                                     0.0108   7.77e-05    139.602      0.000       0.011       0.011\nC(month)[T.10]                                    0.0096   8.63e-05    111.329      0.000       0.009       0.010\nC(month)[T.11]                                    0.0133   8.61e-05    154.166      0.000       0.013       0.013\nC(month)[T.12]                                    0.0201    8.9e-05    225.887      0.000       0.020       0.020\nC(year)[T.2019]                                   0.0026      0.000      9.038      0.000       0.002       0.003\nC(year)[T.2020]                                   0.0111      0.000     38.196      0.000       0.010       0.012\nC(year)[T.2021]                                   0.0056      0.000     19.427      0.000       0.005       0.006\nC(year)[T.2022]                                   0.0148      0.000     51.733      0.000       0.014       0.015\nC(year)[T.2023]                                   0.0266      0.000     92.855      0.000       0.026       0.027\nC(year)[T.2024]                                   0.0471      0.000    164.004      0.000       0.047       0.048\ndays_diff                                        -0.0010   7.61e-06   -126.981      0.000      -0.001      -0.001\n==============================================================================\nOmnibus:                    83059.295   Durbin-Watson:                   0.013\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            87433.984\nSkew:                          -0.122   Prob(JB):                         0.00\nKurtosis:                       3.164   Cond. No.                         92.8\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import statsmodels.api as sm\n\nmd = sm.MixedLM.from_formula(\n    \"cosine_similarity ~ days_diff + C(outlet, Treatment('Other')) + C(month) + C(year)\",\n    groups=\"outlet\",\n    data=enriched_df\n)\nmodel_re = md.fit(reml=False)\nprint(model_re.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:49:28.722005Z","iopub.execute_input":"2025-04-09T19:49:28.722499Z","iopub.status.idle":"2025-04-09T19:55:24.025094Z","shell.execute_reply.started":"2025-04-09T19:49:28.722465Z","shell.execute_reply":"2025-04-09T19:55:24.023882Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n  warnings.warn(msg, ConvergenceWarning)\n/usr/local/lib/python3.10/dist-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n  warnings.warn(msg, ConvergenceWarning)\n","output_type":"stream"},{"name":"stdout","text":"                          Mixed Linear Model Regression Results\n==========================================================================================\nModel:                     MixedLM           Dependent Variable:         cosine_similarity\nNo. Observations:          24418171          Method:                     ML               \nNo. Groups:                11                Scale:                      0.0057           \nMin. group size:           435409            Log-Likelihood:             28382075.1418    \nMax. group size:           8279797           Converged:                  Yes              \nMean group size:           2219833.7                                                      \n------------------------------------------------------------------------------------------\n                                              Coef.  Std.Err.    z     P>|z| [0.025 0.975]\n------------------------------------------------------------------------------------------\nIntercept                                      0.267    0.076    3.531 0.000  0.119  0.416\nC(outlet, Treatment('Other'))[T.infobae]      -0.019    0.107   -0.173 0.863 -0.228  0.191\nC(outlet, Treatment('Other'))[T.proceso]       0.037    0.107    0.347 0.729 -0.173  0.247\nC(outlet, Treatment('Other'))[T.oem]           0.058    0.107    0.540 0.589 -0.152  0.268\nC(outlet, Treatment('Other'))[T.politica]      0.010    0.107    0.097 0.923 -0.199  0.220\nC(outlet, Treatment('Other'))[T.elfinanciero]  0.001    0.107    0.008 0.993 -0.209  0.211\nC(outlet, Treatment('Other'))[T.forbes]        0.006    0.107    0.055 0.956 -0.204  0.216\nC(outlet, Treatment('Other'))[T.elpais]       -0.024    0.107   -0.227 0.821 -0.234  0.186\nC(outlet, Treatment('Other'))[T.eleconomista]  0.041    0.107    0.384 0.701 -0.169  0.251\nC(outlet, Treatment('Other'))[T.lasillarota]   0.061    0.107    0.574 0.566 -0.148  0.271\nC(outlet, Treatment('Other'))[T.milenio]       0.023    0.107    0.211 0.833 -0.187  0.232\nC(month)[T.2]                                  0.013    0.000  173.813 0.000  0.013  0.013\nC(month)[T.3]                                  0.014    0.000  174.576 0.000  0.014  0.014\nC(month)[T.4]                                 -0.000    0.000   -5.819 0.000 -0.001 -0.000\nC(month)[T.5]                                  0.001    0.000    6.607 0.000  0.000  0.001\nC(month)[T.6]                                  0.007    0.000   83.794 0.000  0.007  0.007\nC(month)[T.7]                                  0.004    0.000   53.827 0.000  0.004  0.004\nC(month)[T.8]                                 -0.003    0.000  -37.526 0.000 -0.003 -0.003\nC(month)[T.9]                                  0.011    0.000  148.816 0.000  0.011  0.011\nC(month)[T.10]                                 0.010    0.000  113.734 0.000  0.009  0.010\nC(month)[T.11]                                 0.013    0.000  155.556 0.000  0.013  0.013\nC(month)[T.12]                                 0.020    0.000  234.862 0.000  0.020  0.020\nC(year)[T.2019]                                0.003    0.000    9.597 0.000  0.002  0.003\nC(year)[T.2020]                                0.011    0.000   40.367 0.000  0.011  0.012\nC(year)[T.2021]                                0.006    0.000   20.552 0.000  0.005  0.006\nC(year)[T.2022]                                0.015    0.000   54.744 0.000  0.014  0.015\nC(year)[T.2023]                                0.027    0.000   98.357 0.000  0.026  0.027\nC(year)[T.2024]                                0.047    0.000  173.588 0.000  0.047  0.048\ndays_diff                                     -0.001    0.000 -125.837 0.000 -0.001 -0.001\noutlet Var                                     0.006                                      \n==========================================================================================\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  not efficeint code","metadata":{}},{"cell_type":"code","source":"# Limit the number of top outlets (you could change the number, here we are using top 10)\ntop_outlets = enriched_df['outlet'].value_counts().nlargest(10).index.tolist()\n\n# Create a new column that groups outlets into 'Other' if they're not in the top outlets\nenriched_df['outlet_top'] = enriched_df['outlet'].where(enriched_df['outlet'].isin(top_outlets), 'Other')\n\n# Use `Categorical` dtype to reduce memory usage\nenriched_df['outlet_top'] = enriched_df['outlet_top'].astype('category')\n\n# Create dummies for the top outlets (drop the first to avoid multicollinearity)\noutlet_dummies = pd.get_dummies(enriched_df['outlet_top'], drop_first=True)\n\n# Add the dummies to the dataframe\nenriched_df = pd.concat([enriched_df, outlet_dummies], axis=1)\n\n# Verify memory usage\nprint(enriched_df.memory_usage(deep=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:42:57.199819Z","iopub.execute_input":"2025-04-09T14:42:57.200349Z","iopub.status.idle":"2025-04-09T14:43:13.108917Z","shell.execute_reply.started":"2025-04-09T14:42:57.200311Z","shell.execute_reply":"2025-04-09T14:43:13.103659Z"}},"outputs":[{"name":"stdout","text":"Index                       128\nnews_id               195345368\nspeech_id             195345368\ncosine_similarity      48836342\nnews_date             195345368\nspeech_date           195345368\ndays_diff             195345368\noutlet               1584360090\noutlet_top             24419182\neleconomista           24418171\nelfinanciero           24418171\nelpais                 24418171\nforbes                 24418171\ninfobae                24418171\nlasillarota            24418171\nmilenio                24418171\noem                    24418171\npolitica               24418171\nproceso                24418171\ndtype: int64\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Extract Month and Year from `news_date`\nenriched_df['month'] = enriched_df['news_date'].dt.month.astype('category')\nenriched_df['year'] = enriched_df['news_date'].dt.year.astype('category')\n\n# Create dummies for month and year, also using `drop_first=True` to avoid multicollinearity\nmonth_dummies = pd.get_dummies(enriched_df['month'], prefix='month', drop_first=True)\nyear_dummies = pd.get_dummies(enriched_df['year'], prefix='year', drop_first=True)\n\n# Concatenate the dummies into the dataframe\nenriched_df = pd.concat([enriched_df, month_dummies, year_dummies], axis=1)\n\n# Verify memory usage again\nprint(enriched_df.memory_usage(deep=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:03:25.322343Z","iopub.execute_input":"2025-04-09T15:03:25.322940Z","iopub.status.idle":"2025-04-09T15:03:34.656982Z","shell.execute_reply.started":"2025-04-09T15:03:25.322896Z","shell.execute_reply":"2025-04-09T15:03:34.655727Z"}},"outputs":[{"name":"stdout","text":"Index                       128\nnews_id               195345368\nspeech_id             195345368\ncosine_similarity      48836342\nnews_date             195345368\nspeech_date           195345368\ndays_diff             195345368\noutlet               1584360090\noutlet_top             24419182\neleconomista           24418171\nelfinanciero           24418171\nelpais                 24418171\nforbes                 24418171\ninfobae                24418171\nlasillarota            24418171\nmilenio                24418171\noem                    24418171\npolitica               24418171\nproceso                24418171\nmonth                  24418455\nyear                   24418435\nmonth_2                24418171\nmonth_3                24418171\nmonth_4                24418171\nmonth_5                24418171\nmonth_6                24418171\nmonth_7                24418171\nmonth_8                24418171\nmonth_9                24418171\nmonth_10               24418171\nmonth_11               24418171\nmonth_12               24418171\nyear_2019              24418171\nyear_2020              24418171\nyear_2021              24418171\nyear_2022              24418171\nyear_2023              24418171\nyear_2024              24418171\ndtype: int64\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Sort by date to ensure proper time-series structure\nenriched_df = enriched_df.sort_values(by='news_date')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:04:40.393177Z","iopub.execute_input":"2025-04-09T15:04:40.393740Z","iopub.status.idle":"2025-04-09T15:04:45.191933Z","shell.execute_reply.started":"2025-04-09T15:04:40.393701Z","shell.execute_reply":"2025-04-09T15:04:45.190836Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import statsmodels.api as sm\n# Use `Categorical` for efficient memory use on the independent variables (outlet dummies)\nX = enriched_df.drop(['cosine_similarity', 'speech_id'], axis=1)\nX = sm.add_constant(X)  # Add an intercept\ny = enriched_df['cosine_similarity']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:07:42.320691Z","iopub.execute_input":"2025-04-09T15:07:42.321065Z","iopub.status.idle":"2025-04-09T15:07:52.459431Z","shell.execute_reply.started":"2025-04-09T15:07:42.321039Z","shell.execute_reply":"2025-04-09T15:07:52.458173Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(X.dtypes)\nprint(y.dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:11:43.066756Z","iopub.execute_input":"2025-04-09T15:11:43.067265Z","iopub.status.idle":"2025-04-09T15:11:43.077441Z","shell.execute_reply.started":"2025-04-09T15:11:43.067195Z","shell.execute_reply":"2025-04-09T15:11:43.075979Z"}},"outputs":[{"name":"stdout","text":"const                  float64\nnews_id                  int64\nnews_date       datetime64[ns]\nspeech_date     datetime64[ns]\ndays_diff                int64\noutlet                  object\noutlet_top            category\neleconomista              bool\nelfinanciero              bool\nelpais                    bool\nforbes                    bool\ninfobae                   bool\nlasillarota               bool\nmilenio                   bool\noem                       bool\npolitica                  bool\nproceso                   bool\nmonth                 category\nyear                  category\nmonth_2                   bool\nmonth_3                   bool\nmonth_4                   bool\nmonth_5                   bool\nmonth_6                   bool\nmonth_7                   bool\nmonth_8                   bool\nmonth_9                   bool\nmonth_10                  bool\nmonth_11                  bool\nmonth_12                  bool\nyear_2019                 bool\nyear_2020                 bool\nyear_2021                 bool\nyear_2022                 bool\nyear_2023                 bool\nyear_2024                 bool\ndtype: object\nfloat16\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Convert categorical columns to dummies (one-hot encoding)\nX = pd.get_dummies(X, columns=['outlet_top', 'month', 'year'], drop_first=True)\n# Ensure boolean columns are integers (True -> 1, False -> 0)\nbool_columns = [\n    'eleconomista', 'elfinanciero', 'elpais', 'forbes', 'infobae', \n    'lasillarota', 'milenio', 'oem', 'politica', 'proceso', \n    'month_2', 'month_3', 'month_4', 'month_5', 'month_6', \n    'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', \n    'year_2019', 'year_2020', 'year_2021', 'year_2022', 'year_2023', 'year_2024'\n]\n\nX[bool_columns] = X[bool_columns].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:12:55.075265Z","iopub.execute_input":"2025-04-09T15:12:55.075835Z","execution_failed":"2025-04-09T15:13:13.118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = pd.to_numeric(y, errors='coerce')  # Convert to numeric, coerces errors to NaN\ny = y.dropna()  # Drop rows with missing target values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import statsmodels.api as sm\n\n# Fit the OLS model\nmodel = sm.OLS(y, X).fit()\n\n# Summary of the regression results\nprint(model.summary())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T15:08:12.301050Z","iopub.execute_input":"2025-04-09T15:08:12.301704Z","iopub.status.idle":"2025-04-09T15:10:12.815315Z","shell.execute_reply.started":"2025-04-09T15:08:12.301663Z","shell.execute_reply":"2025-04-09T15:10:12.813432Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-65dbbf563abe>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the OLS model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Summary of the regression results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m                    \"An exception will be raised in the next version.\")\n\u001b[1;32m    920\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         super().__init__(endog, exog, missing=missing,\n\u001b[0m\u001b[1;32m    922\u001b[0m                                   hasconst=hasconst, **kwargs)\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         super().__init__(endog, exog, missing=missing,\n\u001b[0m\u001b[1;32m    747\u001b[0m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[1;32m    748\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv_wexog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloat64Array\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0m\u001b[1;32m     96\u001b[0m                                       **kwargs)\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0m\u001b[1;32m    676\u001b[0m                  **kwargs)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             raise ValueError(\"Pandas data cast to numpy dtype of object. \"\n\u001b[0m\u001b[1;32m    510\u001b[0m                              \"Check input data with np.asarray(data).\")\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."],"ename":"ValueError","evalue":"Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Regression with topic modelling and sentiment analysis ","metadata":{}},{"cell_type":"code","source":"!pip install bertopic\n!pip install umap-learn hdbscan\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:41:40.721779Z","iopub.execute_input":"2025-04-04T09:41:40.722121Z","iopub.status.idle":"2025-04-04T09:41:49.242866Z","shell.execute_reply.started":"2025-04-04T09:41:40.722095Z","shell.execute_reply":"2025-04-04T09:41:49.241764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom bertopic import BERTopic\nimport umap\nimport hdbscan\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:41:56.891525Z","iopub.execute_input":"2025-04-04T09:41:56.891866Z","iopub.status.idle":"2025-04-04T09:42:36.294387Z","shell.execute_reply.started":"2025-04-04T09:41:56.891836Z","shell.execute_reply":"2025-04-04T09:42:36.293683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check files in Kaggle input directory\nprint(os.listdir('/kaggle/input')) \n\n# Define paths\nembeddings_path = \"/kaggle/input/embeddings\"\nprint(os.listdir(embeddings_path))\n\n# Load speech and news embeddings\nspeeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\nnews_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n\n# Convert date columns\nnews_embeddings['news_date'] = pd.to_datetime(news_embeddings['Date'])\nspeeches_embeddings['speech_date'] = pd.to_datetime(speeches_embeddings['date'])\n\n# Show first rows\nprint(\"News Data:\", news_embeddings.head())\nprint(\"\\nSpeeches Data:\", speeches_embeddings.head())\n\n# Define device for efficient computations\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n#Temporal window calculation and expansion\ndef generate_temporal_pairs(news_df, speeches_df, window_days=3):\n    \"\"\"Generate news-speech pairs within a symmetric temporal window (-4 to +4 days)\"\"\"\n    pairs = []\n    chunk_size = 2000\n    news_chunks = np.array_split(news_df, len(news_df) // chunk_size + 1)\n    \n    for chunk in news_chunks:\n        for _, row in chunk.iterrows():\n            news_date = row['news_date']\n            start_date = news_date - pd.Timedelta(days=window_days) \n            end_date = news_date + pd.Timedelta(days=window_days)  \n            \n            mask = (speeches_df['speech_date'] >= start_date) & (speeches_df['speech_date'] <= end_date)\n            speech_ids = speeches_df[mask].index.tolist()\n            pairs.extend([(row.name, s_id) for s_id in speech_ids])\n    \n    return pd.DataFrame(pairs, columns=['news_id', 'speech_id'])\n\n\nalignment_df = generate_temporal_pairs(news_embeddings, speeches_embeddings)\n\n#optimized embeddings calculation\ndef load_embeddings_half(df, col_name):\n    embeddings = []\n    for i, row in df.iterrows():\n        if isinstance(row[col_name], str):\n            arr = np.fromstring(row[col_name].strip(\"[]\"), sep=\" \", dtype=np.float16)\n        else:\n            arr = np.array(row[col_name], dtype=np.float16)\n        embeddings.append(torch.tensor(arr, device=device).half())\n        if i % 1000 == 0: torch.cuda.empty_cache()\n    return torch.stack(embeddings)\n\nnews_tensor = load_embeddings_half(news_embeddings, 'news_embeddings')\nspeeches_tensor = load_embeddings_half(speeches_embeddings, 'speech_embeddings')\n\n#Batched cosine similarity computation\ndef compute_cosine_similarities(pairs_df, news_emb, speech_emb, batch_size=8192):\n    news_norm = F.normalize(news_emb, p=2, dim=1)\n    speech_norm = F.normalize(speech_emb, p=2, dim=1)\n    similarities = []\n    for i in range(0, len(pairs_df), batch_size):\n        batch = pairs_df.iloc[i:i+batch_size]\n        news_batch = news_norm[batch['news_id'].values]\n        speech_batch = speech_norm[batch['speech_id'].values]\n        similarities.append(F.cosine_similarity(news_batch, speech_batch).cpu().numpy())\n        del news_batch, speech_batch\n        torch.cuda.empty_cache()\n    return np.concatenate(similarities)\n\nalignment_df['cosine_similarity'] = compute_cosine_similarities(alignment_df, news_tensor, speeches_tensor)\n\n#include metadata to the embeddings to track temporal dependencies\ndef add_temporal_features(pairs_df, news_df, speeches_df):\n    pairs_df = pairs_df.merge(\n        news_df[['news_date']],\n        left_on='news_id',\n        right_index=True\n    ).merge(\n        speeches_df[['speech_date']],\n        left_on='speech_id',\n        right_index=True\n    )\n    pairs_df['days_diff'] = (pairs_df['news_date'] - pairs_df['speech_date']).dt.days\n    return pairs_df\n\nenriched_df = add_temporal_features(alignment_df, news_embeddings, speeches_embeddings)\n\n#Save data to avoid rerunning everything again \nenriched_df.to_parquet('news_speech_similarities.parquet', engine='pyarrow', compression='zstd')\nprint(\"Processing complete. Results saved with columns:\", enriched_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:42:36.295442Z","iopub.execute_input":"2025-04-04T09:42:36.295678Z","iopub.status.idle":"2025-04-04T09:50:10.604376Z","shell.execute_reply.started":"2025-04-04T09:42:36.295656Z","shell.execute_reply":"2025-04-04T09:50:10.603508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Speeches columns:\", speeches_embeddings.columns)\nprint(\"News columns:\", news_embeddings.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:50:10.606182Z","iopub.execute_input":"2025-04-04T09:50:10.606434Z","iopub.status.idle":"2025-04-04T09:50:10.611732Z","shell.execute_reply.started":"2025-04-04T09:50:10.606414Z","shell.execute_reply":"2025-04-04T09:50:10.610921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 🔹 Count topics in news & speeches\nnews_topic_counts = news_embeddings['topic'].value_counts().sort_index()\nspeech_topic_counts = speeches_embeddings['topic'].value_counts().sort_index()\n\n# 🔹 Plot topic distribution\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.barplot(x=news_topic_counts.index, y=news_topic_counts.values, ax=ax[0], palette=\"Blues\")\nax[0].set_title(\"News Topic Distribution\")\nax[0].set_xlabel(\"Topic\")\nax[0].set_ylabel(\"Count\")\n\nsns.barplot(x=speech_topic_counts.index, y=speech_topic_counts.values, ax=ax[1], palette=\"Oranges\")\nax[1].set_title(\"Speeches Topic Distribution\")\nax[1].set_xlabel(\"Topic\")\nax[1].set_ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n# 🔹 Time trends of topics\nnews_embeddings['month'] = news_embeddings['news_date'].dt.to_period('M')\nspeeches_embeddings['month'] = speeches_embeddings['speech_date'].dt.to_period('M')\n\nnews_topic_trends = news_embeddings.groupby(['month', 'topic']).size().unstack(fill_value=0)\nspeech_topic_trends = speeches_embeddings.groupby(['month', 'topic']).size().unstack(fill_value=0)\n\n# 🔹 Plot topic trends\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\nnews_topic_trends.plot(ax=ax[0], cmap=\"Blues\")\nax[0].set_title(\"News Topic Trends Over Time\")\nax[0].set_ylabel(\"Count\")\n\nspeech_topic_trends.plot(ax=ax[1], cmap=\"Oranges\")\nax[1].set_title(\"Speeches Topic Trends Over Time\")\nax[1].set_ylabel(\"Count\")\n\nplt.xlabel(\"Time (Month)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T12:27:45.387435Z","iopub.execute_input":"2025-04-03T12:27:45.387810Z","iopub.status.idle":"2025-04-03T12:27:46.551109Z","shell.execute_reply.started":"2025-04-03T12:27:45.387775Z","shell.execute_reply":"2025-04-03T12:27:46.550187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"topics = topic_model.get_topics()\nprint(f\"Number of topics generated: {len(topics)}\")\nprint(f\"Topic IDs: {list(topics.keys())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T12:28:01.600215Z","iopub.execute_input":"2025-04-03T12:28:01.600550Z","iopub.status.idle":"2025-04-03T12:28:01.605269Z","shell.execute_reply.started":"2025-04-03T12:28:01.600520Z","shell.execute_reply":"2025-04-03T12:28:01.604330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove the outlier topic (-1) before extracting words\nvalid_topics = [t for t in topic_model.get_topics().keys() if t != -1]\n\n# Extract top 10 words per topic\ntopic_words = {\n    topic_id: [word for word, _ in topic_model.get_topic(topic_id)[:10]]\n    for topic_id in valid_topics\n}\n\n# Print extracted words per topic\nfor topic_id, words in topic_words.items():\n    print(f\"Topic {topic_id}: {', '.join(words)}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract top 10 words per topic\ntopic_words = {}\n\nfor topic_id in range(len(topic_model.get_topics())):\n    words = [word for word, _ in topic_model.get_topic(topic_id)[:10]]  # Top 10 words\n    topic_words[topic_id] = words\n\n# Convert to DataFrame for better readability\ntopic_words_df = pd.DataFrame.from_dict(topic_words, orient='index', columns=[f'Word {i+1}' for i in range(10)])\n\n# Display the first few topics\nprint(topic_words_df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:22:39.656933Z","iopub.execute_input":"2025-04-03T10:22:39.657280Z","iopub.status.idle":"2025-04-03T10:22:39.687143Z","shell.execute_reply.started":"2025-04-03T10:22:39.657258Z","shell.execute_reply":"2025-04-03T10:22:39.686022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 🔹 Extract top 20 words per topic\nnews_top_words = news_topic_model.get_topics()\nspeech_top_words = speech_topic_model.get_topics()\n\n# 🔹 Function to format topic words\ndef extract_top_words(topic_words, top_n=20):\n    top_words_dict = {}\n    for topic, words in topic_words.items():\n        if topic != -1:  # Ignore outliers\n            top_words_dict[topic] = [word for word, _ in words[:top_n]]\n    return top_words_dict\n\n# 🔹 Get top words for each topic\nnews_words_dict = extract_top_words(news_top_words)\nspeech_words_dict = extract_top_words(speech_top_words)\n\n# 🔹 Convert to DataFrame for easier visualization\nnews_words_df = pd.DataFrame.from_dict(news_words_dict, orient='index')\nspeech_words_df = pd.DataFrame.from_dict(speech_words_dict, orient='index')\n\n# 🔹 Display top words per topic\nprint(\"🔹 News Topics - Top 20 Words:\")\nprint(news_words_df.head(10))  # Print first 10 topics\n\nprint(\"\\n🔹 Speeches Topics - Top 20 Words:\")\nprint(speech_words_df.head(10))  # Print first 10 topics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:21:05.981200Z","iopub.execute_input":"2025-04-03T10:21:05.981545Z","iopub.status.idle":"2025-04-03T10:21:06.238705Z","shell.execute_reply.started":"2025-04-03T10:21:05.981523Z","shell.execute_reply":"2025-04-03T10:21:06.237614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### bin for already incorrect code ","metadata":{}},{"cell_type":"code","source":"# Ensure numeric type and handle NaNs safely\nspeeches_embeddings['avg_similarity_x'] = pd.to_numeric(speeches_embeddings['avg_similarity_x'], errors='coerce')\nspeeches_embeddings['avg_similarity_y'] = pd.to_numeric(speeches_embeddings['avg_similarity_y'], errors='coerce')\n\n# Fill NaNs properly\nspeeches_embeddings['avg_similarity'] = speeches_embeddings['avg_similarity_x'].fillna(speeches_embeddings['avg_similarity_y'])\n\n# Drop the redundant columns\nspeeches_embeddings = speeches_embeddings.drop(columns=['avg_similarity_x', 'avg_similarity_y'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:07:10.353828Z","iopub.status.idle":"2025-04-01T17:07:10.354240Z","shell.execute_reply":"2025-04-01T17:07:10.354065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare regression data\nX = speeches_embeddings[['topic', 'avg_similarity']]  # Features\ny = speeches_embeddings['cosine_similarity']  # Target\n\n# Convert categorical topic to numerical encoding\nX = pd.get_dummies(X, columns=['topic'], drop_first=True)\n\n# Train regression model\nreg_model = LinearRegression()\nreg_model.fit(X, y)\n\n# Print model coefficients\nprint(\"Regression Coefficients:\", dict(zip(X.columns, reg_model.coef_)))\n\n# Evaluate model\ny_pred = reg_model.predict(X)\nprint(\"R² Score:\", reg_model.score(X, y))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:07:10.355210Z","iopub.status.idle":"2025-04-01T17:07:10.355569Z","shell.execute_reply":"2025-04-01T17:07:10.355426Z"}},"outputs":[],"execution_count":null}]}