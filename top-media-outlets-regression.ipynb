{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10783414,"sourceType":"datasetVersion","datasetId":6691461},{"sourceId":10788152,"sourceType":"datasetVersion","datasetId":6602831}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nos.listdir('/kaggle/input') \n\nembeddings_path = \"/kaggle/input/embeddings\"\nprint(os.listdir(embeddings_path))\nimport pandas as pd\n\nspeeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\nnews_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n\n# Display first few rows\nprint(\"News Data:\")\nprint(news_embeddings.head())\n\nprint(\"\\nSpeeches Data:\")\nprint(speeches_embeddings.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:18:37.330517Z","iopub.execute_input":"2025-03-24T12:18:37.330742Z","iopub.status.idle":"2025-03-24T12:24:15.858713Z","shell.execute_reply.started":"2025-03-24T12:18:37.330722Z","shell.execute_reply":"2025-03-24T12:24:15.857938Z"}},"outputs":[{"name":"stdout","text":"['speeches_with_embeddings.csv', 'speeches_embeddings_sentiment.csv', 'news_embeddings_sentiment.csv', 'news_with_embeddings.csv']\nNews Data:\n          Index                                               Link  \\\n0  1_01_12_2018  https://www.bbc.com/mundo/noticias-america-lat...   \n1  2_01_12_2018  https://politica.expansion.mx/presidencia/2018...   \n2  3_01_12_2018  https://oem.com.mx/elsoldemexico/mexico/en-don...   \n3  4_01_12_2018  https://politica.expansion.mx/presidencia/2018...   \n4  5_01_12_2018  https://www.eleconomista.com.mx/politica/Nicol...   \n\n                                              Domain  \\\n0  BBC\\nToma de protesta de AMLO: las 5 tradicion...   \n1  Expansión Política\\nAMLO rinde protesta y prom...   \n2  El Sol de México\\n¿Hay Ley Seca este 1 de dici...   \n3  Expansión Política\\nAMLO es un \"líder persiste...   \n4  El Economista\\nNicolás Maduro llega a Palacio ...   \n\n                                               Title        Date  \\\n0  Toma de protesta de AMLO: las 5 tradiciones qu...  2018-12-01   \n1        AMLO rinde protesta y promete no reelegirse  2018-12-01   \n2  ¿Hay Ley Seca este 1 de diciembre por cambio d...  2018-12-01   \n3  AMLO es un \"líder persistente\", dice la superc...  2018-12-01   \n4  Nicolás Maduro llega a Palacio Nacional; no as...  2018-12-01   \n\n                                             Content month_abbr  \\\n0  Fuente de la imagen, Getty Images Desde su cam...        dic   \n1  Síguenos en nuestras redes sociales: CIUDAD DE...        dic   \n2  Por la toma de posesión de Andrés Manuel López...        dic   \n3  Síguenos en nuestras redes sociales: CIUDAD DE...        dic   \n4  Lectura 3:00 min Nicolás Maduro arribó este sá...        dic   \n\n                                   processed_content  \\\n0  fuente imagen getty images campaña presidencia...   \n1  síguenos redes sociales ciudad méxico adnpolít...   \n2  toma posesión andrés manuel lópez obrador pres...   \n3  síguenos redes sociales ciudad méxico adnpolít...   \n4  lectura 3:00 min nicolás maduro   arribó sábad...   \n\n                                         news_chunks  \\\n0  fuente imagen getty images campaña presidencia...   \n1  síguenos redes sociales ciudad méxico adnpolít...   \n2  toma posesión andrés manuel lópez obrador pres...   \n3  síguenos redes sociales ciudad méxico adnpolít...   \n4  lectura 3:00 min nicolás maduro   arribó sábad...   \n\n                                     news_embeddings  \n0  [ 3.77706587e-01  9.13102329e-02 -1.38176888e-...  \n1  [ 2.61866331e-01  2.99132258e-01  1.76378831e-...  \n2  [ 5.06281674e-01  3.32773924e-02 -3.04715186e-...  \n3  [ 3.07641208e-01  7.52940923e-02 -5.62011823e-...  \n4  [ 3.04801702e-01  3.39728445e-01  3.13091815e-...  \n\nSpeeches Data:\n   Unnamed: 0  X  speech_id  \\\n0           1  1          1   \n1           1  1          1   \n2           1  1          1   \n3           1  1          1   \n4           1  1          1   \n\n                                               title  \\\n0  Versión estenográfica de la conferencia de pre...   \n1  Versión estenográfica de la conferencia de pre...   \n2  Versión estenográfica de la conferencia de pre...   \n3  Versión estenográfica de la conferencia de pre...   \n4  Versión estenográfica de la conferencia de pre...   \n\n                                                urls  \\\n0  https://lopezobrador.org.mx/2024/01/09/version...   \n1  https://lopezobrador.org.mx/2024/01/09/version...   \n2  https://lopezobrador.org.mx/2024/01/09/version...   \n3  https://lopezobrador.org.mx/2024/01/09/version...   \n4  https://lopezobrador.org.mx/2024/01/09/version...   \n\n                                             content        date  \\\n0  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n1  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n2  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n3  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n4  2024: Año de Felipe Carrillo Puerto, benemérit...  2024-01-09   \n\n                                       speech_chunks  \\\n0  2024: Año de Felipe Carrillo Puerto, benemérit...   \n1  Son los tres informes. Pero antes quiero dar a...   \n2  Entonces, ya voy a estar en TikTok y quiero in...   \n3  Entonces, ofrecer una disculpa y enviarle un a...   \n4  Me sumo al deseo de este año que sea lo mejor ...   \n\n                                   speech_embeddings  \n0  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n1  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n2  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n3  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n4  [-2.88460612e-01 -3.40319216e-01 -1.11393042e-...  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# Convert and verify date columns\nnews_embeddings['news_date'] = pd.to_datetime(news_embeddings['Date'])\nspeeches_embeddings['speech_date'] = pd.to_datetime(speeches_embeddings['date'])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n#Temporal window calculation and expansion\ndef generate_temporal_pairs(news_df, speeches_df, window_days=4):\n    \"\"\"Generate news-speech pairs within a symmetric temporal window (-4 to +4 days)\"\"\"\n    pairs = []\n    chunk_size = 2000\n    news_chunks = np.array_split(news_df, len(news_df) // chunk_size + 1)\n    \n    for chunk in news_chunks:\n        for _, row in chunk.iterrows():\n            news_date = row['news_date']\n            start_date = news_date - pd.Timedelta(days=window_days) \n            end_date = news_date + pd.Timedelta(days=window_days)  \n            \n            mask = (speeches_df['speech_date'] >= start_date) & (speeches_df['speech_date'] <= end_date)\n            speech_ids = speeches_df[mask].index.tolist()\n            pairs.extend([(row.name, s_id) for s_id in speech_ids])\n    \n    return pd.DataFrame(pairs, columns=['news_id', 'speech_id'])\n\n\nalignment_df = generate_temporal_pairs(news_embeddings, speeches_embeddings)\n\n#optimized embeddings calculation\ndef load_embeddings_half(df, col_name):\n    embeddings = []\n    for i, row in df.iterrows():\n        if isinstance(row[col_name], str):\n            arr = np.fromstring(row[col_name].strip(\"[]\"), sep=\" \", dtype=np.float16)\n        else:\n            arr = np.array(row[col_name], dtype=np.float16)\n        embeddings.append(torch.tensor(arr, device=device).half())\n        if i % 1000 == 0: torch.cuda.empty_cache()\n    return torch.stack(embeddings)\n\nnews_tensor = load_embeddings_half(news_embeddings, 'news_embeddings')\nspeeches_tensor = load_embeddings_half(speeches_embeddings, 'speech_embeddings')\n\n#Batched cosine similarity computation\ndef compute_cosine_similarities(pairs_df, news_emb, speech_emb, batch_size=8192):\n    news_norm = F.normalize(news_emb, p=2, dim=1)\n    speech_norm = F.normalize(speech_emb, p=2, dim=1)\n    similarities = []\n    for i in range(0, len(pairs_df), batch_size):\n        batch = pairs_df.iloc[i:i+batch_size]\n        news_batch = news_norm[batch['news_id'].values]\n        speech_batch = speech_norm[batch['speech_id'].values]\n        similarities.append(F.cosine_similarity(news_batch, speech_batch).cpu().numpy())\n        del news_batch, speech_batch\n        torch.cuda.empty_cache()\n    return np.concatenate(similarities)\n\nalignment_df['cosine_similarity'] = compute_cosine_similarities(alignment_df, news_tensor, speeches_tensor)\n\n#include metadata to the embeddings to track temporal dependencies\ndef add_temporal_features(pairs_df, news_df, speeches_df):\n    pairs_df = pairs_df.merge(\n        news_df[['news_date']],\n        left_on='news_id',\n        right_index=True\n    ).merge(\n        speeches_df[['speech_date']],\n        left_on='speech_id',\n        right_index=True\n    )\n    pairs_df['days_diff'] = (pairs_df['news_date'] - pairs_df['speech_date']).dt.days\n    return pairs_df\n\nenriched_df = add_temporal_features(alignment_df, news_embeddings, speeches_embeddings)\n\n#Save data to avoid rerunning everything again \nenriched_df.to_parquet('news_speech_similarities.parquet', engine='pyarrow', compression='zstd')\nprint(\"Processing complete. Results saved with columns:\", enriched_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:24:15.859474Z","iopub.execute_input":"2025-03-24T12:24:15.859689Z","iopub.status.idle":"2025-03-24T12:27:19.821770Z","shell.execute_reply.started":"2025-03-24T12:24:15.859672Z","shell.execute_reply":"2025-03-24T12:27:19.820614Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"Processing complete. Results saved with columns: ['news_id', 'speech_id', 'cosine_similarity', 'news_date', 'speech_date', 'days_diff']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom urllib.parse import urlparse\n\n# Load data\nembeddings_path = \"/kaggle/input/embeddings\"\n#speeches_embeddings = pd.read_csv(f\"{embeddings_path}/speeches_with_embeddings.csv\")\n#news_embeddings = pd.read_csv(f\"{embeddings_path}/news_with_embeddings.csv\")\n\n# Preprocessing function for media outlets\ndef extract_outlet(url):\n    \"\"\"Extract media outlet name from URL using domain parsing\"\"\"\n    try:\n        parsed = urlparse(url)\n        domain = parsed.netloc\n        if not domain:\n            return \"unknown\"\n        \n        # Clean domain and split into parts\n        domain = domain.lower().replace(\"www.\", \"\")\n        parts = domain.split(\".\")\n        \n        if len(parts) >= 3 and parts[-2] in ['co', 'com', 'org', 'net', 'edu', 'gov']:\n            return parts[-3]  # Handle domains like .co.uk, .com.br\n        elif len(parts) >= 2:\n            return parts[-2]\n        return domain\n    except:\n        return \"unknown\"\n\n# Check and add outlet information\nOUTLET_PATH = \"news_outlets.parquet\"\n\nif not os.path.exists(OUTLET_PATH):\n    print(\"Extracting media outlets...\")\n    news_embeddings['outlet'] = news_embeddings['Link'].apply(extract_outlet)\n    news_embeddings[['outlet']].reset_index().rename(columns={'index': 'news_id'})\\\n        .to_parquet(OUTLET_PATH, engine='pyarrow', compression='zstd')\n    print(f\"Saved outlets to {OUTLET_PATH}\")\n\n# Main analysis workflow (keep existing temporal alignment code)\n# [Keep your existing code for temporal alignment and cosine similarity here]\n\n# Visualization with precomputed outlets\ndef plot_temporal_alignment_heatmap(df, outlet_path=OUTLET_PATH, resample_freq='M'):\n    \"\"\"Use precomputed outlet data for visualization\"\"\"\n    news_metadata = pd.read_parquet(outlet_path)\n    \n    merged_df = df.merge(\n        news_metadata,\n        on='news_id',\n        how='inner'\n    ).dropna(subset=['cosine_similarity'])\n    \n    # Rest of the plotting logic remains the same\n    # [Keep your existing visualization code here]\n\nif __name__ == \"__main__\":\n    try:\n        similarities_df = pd.read_parquet('news_speech_similarities.parquet')\n        plot_temporal_alignment_heatmap(similarities_df)\n    except Exception as e:\n        print(f\"Error: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:27:19.822604Z","iopub.execute_input":"2025-03-24T12:27:19.823067Z","iopub.status.idle":"2025-03-24T12:27:25.019090Z","shell.execute_reply.started":"2025-03-24T12:27:19.823036Z","shell.execute_reply":"2025-03-24T12:27:25.018150Z"}},"outputs":[{"name":"stdout","text":"Extracting media outlets...\nSaved outlets to news_outlets.parquet\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.api as sm\nfrom pathlib import Path\n\ndef prepare_regression_data(enriched_df, outlet_path=\"news_outlets.parquet\"):\n    \"\"\"Preprocess data for regression analysis with caching\"\"\"\n    # Load and merge outlet metadata\n    outlets = pd.read_parquet(outlet_path).set_index('news_id')\n    df = enriched_df.join(outlets, on='news_id')\n    \n    # Calculate top outlets once and cache\n    top_cache = Path(\"top_outlets.csv\")\n    if not top_cache.exists():\n        top_outlets = df['outlet'].value_counts().nlargest(10).index.tolist()\n        pd.Series(top_outlets).to_csv(top_cache, index=False)\n    else:\n        top_outlets = pd.read_csv(top_cache).squeeze().tolist()\n\n    # Create efficient dummy encoding\n    df['outlet'] = pd.Categorical(df['outlet'], categories=top_outlets)\n    dummies = pd.get_dummies(df['outlet'], prefix='outlet', dtype=int)\n    \n    # Prepare final dataset\n    df = df.assign(\n        days_diff=pd.to_numeric(df['days_diff'], errors='coerce'),\n        cosine_similarity=df['cosine_similarity'].astype(float)\n    ).join(dummies).dropna(subset=['days_diff', 'cosine_similarity'])\n    \n    return df\n\ndef run_alignment_regression(preprocessed_df):\n    \"\"\"Run fixed effects regression with robust standard errors\"\"\"\n    # Select features dynamically\n    dummy_cols = [c for c in preprocessed_df if c.startswith('outlet_')]\n    X = sm.add_constant(preprocessed_df[['days_diff'] + dummy_cols])\n    y = preprocessed_df['cosine_similarity']\n    \n    # Build and fit model\n    model = sm.OLS(y, X).fit(\n        cov_type='HC3',\n        use_t=True  # Use t-distribution for p-values\n    )\n    \n    # Format output\n    summary = model.summary2().tables[1]\n    summary['Coef.'] = summary['Coef.'].map(\"{:.4f}\".format)\n    summary['P>|t|'] = summary['P>|t|'].map(lambda x: \"<0.001\" if x < 0.001 else f\"{x:.3f}\")\n    \n    return model, summary\n\n# Usage\nif __name__ == \"__main__\":\n    # Load precomputed data\n    enriched_df = pd.read_parquet('news_speech_similarities.parquet')\n    \n    # Prepare data\n    reg_df = prepare_regression_data(enriched_df)\n    \n    # Run analysis\n    model, results = run_alignment_regression(reg_df)\n    \n    print(\"=== Media Alignment Regression Results ===\")\n    print(results)\n    print(\"\\nKey Insights:\")\n    print(f\"- Temporal effect (days_diff): {model.params['days_diff']:.4f} (p={model.pvalues['days_diff']:.3f})\")\n    print(f\"- Top aligned outlet: {model.params.idxmax()} ({model.params.max():.3f})\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:37:24.200311Z","iopub.execute_input":"2025-03-24T12:37:24.200687Z","iopub.status.idle":"2025-03-24T12:38:32.765698Z","shell.execute_reply.started":"2025-03-24T12:37:24.200636Z","shell.execute_reply":"2025-03-24T12:38:32.764715Z"}},"outputs":[{"name":"stdout","text":"=== Media Alignment Regression Results ===\n                       Coef.  Std.Err.             t   P>|t|    [0.025  \\\nconst                 0.2963  0.000026  11321.480947  <0.001  0.296266   \ndays_diff            -0.0009  0.000005   -176.542877  <0.001 -0.000948   \noutlet_infobae       -0.0135  0.000038   -360.761344  <0.001 -0.013608   \noutlet_proceso        0.0419  0.000042    985.298995  <0.001  0.041774   \noutlet_expansion      0.0103  0.000062    165.356181  <0.001  0.010160   \noutlet_oem            0.0518  0.000065    801.506916  <0.001  0.051643   \noutlet_elfinanciero   0.0108  0.000069    156.940828  <0.001  0.010664   \noutlet_forbes         0.0061  0.000075     80.613589  <0.001  0.005925   \noutlet_elpais        -0.0234  0.000078   -300.680688  <0.001 -0.023528   \noutlet_lasillarota    0.0495  0.000078    638.016662  <0.001  0.049304   \noutlet_eleconomista   0.0456  0.000086    528.347961  <0.001  0.045424   \noutlet_milenio        0.0203  0.000110    184.224925  <0.001  0.020122   \n\n                       0.975]  \nconst                0.296369  \ndays_diff           -0.000927  \noutlet_infobae      -0.013461  \noutlet_proceso       0.041940  \noutlet_expansion     0.010404  \noutlet_oem           0.051896  \noutlet_elfinanciero  0.010934  \noutlet_forbes        0.006220  \noutlet_elpais       -0.023223  \noutlet_lasillarota   0.049608  \noutlet_eleconomista  0.045762  \noutlet_milenio       0.020555  \n\nKey Insights:\n- Temporal effect (days_diff): -0.0009 (p=0.000)\n- Top aligned outlet: const (0.296)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:39:27.500846Z","iopub.execute_input":"2025-03-24T12:39:27.501173Z","iopub.status.idle":"2025-03-24T12:39:29.793764Z","shell.execute_reply.started":"2025-03-24T12:39:27.501145Z","shell.execute_reply":"2025-03-24T12:39:29.792901Z"}},"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      cosine_similarity   R-squared:                       0.078\nModel:                            OLS   Adj. R-squared:                  0.078\nMethod:                 Least Squares   F-statistic:                 2.790e+05\nDate:                Mon, 24 Mar 2025   Prob (F-statistic):               0.00\nTime:                        12:39:29   Log-Likelihood:             3.5765e+07\nNo. Observations:            31327039   AIC:                        -7.153e+07\nDf Residuals:                31327027   BIC:                        -7.153e+07\nDf Model:                          11                                         \nCovariance Type:                  HC3                                         \n=======================================================================================\n                          coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                   0.2963   2.62e-05   1.13e+04      0.000       0.296       0.296\ndays_diff              -0.0009   5.31e-06   -176.543      0.000      -0.001      -0.001\noutlet_infobae         -0.0135   3.75e-05   -360.761      0.000      -0.014      -0.013\noutlet_proceso          0.0419   4.25e-05    985.299      0.000       0.042       0.042\noutlet_expansion        0.0103   6.22e-05    165.356      0.000       0.010       0.010\noutlet_oem              0.0518   6.46e-05    801.507      0.000       0.052       0.052\noutlet_elfinanciero     0.0108   6.88e-05    156.941      0.000       0.011       0.011\noutlet_forbes           0.0061   7.53e-05     80.614      0.000       0.006       0.006\noutlet_elpais          -0.0234   7.77e-05   -300.681      0.000      -0.024      -0.023\noutlet_lasillarota      0.0495   7.75e-05    638.017      0.000       0.049       0.050\noutlet_eleconomista     0.0456   8.63e-05    528.348      0.000       0.045       0.046\noutlet_milenio          0.0203      0.000    184.225      0.000       0.020       0.021\n==============================================================================\nOmnibus:                    85479.848   Durbin-Watson:                   0.012\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            87294.212\nSkew:                          -0.119   Prob(JB):                         0.00\nKurtosis:                       3.103   Cond. No.                         21.1\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n","output_type":"stream"}],"execution_count":5}]}