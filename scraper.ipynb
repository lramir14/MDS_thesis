{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time \n",
    "import random\n",
    "\n",
    "chromedriver_path = r'C:\\Users\\luisf\\Desktop\\MDS_thesis\\scraper_gn\\chromedriver-win64\\chromedriver.exe'\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"amlo dice\"\n",
    "start_date = \"2018-12-03\"\n",
    "end_date = \"2024-09-22\"\n",
    "max_pages = 5\n",
    "limit=100 \n",
    "count=0\n",
    "current_page=1\n",
    "url = f'https://www.google.com/search?q={search_query}+after:{start_date}+before:{end_date}&tbm=nws'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Found 'Next' button using the '#pnnext' selector.\n",
      "Current URL before clicking 'Next': https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&tbm=nws\n",
      "Clicked 'Next' button on page 1.\n",
      "Navigated to new URL: https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=Gfn8ZuTjCeWMxc8PuNPtgQI&start=10&sa=N&ved=2ahUKEwik1MD_mO-IAxVlRvEDHbhpOyAQ8NMDegQIAhAY&biw=929&bih=865&dpr=1\n",
      "Successfully navigated to page 2.\n",
      "Scraping page 2\n",
      "Found 'Next' button using the '#pnnext' selector.\n",
      "Current URL before clicking 'Next': https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=Gfn8ZuTjCeWMxc8PuNPtgQI&start=10&sa=N&ved=2ahUKEwik1MD_mO-IAxVlRvEDHbhpOyAQ8NMDegQIAhAY&biw=929&bih=865&dpr=1\n",
      "Clicked 'Next' button on page 2.\n",
      "Navigated to new URL: https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=Jvn8ZujfCsSKxc8Pj4m6gAU&start=20&sa=N&ved=2ahUKEwjoituFme-IAxVERfEDHY-EDlA4ChDw0wN6BAgFEBs&biw=929&bih=865&dpr=1\n",
      "Successfully navigated to page 3.\n",
      "Scraping page 3\n",
      "Found 'Next' button using the '#pnnext' selector.\n",
      "Current URL before clicking 'Next': https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=Jvn8ZujfCsSKxc8Pj4m6gAU&start=20&sa=N&ved=2ahUKEwjoituFme-IAxVERfEDHY-EDlA4ChDw0wN6BAgFEBs&biw=929&bih=865&dpr=1\n",
      "Clicked 'Next' button on page 3.\n",
      "Navigated to new URL: https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=M_n8ZtLtGpmMi-gPyancmAE&start=30&sa=N&ved=2ahUKEwiS04SMme-IAxUZxgIHHckUFxM4FBDw0wN6BAgCEB0&biw=929&bih=865&dpr=1\n",
      "Successfully navigated to page 4.\n",
      "Scraping page 4\n",
      "Found 'Next' button using the '#pnnext' selector.\n",
      "Current URL before clicking 'Next': https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=M_n8ZtLtGpmMi-gPyancmAE&start=30&sa=N&ved=2ahUKEwiS04SMme-IAxUZxgIHHckUFxM4FBDw0wN6BAgCEB0&biw=929&bih=865&dpr=1\n",
      "Clicked 'Next' button on page 4.\n",
      "Navigated to new URL: https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=PPn8ZvPVF_atxc8P5faPmQs&start=40&sa=N&ved=2ahUKEwjz46aQme-IAxX2VvEDHWX7I7M4HhDw0wN6BAgCEB8&biw=929&bih=865&dpr=1\n",
      "Successfully navigated to page 5.\n",
      "Scraping page 5\n",
      "Found 'Next' button using the '#pnnext' selector.\n",
      "Current URL before clicking 'Next': https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=PPn8ZvPVF_atxc8P5faPmQs&start=40&sa=N&ved=2ahUKEwjz46aQme-IAxX2VvEDHWX7I7M4HhDw0wN6BAgCEB8&biw=929&bih=865&dpr=1\n",
      "Clicked 'Next' button on page 5.\n",
      "Navigated to new URL: https://www.google.com/search?q=amlo+dice+after:2018-12-03+before:2024-09-22&sca_esv=4997d9601951f80e&sca_upv=1&tbm=nws&ei=Rfn8ZtDgD-OG7NYPj6XbmAw&start=50&sa=N&ved=2ahUKEwiQl8SUme-IAxVjA9sEHY_SFsM4KBDw0wN6BAgCECE&biw=929&bih=865&dpr=1\n",
      "Successfully navigated to page 6.\n"
     ]
    }
   ],
   "source": [
    "#Second iteration with pagination... fully functional. \n",
    "news_data = []\n",
    "\n",
    "while current_page <= max_pages and count < limit:\n",
    "    print(f\"Scraping page {current_page}\")\n",
    "    \n",
    "    try:\n",
    "        # Wait for the news results to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.SoaBEf'))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Timeout while waiting for news articles on page {current_page}: {e}\")\n",
    "        # Optional: Take a screenshot for debugging\n",
    "        driver.save_screenshot(f'timeout_page_{current_page}.png')\n",
    "        break  # Exit if articles do not load in time\n",
    "\n",
    "    # Select all news article blocks by targeting the SoaBEf class\n",
    "    news_results = driver.find_elements(By.CSS_SELECTOR, 'div.SoaBEf')\n",
    "    \n",
    "    if not news_results:\n",
    "        print(f\"No articles found on page {current_page}.\")\n",
    "        # Optional: Take a screenshot to inspect the page\n",
    "        driver.save_screenshot(f'no_articles_page_{current_page}.png')\n",
    "        break  # Exit if no articles are found\n",
    "\n",
    "    for news_div in news_results:\n",
    "        if count >= limit:\n",
    "            print(f\"Reached the limit of {limit} articles.\")\n",
    "            break  # Stop if we've reached the limit\n",
    "\n",
    "        try:\n",
    "            # Extract the link to the article\n",
    "            news_link = news_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "            # Extract the title of the article\n",
    "            news_title = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"]').text\n",
    "\n",
    "            # Extract the source domain\n",
    "            news_domain = news_div.find_element(By.CSS_SELECTOR, 'div').text\n",
    "\n",
    "            # Extract the snippet description and the publication date\n",
    "            news_description = news_div.find_element(By.CSS_SELECTOR, 'a div[role=\"heading\"] + div').text\n",
    "            news_date = news_div.find_element(By.CSS_SELECTOR, 'span').text\n",
    "\n",
    "            # Append the collected information to the list\n",
    "            news_data.append([news_link, news_domain, news_title, news_description, news_date])\n",
    "            count += 1  # Increment the count of news articles\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping an article on page {current_page}: {e}\")\n",
    "            # Optional: Take a screenshot for debugging\n",
    "            driver.save_screenshot(f'error_article_page_{current_page}.png')\n",
    "            continue  # Skip to the next article\n",
    "\n",
    "    # Check if we've reached the limit after scraping the current page\n",
    "    if count >= limit:\n",
    "        print(f\"Reached the limit of {limit} articles.\")\n",
    "        break\n",
    "\n",
    "    # Attempt to locate the \"Next\" button and click it using the general selector\n",
    "    try:\n",
    "        # Find the \"Next\" button using its general ID selector\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, '#pnnext'))\n",
    "        )\n",
    "        print(\"Found 'Next' button using the '#pnnext' selector.\")\n",
    "        \n",
    "        # Scroll to the \"Next\" button to ensure it's in view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "        \n",
    "        # Introduce a random sleep to mimic human behavior\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Capture the current URL before clicking \"Next\"\n",
    "        current_url = driver.current_url\n",
    "        print(f\"Current URL before clicking 'Next': {current_url}\")\n",
    "        \n",
    "        # Click the \"Next\" button\n",
    "        next_button.click()\n",
    "        print(f\"Clicked 'Next' button on page {current_page}.\")\n",
    "        \n",
    "        # Increment the current page counter\n",
    "        current_page += 1\n",
    "        \n",
    "        # Optional: Add a short sleep to wait for the next page to load\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        # Capture the new URL after clicking \"Next\"\n",
    "        new_url = driver.current_url\n",
    "        print(f\"Navigated to new URL: {new_url}\")\n",
    "        \n",
    "        # Verify that the URL has changed to prevent looping\n",
    "        if new_url == current_url:\n",
    "            print(\"URL did not change after clicking 'Next'. Possible loop detected.\")\n",
    "            break  # Exit the loop to prevent infinite cycling\n",
    "        else:\n",
    "            print(f\"Successfully navigated to page {current_page}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No more pages available or error clicking 'Next' on page {current_page}: {e}\")\n",
    "        # Optional: Take a screenshot to inspect the state when the error occurred\n",
    "        driver.save_screenshot(f'next_button_error_page_{current_page}.png')\n",
    "        break  # Exit the loop if there's no next button or an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://efe.com/mundo/2024-06-17/mexico-amlo-se-dice-muy-satisfecho-por-eleccion-de-sheinbaum/', 'Agencia EFE\\nMéxico: AMLO se dice \"muy satisfecho\" por elección de Sheinbaum\\nLa gira de Sheinbaum y López Obrador por México continúa. El gobernante mexicano aseveró que “es algo inédito” la gira que comenzaron ambos políticos el viernes...\\n.\\n17 jun 2024', 'México: AMLO se dice \"muy satisfecho\" por elección de Sheinbaum', 'La gira de Sheinbaum y López Obrador por México continúa. El gobernante mexicano aseveró que “es algo inédito” la gira que comenzaron ambos políticos el viernes...', 'Agencia EFE'], ['https://www.bbc.com/mundo/noticias-internacional-53344257', 'BBC\\nDiscurso de AMLO: el agradecimiento a Trump y otros momentos llamativos del encuentro de ambos presidentes en la Casa Blanca\\nEl presidente de México, Andrés Manuel López Obrador, miró a los ojos a su par de Estados Unidos, Donald Trump, y le dijo: \"Fallaron los pronósticos,...\\n.\\n9 jul 2020', 'Discurso de AMLO: el agradecimiento a Trump y otros momentos llamativos del encuentro de ambos presidentes en la Casa Blanca', 'El presidente de México, Andrés Manuel López Obrador, miró a los ojos a su par de Estados Unidos, Donald Trump, y le dijo: \"Fallaron los pronósticos,...', 'BBC'], ['https://cnnespanol.cnn.com/video/lunes-negro-caida-mercados-fortaleza-economia-peso-amlo-perspectivas-mexico-tv-orix', 'CNN en Español\\n\"No nos afecta tanto\", dice AMLO sobre la caída mundial de mercados\\nEl presidente de México, Andrés Manuel López Obrador, aseguró que el gobierno tiene finanzas sólidas y que la devaluación del peso frente al dólar se debe a...\\n.\\n5 ago 2024', '\"No nos afecta tanto\", dice AMLO sobre la caída mundial de mercados', 'El presidente de México, Andrés Manuel López Obrador, aseguró que el gobierno tiene finanzas sólidas y que la devaluación del peso frente al dólar se debe a...', 'CNN en Español'], ['https://www.infobae.com/mexico/2024/09/19/amlo-dice-que-eeuu-dejo-de-opinar-sobre-la-reforma-al-poder-judicial/', 'Infobae\\nAMLO dice que EEUU dejó de opinar sobre la reforma al Poder Judicial\\nDurante la conferencia de prensa mañanera de este jueves, el presidente Andrés Manuel López Obrador (AMLO) dijo, ante la pregunta de cómo va la situación de...\\n.\\nhace 2 semanas', 'AMLO dice que EEUU dejó de opinar sobre la reforma al Poder Judicial', 'Durante la conferencia de prensa mañanera de este jueves, el presidente Andrés Manuel López Obrador (AMLO) dijo, ante la pregunta de cómo va la situación de...', 'Infobae'], ['https://cnnespanol.cnn.com/2024/05/07/seguir-juntos-amlo-rumores-divorcio-beatriz-gutierrez-orix', 'CNN en Español\\n“Vamos a seguir juntos”, dice presidente de México ante rumores sobre posible divorcio de la escritora Beatriz Gutiérrez\\nEl presidente de México, Andrés Manuel López Obrador, rechazó este martes que vaya a separarse de su esposa, Beatriz Gutiérrez Müller, acallando así rumores...\\n.\\n7 may 2024', '“Vamos a seguir juntos”, dice presidente de México ante rumores sobre posible divorcio de la escritora Beatriz Gutiérrez', 'El presidente de México, Andrés Manuel López Obrador, rechazó este martes que vaya a separarse de su esposa, Beatriz Gutiérrez Müller, acallando así rumores...', 'CNN en Español'], ['https://www.capital21.cdmx.gob.mx/noticias/?p=36222', 'Capital 21\\n¡Ya se acabó la robadera! AMLO le dice a partidos de ‘Va por México’\\nEl presidente López Obrador señaló que la coalición del PRI, PAN y PRD busca detener el proceso de transformación que vive el país y regresar por sus...\\n.\\n20 sept 2023', '¡Ya se acabó la robadera! AMLO le dice a partidos de ‘Va por México’', 'El presidente López Obrador señaló que la coalición del PRI, PAN y PRD busca detener el proceso de transformación que vive el país y regresar por sus...', 'Capital 21'], ['https://verificado.com.mx/video-manipulado-amlo-secuestros-sociedad-pobre/', 'Verificado\\nRecircula video manipulado donde AMLO dice que los secuestros se solucionan con una sociedad pobre\\nRecircula video manipulado donde AMLO dice que los secuestros se solucionan con una sociedad pobre ... En WhatsApp circula un video donde supuestamente Andrés...\\n.\\n25 abr 2024', 'Recircula video manipulado donde AMLO dice que los secuestros se solucionan con una sociedad pobre', 'Recircula video manipulado donde AMLO dice que los secuestros se solucionan con una sociedad pobre ... En WhatsApp circula un video donde supuestamente Andrés...', 'Verificado'], ['https://www.reforma.com/dice-amlo-que-exhibira-casos-de-impuestos-parados-en-pj/ar2841944', \"Reforma\\nDice AMLO que exhibirá casos de impuestos 'parados' en PJ\\nEl Presidente López Obrador anunció que exhibirá casos de impuestos 'parados' por el PJ, los cuales ascienden a alrededor de 100 mil mdp.\\n.\\n17 jul 2024\", \"Dice AMLO que exhibirá casos de impuestos 'parados' en PJ\", \"El Presidente López Obrador anunció que exhibirá casos de impuestos 'parados' por el PJ, los cuales ascienden a alrededor de 100 mil mdp.\", 'Reforma'], ['https://www.bbc.com/mundo/noticias-america-latina-48374340', 'BBC\\nAMLO: qué es el Instituto para Devolverle al Pueblo lo Robado que instauró el presidente de México\\nAutos de lujo, residencias y joyas incautados a criminales serán puestas a subasta pública en México para obtener al menos US$62 millones.\\n.\\n23 may 2019', 'AMLO: qué es el Instituto para Devolverle al Pueblo lo Robado que instauró el presidente de México', 'Autos de lujo, residencias y joyas incautados a criminales serán puestas a subasta pública en México para obtener al menos US$62 millones.', 'BBC'], ['https://elpais.com/mexico/2024-01-22/lopez-obrador-sobre-las-acusaciones-de-corrupcion-contra-sus-hijos-no-me-quita-el-sueno.html', 'EL PAÍS\\nLópez Obrador, sobre las acusaciones de corrupción contra sus hijos: “No me quita el sueño”\\nEl presidente descarta que sus hijos estén involucrados en las adjudicaciones públicas que favorecieron a los amigos familiares.\\n.\\n22 ene 2024', 'López Obrador, sobre las acusaciones de corrupción contra sus hijos: “No me quita el sueño”', 'El presidente descarta que sus hijos estén involucrados en las adjudicaciones públicas que favorecieron a los amigos familiares.', 'EL PAÍS']]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(news_data)\n",
    "print(len(news_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Link Domain  \\\n",
      "0  https://www.bbc.com/mundo/articles/cv22e6g3x59o    BBC   \n",
      "\n",
      "                                               Title  \\\n",
      "0  AMLO: los 4 pilares que explican su alta popul...   \n",
      "\n",
      "                                         Description         Date  \n",
      "0  A cuatro meses del fin de su mandato, el presi...  29 may 2024  \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(news_data, columns=[\"Link\", \"Domain\", \"Title\", \"Description\", \"Date\"])\n",
    "print(df)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the results to a CSV file\n",
    "csv_file_path = \"news_results.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Link\", \"Domain\", \"Title\", \"Description\", \"Date\"])\n",
    "    # Write the news data\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete code for scraping and iterating over pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import quote\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Configure Chrome Options\n",
    "# -----------------------------\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "user_agent = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/112.0.0.0 Safari/537.36\"\n",
    ")\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize WebDriver\n",
    "# -----------------------------\n",
    "chromedriver_path = r'C:\\Users\\luisf\\Desktop\\MDS_thesis\\scraper_gn\\chromedriver-win64\\chromedriver.exe'\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# -----------------------------\n",
    "# Define Search Parameters\n",
    "# -----------------------------\n",
    "search_query = \"amlo dice\"\n",
    "start_date = \"2018-12-03\"\n",
    "end_date = \"2024-09-22\"\n",
    "max_pages = 3  # Set the number of pages to scrape\n",
    "limit = 100  # Set the limit for the number of articles\n",
    "count = 0\n",
    "current_page = 1\n",
    "\n",
    "# Encode the search query for URL\n",
    "encoded_query = quote(search_query)\n",
    "url = f'https://www.google.com/search?q={encoded_query}+after:{start_date}+before:{end_date}&tbm=nws'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Timeout while waiting for news articles on page 1.\n",
      "Scraping completed. 0 articles saved to 'news_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a list to hold the results\n",
    "news_data = []\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    \n",
    "    while current_page <= max_pages and count < limit:\n",
    "        print(f\"Scraping page {current_page}\")\n",
    "        \n",
    "        try:\n",
    "            # Wait for the news results to load\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.SoaBEf'))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout while waiting for news articles on page {current_page}.\")\n",
    "            break  # Exit if articles do not load in time\n",
    "        \n",
    "        # Select all news article blocks\n",
    "        news_results = driver.find_elements(By.CSS_SELECTOR, 'div.SoaBEf')\n",
    "        \n",
    "        for news_div in news_results:\n",
    "            if count >= limit:\n",
    "                break  # Stop if we've reached the limit\n",
    "\n",
    "            try:\n",
    "                # Extract the link to the article\n",
    "                news_link = news_div.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "                # Extract the title of the article\n",
    "                news_title = news_div.find_element(By.CSS_SELECTOR, 'div[role=\"heading\"]').text\n",
    "\n",
    "                # Extract the source domain\n",
    "                try:\n",
    "                    news_domain = news_div.find_element(By.CSS_SELECTOR, 'div.XTjFC').text\n",
    "                except NoSuchElementException:\n",
    "                    news_domain = \"Domain not found\"\n",
    "\n",
    "                # Extract the description\n",
    "                try:\n",
    "                    news_description = news_div.find_element(By.CSS_SELECTOR, 'div.Y3v8qd').text\n",
    "                except NoSuchElementException:\n",
    "                    news_description = \"Description not found\"\n",
    "\n",
    "                # Extract the date\n",
    "                try:\n",
    "                    news_date = news_div.find_element(By.CSS_SELECTOR, 'span.WG9SHc').text\n",
    "                except NoSuchElementException:\n",
    "                    news_date = \"Date not found\"\n",
    "\n",
    "                # Append the collected information to the list\n",
    "                news_data.append([news_link, news_domain, news_title, news_description, news_date])\n",
    "                count += 1  # Increment the count of news articles\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping an article on page {current_page}: {e}\")\n",
    "                continue  # Skip to the next article\n",
    "        \n",
    "        if count >= limit:\n",
    "            print(f\"Reached the limit of {limit} articles.\")\n",
    "            break\n",
    "\n",
    "        # Attempt to click the \"Next\" button to go to the next page\n",
    "        try:\n",
    "            # Locate the \"Next\" button by its aria-label\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//a[@aria-label=\"Next\"]'))\n",
    "            )\n",
    "            \n",
    "            # Scroll to the \"Next\" button to ensure it's in view\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "            \n",
    "            # Introduce a random sleep to mimic human behavior\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            \n",
    "            # Click the \"Next\" button\n",
    "            next_button.click()\n",
    "            \n",
    "            # Increment the current page counter\n",
    "            current_page += 1\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"No more pages available or 'Next' button not found.\")\n",
    "            break  # Exit the loop if there's no next button\n",
    "        except ElementClickInterceptedException as e:\n",
    "            print(f\"Could not click the 'Next' button on page {current_page}: {e}\")\n",
    "            break  # Exit the loop if clicking fails\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error while navigating to next page: {e}\")\n",
    "            break  # Exit the loop on any other unexpected errors\n",
    "\n",
    "        # Optional: Add a short sleep before scraping the next page\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    \n",
    "    # -----------------------------\n",
    "    # Save the Data to a CSV File\n",
    "    # -----------------------------\n",
    "    df = pd.DataFrame(news_data, columns=[\"Link\", \"Domain\", \"Title\", \"Description\", \"Date\"])\n",
    "    df.to_csv('news_data.csv', index=False, encoding='utf-8')\n",
    "    print(f\"Scraping completed. {count} articles saved to 'news_data.csv'.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Link, Domain, Title, Description, Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
